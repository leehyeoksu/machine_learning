{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6ì¼ì°¨: PyTorch í…ì„œì™€ Autograd ê¸°ë³¸\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- PyTorch í…ì„œ ê¸°ë³¸ ì‚¬ìš©ë²• í•™ìŠµ\n",
    "- Autograd(ìë™ ë¯¸ë¶„) ë©”ì»¤ë‹ˆì¦˜ ì´í•´\n",
    "- Gradient ê³„ì‚° ë° ì—­ì „íŒŒ ì‹¤ìŠµ\n",
    "- Gradient ì¶”ì  ì œì–´í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ë²„ì „: 2.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print(f'PyTorch ë²„ì „: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ ì´ë¡  ê°œìš”\n",
    "\n",
    "### PyTorch Tensor\n",
    "- NumPyì™€ ìœ ì‚¬í•œ ë‹¤ì°¨ì› ë°°ì—´\n",
    "- **GPU ì—°ì‚° ì§€ì›**\n",
    "- **ìë™ ë¯¸ë¶„ ì§€ì›** (autograd)\n",
    "\n",
    "### Autograd (ìë™ ë¯¸ë¶„)\n",
    "- ì—°ì‚° ê·¸ë˜í”„ ìë™ ìƒì„±\n",
    "- `.backward()` í˜¸ì¶œë¡œ gradient ê³„ì‚°\n",
    "- ë”¥ëŸ¬ë‹ ì—­ì „íŒŒì˜ í•µì‹¬\n",
    "\n",
    "### ì£¼ìš” ê°œë…\n",
    "- `requires_grad=True`: gradient ì¶”ì  ì‹œì‘\n",
    "- `.backward()`: gradient ê³„ì‚°\n",
    "- `.grad`: ê³„ì‚°ëœ gradient ì €ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š 1. PyTorch í…ì„œ ê¸°ë³¸\n",
    "\n",
    "### 1.1 í…ì„œ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ì„œ: tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n",
      "íƒ€ì…: <class 'torch.Tensor'>\n",
      "dtype: torch.int32\n",
      "shape: torch.Size([5])\n",
      "device: cpu\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° í…ì„œ ìƒì„±\n",
    "t = torch.tensor([1, 2, 3, 4, 5],dtype=torch.int)\n",
    "print('í…ì„œ:', t)\n",
    "print(f'íƒ€ì…: {type(t)}')\n",
    "print(f'dtype: {t.dtype}')\n",
    "print(f'shape: {t.shape}')\n",
    "print(f'device: {t.device}')  # cpu ë˜ëŠ” cuda\n",
    "print(torch.linalg.matrix_rank(torch.tensor([[1,2],[3,4]],dtype=torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros (2Ã—3):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "ones (3Ã—2):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "rand (2Ã—2):\n",
      "tensor([[0.7900, 0.9668],\n",
      "        [0.2255, 0.1649]])\n",
      "\n",
      "randn (2Ã—2):\n",
      "tensor([[ 0.2248,  0.5166],\n",
      "        [ 0.2952, -1.2607]])\n"
     ]
    }
   ],
   "source": [
    "# ë‹¤ì–‘í•œ í…ì„œ ìƒì„± ë°©ë²•\n",
    "zeros = torch.zeros(2, 3)\n",
    "ones = torch.ones(3, 2)\n",
    "rand = torch.rand(2, 2)  # [0, 1) ê· ë“±ë¶„í¬\n",
    "randn = torch.randn(2, 2)  # í‘œì¤€ì •ê·œë¶„í¬\n",
    "\n",
    "print('zeros (2Ã—3):'); print(zeros)\n",
    "print('\\nones (3Ã—2):'); print(ones)\n",
    "print('\\nrand (2Ã—2):'); print(rand)\n",
    "print('\\nrandn (2Ã—2):'); print(randn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 NumPyì™€ ìƒí˜¸ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy ë°°ì—´:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "PyTorch í…ì„œ:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "ë‹¤ì‹œ NumPyë¡œ:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "âš ï¸ ì£¼ì˜: ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•©ë‹ˆë‹¤!\n",
      "NumPy ìˆ˜ì • í›„ í…ì„œ: tensor([[999,   2],\n",
      "        [  3,   4]])\n"
     ]
    }
   ],
   "source": [
    "# NumPy â†’ PyTorch\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "\n",
    "print('NumPy ë°°ì—´:')\n",
    "print(np_array)\n",
    "print('\\nPyTorch í…ì„œ:')\n",
    "print(torch_tensor)\n",
    "print()\n",
    "\n",
    "# PyTorch â†’ NumPy\n",
    "back_to_numpy = torch_tensor.numpy()\n",
    "print('ë‹¤ì‹œ NumPyë¡œ:')\n",
    "print(back_to_numpy)\n",
    "print()\n",
    "\n",
    "print('âš ï¸ ì£¼ì˜: ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•©ë‹ˆë‹¤!')\n",
    "np_array[0, 0] = 999\n",
    "print('NumPy ìˆ˜ì • í›„ í…ì„œ:', torch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š 2. Autograd ê¸°ë³¸\n",
    "\n",
    "### 2.1 ê°„ë‹¨í•œ ë¯¸ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 2.0\n",
      "requires_grad: True\n",
      "\n",
      "y = xÂ² + 3x + 5 = 15.0\n",
      "\n",
      "Gradient ê³„ì‚° ì™„ë£Œ!\n",
      "dy/dx = 2x + 3 = 2Ã—2.0 + 3 = 7.0\n",
      "\n",
      "ì‹¤ì œ gradient: 7.0\n"
     ]
    }
   ],
   "source": [
    "# y = xÂ² + 3x + 5ì—ì„œ dy/dx ê³„ì‚°\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(f'x = {x}')\n",
    "print(f'requires_grad: {x.requires_grad}')\n",
    "print()\n",
    "\n",
    "# Forward pass\n",
    "y = x**2 + 3*x + 5\n",
    "print(f'y = xÂ² + 3x + 5 = {y.item()}')\n",
    "print()\n",
    "\n",
    "# Backward pass (gradient ê³„ì‚°)\n",
    "y.backward()\n",
    "\n",
    "print('Gradient ê³„ì‚° ì™„ë£Œ!')\n",
    "print(f'dy/dx = 2x + 3 = 2Ã—{x.item()} + 3 = {x.grad.item()}')\n",
    "print()\n",
    "print(f'ì‹¤ì œ gradient: {x.grad.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 1.0, y = 2.0\n",
      "\n",
      "z = xÂ² + 2xy + yÂ² = 9.0\n",
      "\n",
      "Gradients:\n",
      "âˆ‚z/âˆ‚x = 2x + 2y = 2Ã—1.0 + 2Ã—2.0 = 6.0\n",
      "âˆ‚z/âˆ‚y = 2x + 2y = 2Ã—1.0 + 2Ã—2.0 = 6.0\n",
      "\n",
      "ìˆ˜ì‹: âˆ‚z/âˆ‚x = âˆ‚z/âˆ‚y = 2x + 2y\n"
     ]
    }
   ],
   "source": [
    "# z = xÂ² + 2xy + yÂ²ì—ì„œ âˆ‚z/âˆ‚x, âˆ‚z/âˆ‚y\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "print(f'x = {x.item()}, y = {y.item()}')\n",
    "print()\n",
    "\n",
    "# Forward\n",
    "z = x**2 + 2*x*y + y**2\n",
    "print(f'z = xÂ² + 2xy + yÂ² = {z.item()}')\n",
    "print()\n",
    "\n",
    "# Backward\n",
    "z.backward()\n",
    "\n",
    "print('Gradients:')\n",
    "print(f'âˆ‚z/âˆ‚x = 2x + 2y = 2Ã—{x.item()} + 2Ã—{y.item()} = {x.grad.item()}')\n",
    "print(f'âˆ‚z/âˆ‚y = 2x + 2y = 2Ã—{x.item()} + 2Ã—{y.item()} = {y.grad.item()}')\n",
    "print()\n",
    "print('ìˆ˜ì‹: âˆ‚z/âˆ‚x = âˆ‚z/âˆ‚y = 2x + 2y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ë²¡í„° ë° í–‰ë ¬ ì—°ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = tensor([1., 2., 3.], requires_grad=True)\n",
      "w = tensor([4., 5., 6.])\n",
      "\n",
      "vÂ·w = 32.0\n",
      "\n",
      "âˆ‚(vÂ·w)/âˆ‚v = w\n",
      "gradient: tensor([4., 5., 6.])\n",
      "wì™€ ì¼ì¹˜: True\n"
     ]
    }
   ],
   "source": [
    "# ë²¡í„° ë‚´ì ì˜ gradient\n",
    "v = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "w = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(f'v = {v}')\n",
    "print(f'w = {w}')\n",
    "print()\n",
    "\n",
    "# ë‚´ì : vÂ·w\n",
    "dot_product = torch.dot(v, w)\n",
    "print(f'vÂ·w = {dot_product.item()}')\n",
    "print()\n",
    "\n",
    "dot_product.backward()\n",
    "\n",
    "print('âˆ‚(vÂ·w)/âˆ‚v = w')\n",
    "print(f'gradient: {v.grad}')\n",
    "print(f'wì™€ ì¼ì¹˜: {torch.allclose(v.grad, w)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š 3. Gradient ì œì–´\n",
    "\n",
    "### 3.1 torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¼ë°˜ ì—°ì‚° (gradient ì¶”ì ):\n",
      "y.requires_grad: True\n",
      "\n",
      "torch.no_grad() ì‚¬ìš©:\n",
      "z.requires_grad: False\n",
      "\n",
      "âš ï¸ Inference ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ no_grad ì‚¬ìš©!\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "print('ì¼ë°˜ ì—°ì‚° (gradient ì¶”ì ):')\n",
    "y = x * 2\n",
    "print(f'y.requires_grad: {y.requires_grad}')\n",
    "print()\n",
    "\n",
    "print('torch.no_grad() ì‚¬ìš©:')\n",
    "with torch.no_grad():\n",
    "    z = x * 2\n",
    "    print(f'z.requires_grad: {z.requires_grad}')\n",
    "print()\n",
    "print('âš ï¸ Inference ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ no_grad ì‚¬ìš©!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Gradient ëˆ„ì ê³¼ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ë²ˆì§¸ backward í›„ x.grad: 6.0\n",
      "ë‘ ë²ˆì§¸ backward í›„ x.grad: 33.0\n",
      "â†’ Gradientê°€ ëˆ„ì ë¨!\n",
      "\n",
      "ì´ˆê¸°í™” í›„ x.grad: 0.0\n",
      "\n",
      "ğŸ’¡ í•™ìŠµ ë£¨í”„ì—ì„œëŠ” ë§¤ iterationë§ˆë‹¤ optimizer.zero_grad() í•„ìš”!\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# ì²« ë²ˆì§¸ backward\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f'ì²« ë²ˆì§¸ backward í›„ x.grad: {x.grad}')\n",
    "\n",
    "# gradient ì´ˆê¸°í™” ì—†ì´ ë‘ ë²ˆì§¸ backward\n",
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(f'ë‘ ë²ˆì§¸ backward í›„ x.grad: {x.grad}')\n",
    "print('â†’ Gradientê°€ ëˆ„ì ë¨!')\n",
    "print()\n",
    "\n",
    "# Gradient ì´ˆê¸°í™”\n",
    "x.grad.zero_()\n",
    "print(f'ì´ˆê¸°í™” í›„ x.grad: {x.grad}')\n",
    "print()\n",
    "print('ğŸ’¡ í•™ìŠµ ë£¨í”„ì—ì„œëŠ” ë§¤ iterationë§ˆë‹¤ optimizer.zero_grad() í•„ìš”!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š 4. ì‹¤ì „ ì˜ˆì œ: ì„ í˜• íšŒê·€\n",
    "\n",
    "### 4.1 ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° shape:\n",
      "x: torch.Size([100, 1])\n",
      "y: torch.Size([100, 1])\n",
      "\n",
      "ëª©í‘œ: y = 3x + 1ì„ í•™ìŠµìœ¼ë¡œ ì°¾ê¸°\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°: y = 3x + 1 + noise\n",
    "torch.manual_seed(42)\n",
    "x_data = torch.randn(100, 1)\n",
    "y_data = 3 * x_data + 1 + 0.1 * torch.randn(100, 1)\n",
    "\n",
    "print('ë°ì´í„° shape:')\n",
    "print(f'x: {x_data.shape}')\n",
    "print(f'y: {y_data.shape}')\n",
    "print()\n",
    "print('ëª©í‘œ: y = 3x + 1ì„ í•™ìŠµìœ¼ë¡œ ì°¾ê¸°')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ëª¨ë¸ íŒŒë¼ë¯¸í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ˆê¸° w: 0.3932\n",
      "ì´ˆê¸° b: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\n",
    "w = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.zeros(1, 1, requires_grad=True)\n",
    "\n",
    "print(f'ì´ˆê¸° w: {w.item():.4f}')\n",
    "print(f'ì´ˆê¸° b: {b.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 í•™ìŠµ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 3.6352, w: 1.2522, b: 0.3761\n",
      "Epoch [40/100], Loss: 1.6105, w: 1.8276, b: 0.6132\n",
      "Epoch [60/100], Loss: 0.7175, w: 2.2132, b: 0.7621\n",
      "Epoch [80/100], Loss: 0.3227, w: 2.4717, b: 0.8553\n",
      "Epoch [100/100], Loss: 0.1479, w: 2.6452, b: 0.9132\n",
      "\n",
      "ìµœì¢… w: 2.6452 (ëª©í‘œ: 3.0)\n",
      "ìµœì¢… b: 0.9132 (ëª©í‘œ: 1.0)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = x_data @ w + b\n",
    "    \n",
    "    # Loss (MSE)\n",
    "    loss = torch.mean((y_pred - y_data) ** 2)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update (gradient descent)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # Gradient ì´ˆê¸°í™”\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, w: {w.item():.4f}, b: {b.item():.4f}')\n",
    "\n",
    "print()\n",
    "print(f'ìµœì¢… w: {w.item():.4f} (ëª©í‘œ: 3.0)')\n",
    "print(f'ìµœì¢… b: {b.item():.4f} (ëª©í‘œ: 1.0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "### ë¬¸ì œ 1: ë‹¨ì¼ ë³€ìˆ˜ ë¯¸ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: f(x) = xÂ³ - 2xÂ² + 5x - 1ì—ì„œ\n",
    "# x = 2ì¼ ë•Œ df/dxë¥¼ ê³„ì‚°í•˜ì„¸ìš”\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œ ì‘ì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = xÂ³ - 2xÂ² + 5x - 1\n",
      "f'(x) = 3xÂ² - 4x + 5\n",
      "\n",
      "x = 2ì¼ ë•Œ:\n",
      "f(2) = 9.0\n",
      "f'(2) = 3Ã—4 - 4Ã—2 + 5 = 9\n",
      "ê³„ì‚°ëœ gradient: 9.0\n"
     ]
    }
   ],
   "source": [
    "# í•´ë‹µ\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "f = x**3 - 2*x**2 + 5*x - 1\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(f'f(x) = xÂ³ - 2xÂ² + 5x - 1')\n",
    "print(f'f\\'(x) = 3xÂ² - 4x + 5')\n",
    "print(f'\\nx = 2ì¼ ë•Œ:')\n",
    "print(f'f(2) = {f.item()}')\n",
    "print(f'f\\'(2) = 3Ã—4 - 4Ã—2 + 5 = 9')\n",
    "print(f'ê³„ì‚°ëœ gradient: {x.grad.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¬¸ì œ 2: ë²¡í„° í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: x = [1, 2, 3]ì¼ ë•Œ\n",
    "# f(x) = Î£xáµ¢Â²ì˜ gradientë¥¼ êµ¬í•˜ì„¸ìš”\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œ ì‘ì„±í•œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = Î£xáµ¢Â² = xâ‚Â² + xâ‚‚Â² + xâ‚ƒÂ²\n",
      "âˆ‚f/âˆ‚xáµ¢ = 2xáµ¢\n",
      "\n",
      "x = [1, 2, 3]\n",
      "gradient = [2Ã—1, 2Ã—2, 2Ã—3] = tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# í•´ë‹µ\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "f = torch.sum(x ** 2)\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(f'f(x) = Î£xáµ¢Â² = xâ‚Â² + xâ‚‚Â² + xâ‚ƒÂ²')\n",
    "print(f'âˆ‚f/âˆ‚xáµ¢ = 2xáµ¢')\n",
    "print(f'\\nx = [1, 2, 3]')\n",
    "print(f'gradient = [2Ã—1, 2Ã—2, 2Ã—3] = {x.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ë³µìŠµ\n",
    "\n",
    "### í•µì‹¬ ì •ë¦¬\n",
    "\n",
    "**PyTorch Tensor**\n",
    "- GPU ì—°ì‚° ê°€ëŠ¥ (`.to('cuda')`)\n",
    "- NumPy í˜¸í™˜ (`.numpy()`, `torch.from_numpy()`)\n",
    "- ìë™ ë¯¸ë¶„ ì§€ì›\n",
    "\n",
    "**Autograd**\n",
    "- `requires_grad=True`: ì¶”ì  ì‹œì‘\n",
    "- `.backward()`: gradient ê³„ì‚°\n",
    "- `.grad`: gradient ê°’\n",
    "- `zero_()`: gradient ì´ˆê¸°í™”\n",
    "\n",
    "**ì£¼ì˜ì‚¬í•­**\n",
    "1. Gradient ëˆ„ì  - ë§¤ë²ˆ ì´ˆê¸°í™” í•„ìš”\n",
    "2. Inference ì‹œ `torch.no_grad()` ì‚¬ìš©\n",
    "3. `.item()`ìœ¼ë¡œ ìŠ¤ì¹¼ë¼ ê°’ ì¶”ì¶œ\n",
    "\n",
    "### ë‹¤ìŒ í•™ìŠµ\n",
    "Day 7: nn.Module, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €ë¡œ ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
