{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03. 종합 프로젝트: MNIST 손글씨 인식\n",
                "\n",
                "## 프로젝트 개요\n",
                "- **데이터**: MNIST (28x28 흑백 손글씨 숫자 이미지)\n",
                "- **목표**: 0-9 숫자 분류 (10-class classification)\n",
                "- **구현**: Dataset, DataLoader, CNN, 훈련, 평가, 시각화\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import datasets, transforms\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 데이터 준비\n",
                "\n",
                "### 1.1 Transforms (데이터 전처리)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transform: PIL Image → Tensor, 정규화\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),  # [0, 255] → [0, 1] Tensor\n",
                "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 평균, 표준편차\n",
                "])\n",
                "\n",
                "print(\"Transform pipeline:\")\n",
                "print(transform)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Dataset 로드"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MNIST dataset 다운로드 및 로드\n",
                "train_dataset = datasets.MNIST(\n",
                "    root='./data',  # 저장 경로\n",
                "    train=True,  # Training set\n",
                "    transform=transform,\n",
                "    download=True  # 없으면 다운로드\n",
                ")\n",
                "\n",
                "test_dataset = datasets.MNIST(\n",
                "    root='./data',\n",
                "    train=False,  # Test set\n",
                "    transform=transform,\n",
                "    download=True\n",
                ")\n",
                "\n",
                "print(f\"Train dataset size: {len(train_dataset)}\")\n",
                "print(f\"Test dataset size: {len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 DataLoader (배치 생성)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 64\n",
                "\n",
                "train_loader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=batch_size,\n",
                "    shuffle=True,  # 매 epoch마다 섞기\n",
                "    num_workers=2  # 병렬 데이터 로딩\n",
                ")\n",
                "\n",
                "test_loader = DataLoader(\n",
                "    test_dataset,\n",
                "    batch_size=batch_size,\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "print(f\"Number of train batches: {len(train_loader)}\")\n",
                "print(f\"Number of test batches: {len(test_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.4 데이터 시각화"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 샘플 이미지 보기\n",
                "examples = iter(train_loader)\n",
                "example_data, example_targets = next(examples)\n",
                "\n",
                "print(f\"Batch shape: {example_data.shape}\")  # (64, 1, 28, 28)\n",
                "print(f\"Labels: {example_targets[:10]}\")\n",
                "\n",
                "# 시각화\n",
                "fig = plt.figure(figsize=(12, 6))\n",
                "for i in range(12):\n",
                "    plt.subplot(3, 4, i+1)\n",
                "    plt.imshow(example_data[i][0], cmap='gray')\n",
                "    plt.title(f'Label: {example_targets[i]}')\n",
                "    plt.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. CNN 모델 정의\n",
                "\n",
                "### Architecture\n",
                "```\n",
                "Input (1, 28, 28)\n",
                "  ↓ Conv1 (32 filters, 3x3)\n",
                "  ↓ ReLU\n",
                "  ↓ MaxPool (2x2)\n",
                "  ↓ Conv2 (64 filters, 3x3)\n",
                "  ↓ ReLU\n",
                "  ↓ MaxPool (2x2)\n",
                "  ↓ Flatten\n",
                "  ↓ FC1 (128)\n",
                "  ↓ ReLU\n",
                "  ↓ Dropout\n",
                "  ↓ FC2 (10)\n",
                "Output (10 classes)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CNN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(CNN, self).__init__()\n",
                "        \n",
                "        # Convolutional layers\n",
                "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
                "        \n",
                "        # Pooling\n",
                "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        \n",
                "        # Fully connected layers\n",
                "        # After 2 pools: 28 → 14 → 7\n",
                "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
                "        self.fc2 = nn.Linear(128, 10)\n",
                "        \n",
                "        # Dropout (regularization)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # Conv block 1\n",
                "        x = self.conv1(x)  # (batch, 1, 28, 28) → (batch, 32, 28, 28)\n",
                "        x = F.relu(x)\n",
                "        x = self.pool(x)  # (batch, 32, 28, 28) → (batch, 32, 14, 14)\n",
                "        \n",
                "        # Conv block 2\n",
                "        x = self.conv2(x)  # (batch, 32, 14, 14) → (batch, 64, 14, 14)\n",
                "        x = F.relu(x)\n",
                "        x = self.pool(x)  # (batch, 64, 14, 14) → (batch, 64, 7, 7)\n",
                "        \n",
                "        # Flatten\n",
                "        x = x.view(-1, 64 * 7 * 7)  # (batch, 64, 7, 7) → (batch, 3136)\n",
                "        \n",
                "        # FC layers\n",
                "        x = self.fc1(x)  # (batch, 3136) → (batch, 128)\n",
                "        x = F.relu(x)\n",
                "        x = self.dropout(x)  # 훈련 시에만 적용\n",
                "        x = self.fc2(x)  # (batch, 128) → (batch, 10)\n",
                "        \n",
                "        return x  # Logits (softmax 전)\n",
                "\n",
                "# 모델 생성\n",
                "model = CNN().to(device)\n",
                "print(model)\n",
                "\n",
                "# 파라미터 수\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"\\nTotal params: {total_params:,}\")\n",
                "print(f\"Trainable params: {trainable_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Loss & Optimizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "# Learning rate scheduler (선택)\n",
                "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
                "\n",
                "print(\"Loss function:\", criterion)\n",
                "print(\"Optimizer:\", optimizer)\n",
                "print(\"Scheduler:\", scheduler)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. 훈련 함수"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, dataloader, criterion, optimizer,device):\n",
                "    \"\"\"1 epoch 훈련\"\"\"\n",
                "    model.train()  # 훈련 모드\n",
                "    \n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    pbar = tqdm(dataloader, desc=\"Training\")\n",
                "    for images, labels in pbar:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        # Forward\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        \n",
                "        # Backward\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        # 통계\n",
                "        running_loss += loss.item() * images.size(0)\n",
                "        _, predicted = torch.max(outputs, 1)\n",
                "        total += labels.size(0)\n",
                "        correct += (predicted == labels).sum().item()\n",
                "        \n",
                "        # Progress bar 업데이트\n",
                "        pbar.set_postfix({'loss': loss.item(), 'acc': correct/total})\n",
                "    \n",
                "    epoch_loss = running_loss / total\n",
                "    epoch_acc = correct / total\n",
                "    \n",
                "    return epoch_loss, epoch_acc"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 평가 함수"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate(model, dataloader, criterion, device):\n",
                "    \"\"\"모델 평가\"\"\"\n",
                "    model.eval()  # 평가 모드 (Dropout off)\n",
                "    \n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    with torch.no_grad():  # Gradient 계산 안 함\n",
                "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            \n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            running_loss += loss.item() * images.size(0)\n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "    \n",
                "    epoch_loss = running_loss / total\n",
                "    epoch_acc = correct / total\n",
                "    \n",
                "    return epoch_loss, epoch_acc"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 훈련 실행"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_epochs = 10\n",
                "\n",
                "train_losses, train_accs = [], []\n",
                "test_losses, test_accs = [], []\n",
                "\n",
                "for epoch in range(num_epochs):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
                "    print('='*60)\n",
                "    \n",
                "    # 훈련\n",
                "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
                "    train_losses.append(train_loss)\n",
                "    train_accs.append(train_acc)\n",
                "    \n",
                "    # 평가\n",
                "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
                "    test_losses.append(test_loss)\n",
                "    test_accs.append(test_acc)\n",
                "    \n",
                "    # Scheduler step\n",
                "    scheduler.step()\n",
                "    current_lr = optimizer.param_groups[0]['lr']\n",
                "    \n",
                "    print(f\"\\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
                "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
                "    print(f\"Learning Rate: {current_lr}\")\n",
                "\n",
                "print(\"\\n훈련 완료!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. 결과 시각화"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Loss curves\n",
                "axes[0].plot(train_losses, label='Train Loss', marker='o')\n",
                "axes[0].plot(test_losses, label='Test Loss', marker='s')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Training and Test Loss')\n",
                "axes[0].legend()\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Accuracy curves\n",
                "axes[1].plot(train_accs, label='Train Acc', marker='o')\n",
                "axes[1].plot(test_accs, label='Test Acc', marker='s')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy')\n",
                "axes[1].set_title('Training and Test Accuracy')\n",
                "axes[1].legend()\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nFinal Test Accuracy: {test_accs[-1]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. 예측 예시"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 테스트 이미지로 예측\n",
                "model.eval()\n",
                "\n",
                "test_examples = iter(test_loader)\n",
                "test_images, test_labels = next(test_examples)\n",
                "test_images = test_images.to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    test_outputs = model(test_images)\n",
                "    _, test_predicted = torch.max(test_outputs, 1)\n",
                "\n",
                "# 시각화\n",
                "fig = plt.figure(figsize=(15, 8))\n",
                "for i in range(12):\n",
                "    plt.subplot(3, 4, i+1)\n",
                "    img = test_images[i].cpu()[0]\n",
                "    plt.imshow(img, cmap='gray')\n",
                "    \n",
                "    true_label = test_labels[i].item()\n",
                "    pred_label = test_predicted[i].item()\n",
                "    \n",
                "    color = 'green' if true_label == pred_label else 'red'\n",
                "    plt.title(f'True: {true_label}, Pred: {pred_label}', color=color)\n",
                "    plt.axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "import seaborn as sns\n",
                "\n",
                "# 전체 테스트 데이터 예측\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    for images, labels in test_loader:\n",
                "        images = images.to(device)\n",
                "        outputs = model(images)\n",
                "        _, predicted = torch.max(outputs, 1)\n",
                "        \n",
                "        all_preds.extend(predicted.cpu().numpy())\n",
                "        all_labels.extend(labels.numpy())\n",
                "\n",
                "# Confusion matrix\n",
                "cm = confusion_matrix(all_labels, all_preds)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('True')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.show()\n",
                "\n",
                "# Per-class accuracy\n",
                "print(\"\\nPer-class Accuracy:\")\n",
                "for i in range(10):\n",
                "    class_acc = cm[i, i] / cm[i, :].sum()\n",
                "    print(f\"Class {i}: {class_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. 모델 저장"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 모델 저장\n",
                "checkpoint = {\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'optimizer_state_dict': optimizer.state_dict(),\n",
                "    'epoch': num_epochs,\n",
                "    'test_acc': test_accs[-1],\n",
                "}\n",
                "\n",
                "torch.save(checkpoint, 'mnist_cnn.pth')\n",
                "print(\"Model saved to mnist_cnn.pth\")\n",
                "\n",
                "# 모델 로드 예시\n",
                "# loaded_model = CNN().to(device)\n",
                "# checkpoint = torch.load('mnist_cnn.pth')\n",
                "# loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
                "# loaded_model.eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 요약\n",
                "\n",
                "### 프로젝트 구조\n",
                "1. ✅ **데이터 준비**: MNIST dataset, DataLoader\n",
                "2. ✅ **전처리**: Transforms (ToTensor, Normalize)\n",
                "3. ✅ **모델 정의**: CNN (Conv → Pool → FC)\n",
                "4. ✅ **훈련 루프**: train_epoch(), evaluate()\n",
                "5. ✅ **평가**: Accuracy, Confusion Matrix\n",
                "6. ✅ **시각화**: Loss/Accuracy curves, 예측 결과\n",
                "7. ✅ **모델 저장**: checkpoint 방식\n",
                "\n",
                "### AI 연구원/엔지니어로서 알아야 할 것\n",
                "- **Dataset/DataLoader**: 효율적 데이터 로딩\n",
                "- **Transforms**: 데이터 augmentation\n",
                "- **CNN architecture**: 이미지 처리의 기본\n",
                "- **Training loop**: 표준 패턴\n",
                "- **Evaluation**: Metrics, visualization\n",
                "- **Model checkpoint**: 실험 관리\n",
                "\n",
                "### 추가 학습 주제\n",
                "- Data augmentation (RandomCrop, RandomHorizontalFlip)\n",
                "- Batch Normalization\n",
                "- Residual connections (ResNet)\n",
                "- Transfer learning\n",
                "- Mixed precision training\n",
                "- Distributed training"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}