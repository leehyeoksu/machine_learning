{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 01. PyTorch 기초 - Tensor 완전 정복\n",
                "\n",
                "## 학습 목표\n",
                "- Tensor의 개념과 NumPy array와의 차이\n",
                "- Tensor 생성, 연산, 변형\n",
                "- GPU 사용 및 메모리 관리\n",
                "- Broadcasting과 Indexing\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Tensor 생성\n",
                "\n",
                "**Tensor**: PyTorch의 기본 데이터 구조. N차원 배열.\n",
                "\n",
                "### NumPy와의 차이\n",
                "1. **GPU 지원**: Tensor는 GPU에서 연산 가능\n",
                "2. **Autograd**: 자동 미분 지원 (딥러닝의 핵심!)\n",
                "3. **최적화**: CUDA, CuDNN 등 GPU 최적화"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.1 직접 생성\n",
                "x = torch.tensor([[1, 2], [3, 4]])\n",
                "print(\"Direct creation:\")\n",
                "print(x)\n",
                "print(f\"Shape: {x.shape}, dtype: {x.dtype}, device: {x.device}\")\n",
                "\n",
                "# 1.2 특수 Tensor\n",
                "zeros = torch.zeros(2, 3)\n",
                "ones = torch.ones(2, 3)\n",
                "eyes = torch.eye(3)  # Identity matrix\n",
                "rand = torch.rand(2, 3)  # Uniform [0, 1)\n",
                "randn = torch.randn(2, 3)  # Normal(0, 1)\n",
                "\n",
                "print(\"\\nZeros:\")\n",
                "print(zeros)\n",
                "print(\"\\nRandom (uniform):\")\n",
                "print(rand)\n",
                "\n",
                "# 1.3 NumPy에서 변환\n",
                "arr = np.array([[5, 6], [7, 8]])\n",
                "from_numpy = torch.from_numpy(arr)\n",
                "print(\"\\nFrom NumPy:\")\n",
                "print(from_numpy)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Tensor 연산\n",
                "\n",
                "### 2.1 기본 연산 (Element-wise)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "a = torch.tensor([1.0, 2.0, 3.0])\n",
                "b = torch.tensor([4.0, 5.0, 6.0])\n",
                "\n",
                "# 덧셈\n",
                "print(\"a + b =\", a + b)\n",
                "print(\"torch.add(a, b) =\", torch.add(a, b))\n",
                "\n",
                "# 곱셈 (element-wise)\n",
                "print(\"\\na * b =\", a * b)\n",
                "\n",
                "# 제곱\n",
                "print(\"\\na ** 2 =\", a ** 2)\n",
                "\n",
                "# 활성화 함수\n",
                "print(\"\\ntorch.sigmoid(a) =\", torch.sigmoid(a))\n",
                "print(\"torch.relu(a) =\", torch.relu(a))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 행렬 연산"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = torch.randn(3, 4)\n",
                "B = torch.randn(4, 2)\n",
                "\n",
                "# 행렬 곱 (Matrix multiplication)\n",
                "C = torch.matmul(A, B)  # 또는 A @ B\n",
                "print(f\"A shape: {A.shape}, B shape: {B.shape}\")\n",
                "print(f\"C = A @ B, shape: {C.shape}\")\n",
                "\n",
                "# Transpose\n",
                "print(f\"\\nA.T shape: {A.T.shape}\")\n",
                "\n",
                "# Dot product (벡터의 내적)\n",
                "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
                "v2 = torch.tensor([4.0, 5.0, 6.0])\n",
                "dot = torch.dot(v1, v2)\n",
                "print(f\"\\nv1 · v2 = {dot}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Reduction 연산"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.randn(3, 4)\n",
                "print(\"x:\")\n",
                "print(x)\n",
                "\n",
                "# 전체 합\n",
                "print(f\"\\nSum (all): {x.sum()}\")\n",
                "\n",
                "# 차원별 합\n",
                "print(f\"Sum (dim=0): {x.sum(dim=0)}\")  # 각 열의 합\n",
                "print(f\"Sum (dim=1): {x.sum(dim=1)}\")  # 각 행의 합\n",
                "\n",
                "# 평균, 최대, 최소\n",
                "print(f\"\\nMean: {x.mean()}\")\n",
                "print(f\"Max: {x.max()}\")\n",
                "print(f\"Argmax (전체): {x.argmax()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Tensor 변형 (Reshaping)\n",
                "\n",
                "**중요**: 딥러닝에서 데이터 크기 변경은 매우 빈번!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
                "print(\"Original:\", x)\n",
                "\n",
                "# Reshape\n",
                "x_reshaped = x.view(3, 4)\n",
                "print(\"\\nview(3, 4):\")\n",
                "print(x_reshaped)\n",
                "\n",
                "# -1: 자동 계산\n",
                "x_auto = x.view(2, -1)  # 2 rows, auto columns\n",
                "print(\"\\nview(2, -1):\")\n",
                "print(x_auto)\n",
                "\n",
                "# Flatten\n",
                "x_flat = x_reshaped.flatten()\n",
                "print(\"\\nFlattened:\", x_flat)\n",
                "\n",
                "# Squeeze & Unsqueeze (차원 제거/추가)\n",
                "x = torch.randn(1, 3, 1, 4)\n",
                "print(f\"\\nOriginal shape: {x.shape}\")\n",
                "print(f\"After squeeze: {x.squeeze().shape}\")  # (3, 4)\n",
                "print(f\"After unsqueeze(0): {x.squeeze().unsqueeze(0).shape}\")  # (1, 3, 4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Indexing & Slicing\n",
                "\n",
                "NumPy와 거의 동일!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.arange(20).reshape(4, 5)\n",
                "print(\"x:\")\n",
                "print(x)\n",
                "\n",
                "# 단일 요소\n",
                "print(f\"\\nx[0, 0] = {x[0, 0]}\")\n",
                "\n",
                "# 행 선택\n",
                "print(f\"\\nx[1] = {x[1]}\")\n",
                "\n",
                "# 열 선택\n",
                "print(f\"\\nx[:, 2] = {x[:, 2]}\")\n",
                "\n",
                "# Slicing\n",
                "print(f\"\\nx[1:3, 2:4] =\\n{x[1:3, 2:4]}\")\n",
                "\n",
                "# Boolean indexing\n",
                "mask = x > 10\n",
                "print(f\"\\nx[x > 10] = {x[mask]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Broadcasting\n",
                "\n",
                "**Broadcasting**: 크기가 다른 Tensor끼리 연산할 때 자동으로 크기 맞춤\n",
                "\n",
                "**규칙**:\n",
                "1. 뒤에서부터 차원 비교\n",
                "2. 차원이 1이거나 같으면 OK\n",
                "3. 없는 차원은 1로 간주"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 예제 1: Scalar + Matrix\n",
                "x = torch.ones(3, 4)\n",
                "y = 10 \n",
                "print(\"x + 10:\")\n",
                "print(x + y)\n",
                "\n",
                "# 예제 2: Vector + Matrix\n",
                "x = torch.ones(3, 4)\n",
                "v = torch.tensor([1, 2, 3, 4])  # (4,)\n",
                "print(\"\\nx + v (broadcasting):\")\n",
                "print(x + v)  # v가 (3, 4)로 확장됨\n",
                "\n",
                "# 예제 3: Column vector + Matrix\n",
                "v_col = torch.tensor([[1], [2], [3]])  # (3, 1)\n",
                "print(\"\\nx + v_col:\")\n",
                "print(x + v_col)  # v_col이 (3, 4)로 확장\n",
                "\n",
                "# 시각화\n",
                "print(\"\\n=== Broadcasting 규칙 ===\")\n",
                "print(\"x.shape = (3, 4)\")\n",
                "print(\"v.shape = (4,) → (1, 4) → (3, 4)\")\n",
                "print(\"v_col.shape = (3, 1) → (3, 4)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. GPU 사용\n",
                "\n",
                "**CPU vs GPU**:\n",
                "- CPU: 순차 처리에 강함\n",
                "- GPU: 병렬 처리에 강함 (행렬 연산!)\n",
                "\n",
                "**주의**: GPU와 CPU 간 데이터 이동은 느림!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CPU Tensor\n",
                "x_cpu = torch.randn(3, 3)\n",
                "print(f\"CPU tensor device: {x_cpu.device}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    # GPU로 이동\n",
                "    x_gpu = x_cpu.to('cuda')  # 또는 .cuda()\n",
                "    print(f\"GPU tensor device: {x_gpu.device}\")\n",
                "    \n",
                "    # GPU에서 직접 생성\n",
                "    y_gpu = torch.randn(3, 3, device='cuda')\n",
                "    \n",
                "    # GPU에서 연산\n",
                "    z_gpu = x_gpu + y_gpu\n",
                "    print(f\"Result device: {z_gpu.device}\")\n",
                "    \n",
                "    # CPU로 다시 가져오기\n",
                "    z_cpu = z_gpu.cpu()  # 또는 .to('cpu')\n",
                "    print(f\"Back to CPU: {z_cpu.device}\")\n",
                "    \n",
                "    # 속도 비교\n",
                "    import time\n",
                "    \n",
                "    size = 5000\n",
                "    a_cpu = torch.randn(size, size)\n",
                "    b_cpu = torch.randn(size, size)\n",
                "    a_gpu = a_cpu.cuda()\n",
                "    b_gpu = b_cpu.cuda()\n",
                "    \n",
                "    # CPU\n",
                "    start = time.time()\n",
                "    c_cpu = a_cpu @ b_cpu\n",
                "    print(f\"\\nCPU time: {time.time() - start:.4f}s\")\n",
                "    \n",
                "    # GPU\n",
                "    torch.cuda.synchronize()  # GPU 연산 완료 대기\n",
                "    start = time.time()\n",
                "    c_gpu = a_gpu @ b_gpu\n",
                "    torch.cuda.synchronize()\n",
                "    print(f\"GPU time: {time.time() - start:.4f}s\")\n",
                "    \n",
                "else:\n",
                "    print(\"GPU not available\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. In-place 연산\n",
                "\n",
                "**In-place**: 원본 Tensor를 직접 수정 (메모리 절약)\n",
                "\n",
                "**주의**: Autograd와 함께 사용 시 조심!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.tensor([1.0, 2.0, 3.0])\n",
                "print(f\"Original x: {x}\")\n",
                "print(f\"ID: {id(x)}\")\n",
                "\n",
                "# 일반 연산 (새 Tensor 생성)\n",
                "y = x.add(10)\n",
                "print(f\"\\nAfter y = x.add(10):\")\n",
                "print(f\"x: {x}\")\n",
                "print(f\"y: {y}\")\n",
                "print(f\"x ID: {id(x)}, y ID: {id(y)}\")\n",
                "\n",
                "# In-place 연산 (x 직접 수정)\n",
                "x.add_(10)  # '_' suffix = in-place\n",
                "print(f\"\\nAfter x.add_(10):\")\n",
                "print(f\"x: {x}\")\n",
                "print(f\"ID: {id(x)}\")\n",
                "\n",
                "# 다른 in-place 연산들\n",
                "# x.mul_(2)    # x *= 2\n",
                "# x.zero_()    # x = 0\n",
                "# x.fill_(5)   # x = 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. 연습문제\n",
                "\n",
                "### 문제 1: Batch Matrix Multiplication\n",
                "배치 크기 32, 입력 차원 10, 출력 차원 5인 선형 변환을 구현하시오.\n",
                "\n",
                "$$Y = XW^T + b$$\n",
                "\n",
                "where X: (32, 10), W: (5, 10), b: (5,)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 문제 1 풀이\n",
                "batch_size = 32\n",
                "input_dim = 10\n",
                "output_dim = 5\n",
                "\n",
                "X = torch.randn(batch_size, input_dim)\n",
                "W = torch.randn(output_dim, input_dim)\n",
                "b = torch.randn(output_dim)\n",
                "\n",
                "# Y = XW^T + b\n",
                "Y = X @ W.T + b  # Broadcasting: b (5,) → (32, 5)\n",
                "\n",
                "print(f\"X shape: {X.shape}\")\n",
                "print(f\"W shape: {W.shape}\")\n",
                "print(f\"b shape: {b.shape}\")\n",
                "print(f\"Y shape: {Y.shape}\")\n",
                "print(f\"\\nExpected: (32, 5)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 문제 2: Softmax 구현\n",
                "\n",
                "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 문제 2 풀이\n",
                "def my_softmax(x, dim=-1):\n",
                "    \"\"\"Numerically stable softmax\"\"\"\n",
                "    # Subtract max for numerical stability\n",
                "    x_max = x.max(dim=dim, keepdim=True)[0]\n",
                "    x_exp = torch.exp(x - x_max)\n",
                "    return x_exp / x_exp.sum(dim=dim, keepdim=True)\n",
                "\n",
                "# Test\n",
                "logits = torch.randn(4, 10)  # 배치 4, 클래스 10\n",
                "probs_my = my_softmax(logits, dim=1)\n",
                "probs_torch = torch.softmax(logits, dim=1)\n",
                "\n",
                "print(\"My softmax:\")\n",
                "print(probs_my[0])\n",
                "print(f\"Sum: {probs_my[0].sum()}\")\n",
                "\n",
                "print(\"\\nPyTorch softmax:\")\n",
                "print(probs_torch[0])\n",
                "\n",
                "print(f\"\\nDifference: {(probs_my - probs_torch).abs().max()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. 요약\n",
                "\n",
                "### 핵심 개념\n",
                "1. **Tensor**: PyTorch의 기본 데이터 구조, N차원 배열\n",
                "2. **GPU 지원**: `.to(device)` or `.cuda()`\n",
                "3. **Broadcasting**: 자동 크기 맞춤\n",
                "4. **In-place**: `_` suffix (메모리 절약, 주의 필요)\n",
                "\n",
                "### 주요 함수\n",
                "- **생성**: `torch.tensor()`, `torch.zeros()`, `torch.randn()`\n",
                "- **연산**: `+`, `*`, `@`, `.matmul()`, `.dot()`\n",
                "- **변형**: `.view()`, `.reshape()`, `.transpose()`, `.squeeze()`\n",
                "- **Reduction**: `.sum()`, `.mean()`, `.max()`, `.argmax()`\n",
                "\n",
                "### 다음 단계\n",
                "- `02_autograd_and_gradients.ipynb` - 자동 미분의 핵심!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}