{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02. PyTorch 훈련 루프 완전 정복\n",
                "\n",
                "## 학습 목표\n",
                "- Autograd (자동 미분) 이해\n",
                "- Loss function과 Optimizer\n",
                "- 완전한 훈련 루프 구현\n",
                "- 검증 및 평가\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Autograd - 자동 미분\n",
                "\n",
                "**Autograd**: PyTorch의 핵심! Backpropagation을 자동으로 계산\n",
                "\n",
                "### 1.1 기본 사용"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# requires_grad=True: gradient 계산 활성화\n",
                "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
                "print(f\"x: {x}\")\n",
                "print(f\"x.requires_grad: {x.requires_grad}\")\n",
                "\n",
                "# Forward pass\n",
                "y = x ** 2  # y = x^2\n",
                "z = y.sum()  # z = Σx^2\n",
                "\n",
                "print(f\"\\ny: {y}\")\n",
                "print(f\"z: {z}\")\n",
                "\n",
                "# Backward pass (gradient 계산)\n",
                "z.backward()  # dz/dx 계산\n",
                "\n",
                "# Gradient 확인\n",
                "print(f\"\\ndz/dx = x.grad: {x.grad}\")\n",
                "print(f\"Expected (2x): {2 * x.data}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Computational Graph\n",
                "\n",
                "PyTorch는 연산의 **그래프**를 만들어 역전파합니다.\n",
                "\n",
                "```\n",
                "x → (square) → y → (sum) → z\n",
                "          ↓ backward ↓\n",
                "grad ← (2x) ← (1) ←\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 복잡한 예제\n",
                "x = torch.tensor(2.0, requires_grad=True)\n",
                "w = torch.tensor(3.0, requires_grad=True)\n",
                "b = torch.tensor(1.0, requires_grad=True)\n",
                "\n",
                "# Forward: y = wx + b\n",
                "y = w * x + b\n",
                "print(f\"y = wx + b = {y}\")\n",
                "\n",
                "# Backward\n",
                "y.backward()\n",
                "\n",
                "print(f\"\\ndy/dx = {x.grad}  (= w = {w.item()})\")\n",
                "print(f\"dy/dw = {w.grad}  (= x = {x.item()})\")\n",
                "print(f\"dy/db = {b.grad}  (= 1)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 Gradient Accumulation\n",
                "\n",
                "**주의**: `.backward()`를 여러 번 호출하면 gradient가 **누적**됩니다!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
                "\n",
                "# First backward\n",
                "y1 = (x ** 2).sum()\n",
                "y1.backward()\n",
                "print(f\"After 1st backward: {x.grad}\")\n",
                "\n",
                "# Second backward (누적!)\n",
                "y2 = (x ** 2).sum()\n",
                "y2.backward()\n",
                "print(f\"After 2nd backward (accumulated): {x.grad}\")\n",
                "\n",
                "# Gradient 초기화\n",
                "x.grad.zero_()\n",
                "print(f\"After .zero_(): {x.grad}\")\n",
                "\n",
                "# 다시 계산\n",
                "y3 = (x ** 2).sum()\n",
                "y3.backward()\n",
                "print(f\"After 3rd backward (reset): {x.grad}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 신경망 정의\n",
                "\n",
                "**`nn.Module`**: 모든 신경망의 부모 클래스"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleNN(nn.Module):\n",
                "    \"\"\"간단한 2-layer 신경망\n",
                "    \n",
                "    Architecture:\n",
                "        Input (input_dim) → Hidden (hidden_dim) → Output (output_dim)\n",
                "    \"\"\"\n",
                "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
                "        super(SimpleNN, self).__init__()  # 부모 클래스 초기화 (필수!)\n",
                "        \n",
                "        # Layers 정의\n",
                "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # W1: (input_dim, hidden_dim), b1: (hidden_dim,)\n",
                "        self.relu = nn.ReLU()  # 활성화 함수\n",
                "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # W2, b2\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"Forward pass\n",
                "        \n",
                "        Args:\n",
                "            x: (batch_size, input_dim)\n",
                "        \n",
                "        Returns:\n",
                "            out: (batch_size, output_dim)\n",
                "        \"\"\"\n",
                "        x = self.fc1(x)  # (batch, hidden_dim)\n",
                "        x = self.relu(x)  # Element-wise activation\n",
                "        x = self.fc2(x)  # (batch, output_dim)\n",
                "        return x\n",
                "\n",
                "# 모델 생성\n",
                "model = SimpleNN(input_dim=10, hidden_dim=20, output_dim=5)\n",
                "print(model)\n",
                "\n",
                "# Parameters 확인\n",
                "print(\"\\n=== Parameters ===\")\n",
                "for name, param in model.named_parameters():\n",
                "    print(f\"{name}: {param.shape}\")\n",
                "\n",
                "# Forward pass 테스트\n",
                "x_test = torch.randn(32, 10)  # 배치 32개\n",
                "output = model(x_test)\n",
                "print(f\"\\nOutput shape: {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Loss Function\n",
                "\n",
                "**Loss**: 예측과 실제 값의 차이를 측정\n",
                "\n",
                "### 주요 Loss Functions\n",
                "- **Classification**: `nn.CrossEntropyLoss()`, `nn.BCELoss()`\n",
                "- **Regression**: `nn.MSELoss()`, `nn.L1Loss()`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification example\n",
                "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "# 예측 (logits, softmax 전)\n",
                "logits = torch.randn(4, 5)  # 배치 4, 클래스 5\n",
                "# 정답 (class indices)\n",
                "targets = torch.tensor([0, 2, 1, 4])  # 정답 클래스\n",
                "\n",
                "loss = criterion(logits, targets)\n",
                "print(f\"Logits:\\n{logits}\")\n",
                "print(f\"\\nTargets: {targets}\")\n",
                "print(f\"\\nCross-Entropy Loss: {loss.item():.4f}\")\n",
                "\n",
                "# MSE example\n",
                "mse_criterion = nn.MSELoss()\n",
                "predictions = torch.randn(4, 1)\n",
                "targets_reg = torch.randn(4, 1)\n",
                "mse_loss = mse_criterion(predictions, targets_reg)\n",
                "print(f\"\\nMSE Loss: {mse_loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Optimizer\n",
                "\n",
                "**Optimizer**: Gradient를 사용하여 파라미터를 업데이트\n",
                "\n",
                "### 주요 Optimizers\n",
                "- **SGD**: Stochastic Gradient Descent\n",
                "- **Adam**: Adaptive Moment Estimation (가장 많이 사용)\n",
                "- **AdamW**: Adam + Weight Decay"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optimizer 생성\n",
                "learning_rate = 0.001\n",
                "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
                "\n",
                "print(\"=== Optimizer ===\")\n",
                "print(optimizer)\n",
                "print(f\"\\nLearning rate: {learning_rate}\")\n",
                "print(f\"Parameters to optimize: {sum(p.numel() for p in model.parameters())}\")\n",
                "\n",
                "# 다른 optimizer 예시\n",
                "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
                "# optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 완전한 훈련 루프\n",
                "\n",
                "### 훈련 1 epoch의 구조\n",
                "```python\n",
                "for epoch in range(num_epochs):\n",
                "    for batch in dataloader:\n",
                "        # 1. Forward pass\n",
                "        outputs = model(inputs)\n",
                "        loss = criterion(outputs, targets)\n",
                "        \n",
                "        # 2. Backward pass\n",
                "        optimizer.zero_grad()  # Gradient 초기화\n",
                "        loss.backward()  # Gradient 계산\n",
                "        \n",
                "        # 3. Update weights\n",
                "        optimizer.step()\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 간단한 데이터 생성 (binary classification)\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# 데이터 생성\n",
                "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# 정규화\n",
                "scaler = StandardScaler()\n",
                "X_train = scaler.fit_transform(X_train)\n",
                "X_test = scaler.transform(X_test)\n",
                "\n",
                "# Tensor 변환\n",
                "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
                "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
                "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
                "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
                "\n",
                "print(f\"Train: {X_train_tensor.shape}, Test: {X_test_tensor.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 모델, Loss, Optimizer 생성\n",
                "model = SimpleNN(input_dim=20, hidden_dim=64, output_dim=2).to(device)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "# 훈련 루프\n",
                "num_epochs = 100\n",
                "batch_size = 32\n",
                "train_losses = []\n",
                "test_accuracies = []\n",
                "\n",
                "from tqdm import tqdm\n",
                "\n",
                "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
                "    model.train()  # 훈련 모드 (Dropout, BatchNorm에 영향)\n",
                "    \n",
                "    # Mini-batch 훈련\n",
                "    epoch_loss = 0\n",
                "    n_batches = 0\n",
                "    \n",
                "    for i in range(0, len(X_train_tensor), batch_size):\n",
                "        # Batch 추출\n",
                "        X_batch = X_train_tensor[i:i+batch_size]\n",
                "        y_batch = y_train_tensor[i:i+batch_size]\n",
                "        \n",
                "        # 1. Forward\n",
                "        outputs = model(X_batch)\n",
                "        loss = criterion(outputs, y_batch)\n",
                "        \n",
                "        # 2. Backward\n",
                "        optimizer.zero_grad()  # 이전 gradient 제거\n",
                "        loss.backward()  # Gradient 계산\n",
                "        \n",
                "        # 3. Update\n",
                "        optimizer.step()  # 파라미터 업데이트\n",
                "        \n",
                "        epoch_loss += loss.item()\n",
                "        n_batches += 1\n",
                "    \n",
                "    # Epoch 평균 loss\n",
                "    avg_loss = epoch_loss / n_batches\n",
                "    train_losses.append(avg_loss)\n",
                "    \n",
                "    # Validation (test set)\n",
                "    model.eval()  # 평가 모드\n",
                "    with torch.no_grad():  # Gradient 계산 안 함 (메모리 절약)\n",
                "        test_outputs = model(X_test_tensor)\n",
                "        _, predicted = torch.max(test_outputs, 1)\n",
                "        accuracy = (predicted == y_test_tensor).float().mean().item()\n",
                "        test_accuracies.append(accuracy)\n",
                "    \n",
                "    # 10 epoch마다 출력\n",
                "    if (epoch + 1) % 10 == 0:\n",
                "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Test Acc: {accuracy:.4f}\")\n",
                "\n",
                "print(\"\\n훈련 완료!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 결과 시각화\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Loss curve\n",
                "axes[0].plot(train_losses, label='Train Loss')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Training Loss')\n",
                "axes[0].legend()\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Accuracy curve\n",
                "axes[1].plot(test_accuracies, label='Test Accuracy', color='orange')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy')\n",
                "axes[1].set_title('Test Accuracy')\n",
                "axes[1].legend()\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nFinal Test Accuracy: {test_accuracies[-1]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 모델 저장 및 로드\n",
                "\n",
                "### State Dict\n",
                "- 모델의 **파라미터**만 저장 (가벼움)\n",
                "- **권장 방법**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 모델 저장\n",
                "torch.save(model.state_dict(), 'model.pth')\n",
                "print(\"Model saved to model.pth\")\n",
                "\n",
                "# 모델 로드\n",
                "loaded_model = SimpleNN(input_dim=20, hidden_dim=64, output_dim=2).to(device)\n",
                "loaded_model.load_state_dict(torch.load('model.pth'))\n",
                "loaded_model.eval()\n",
                "print(\"Model loaded successfully\")\n",
                "\n",
                "# 검증\n",
                "with torch.no_grad():\n",
                "    loaded_outputs = loaded_model(X_test_tensor)\n",
                "    _, loaded_predicted = torch.max(loaded_outputs, 1)\n",
                "    loaded_accuracy = (loaded_predicted == y_test_tensor).float().mean().item()\n",
                "    \n",
                "print(f\"Loaded model accuracy: {loaded_accuracy:.4f}\")\n",
                "print(f\"Original model accuracy: {test_accuracies[-1]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. 요약\n",
                "\n",
                "### 훈련 루프 체크리스트\n",
                "1. ✅ 데이터 준비 (Tensor 변환)\n",
                "2. ✅ 모델 정의 (`nn.Module` 상속)\n",
                "3. ✅ Loss function 선택\n",
                "4. ✅ Optimizer 선택\n",
                "5. ✅ 훈련 루프:\n",
                "   - `optimizer.zero_grad()`\n",
                "   - Forward pass\n",
                "   - Loss 계산\n",
                "   - `loss.backward()`\n",
                "   - `optimizer.step()`\n",
                "6. ✅ 평가 (`model.eval()`, `torch.no_grad()`)\n",
                "7. ✅ 모델 저장/로드\n",
                "\n",
                "### 주의사항\n",
                "- **Gradient 초기화**: `optimizer.zero_grad()` 필수!\n",
                "- **평가 모드**: `model.eval()` + `torch.no_grad()`\n",
                "- **Device 일치**: 모델, 데이터 모두 같은 device\n",
                "\n",
                "### 다음 단계\n",
                "- `03_complete_project.ipynb` - 실전 프로젝트!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}