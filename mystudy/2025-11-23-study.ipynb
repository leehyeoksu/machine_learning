{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2025-11-23 ML 공부 정리 및 검증\n",
    "\n",
    "오늘 공부한 내용:\n",
    "1. Matrix Gradient (행렬 미분)\n",
    "2. SGD와 Backpropagation\n",
    "3. SVM (Hard/Soft Margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matrix Gradient 기초\n",
    "\n",
    "### 핵심 규칙\n",
    "- 스칼라 함수를 벡터/행렬로 미분할 때, **미분 결과의 shape은 미분 변수의 shape과 같아야 함**\n",
    "- `Ax` 형태에서:\n",
    "  - `A`는 행렬 (m×n)\n",
    "  - `x`는 벡터 (n×1)\n",
    "  - 결과: (m×1)\n",
    "\n",
    "### Case 1: Ax (A는 행, x는 열)\n",
    "- `∂(Ax)/∂A = x^T` ✓ (맞음)\n",
    "- `∂(Ax)/∂x = A^T` ✓ (맞음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SGD와 Loss Function 미분\n",
    "\n",
    "### MSE Loss\n",
    "$$L = \\sum (y - \\hat{y})^2 = \\sum (y - (wx + b))^2$$\n",
    "\n",
    "### ❌ 틀린 부분 수정\n",
    "**원래 작성:** `∂L/∂w = 2∑(ŷ-y)x^T`\n",
    "\n",
    "**올바른 식:** `∂L/∂w = -2∑(y-ŷ)x^T = 2∑(ŷ-y)x^T` ✓\n",
    "\n",
    "- w는 (1×n) 행벡터\n",
    "- x는 (n×1) 열벡터\n",
    "- ∂L/∂w의 shape은 (1×n)이어야 함\n",
    "- 따라서 x를 전치하여 x^T (1×n) 형태로 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (Cross Entropy)\n",
    "$$L = -\\sum [y\\log(\\sigma(wx+b)) + (1-y)\\log(1-\\sigma(wx+b))]$$\n",
    "\n",
    "**미분 결과:**\n",
    "$$\\frac{\\partial L}{\\partial w} = \\sum (\\sigma(wx+b) - y)x^T$$ ✓ (맞음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLP Backpropagation\n",
    "\n",
    "### 4층 신경망 (Input → Hidden1 → Hidden2 → Output)\n",
    "\n",
    "**Forward:**\n",
    "- z1 = w1·x\n",
    "- a1 = τ(z1)\n",
    "- z2 = w2·a1\n",
    "- a2 = τ(z2)\n",
    "- z3 = w3·a2\n",
    "- o = σ(z3)\n",
    "\n",
    "### ❌ 틀린 부분 수정\n",
    "\n",
    "**SGD (single sample) - 원래 작성:**\n",
    "- `∂L/∂w3 = (o-y)τ'(z3)z2^T`\n",
    "- `∂L/∂w2 = σ3·τ'(z2)z1^T`\n",
    "- `∂L/∂w1 = σ2·τ'(z1)x^T`\n",
    "\n",
    "**올바른 식:**\n",
    "- `∂L/∂w3 = (o-y)·σ'(z3)·a2^T` ✓ (output은 sigmoid)\n",
    "- `∂L/∂w2 = δ3·w3^T·τ'(z2)·a1^T` where δ3=(o-y)·σ'(z3)\n",
    "- `∂L/∂w1 = δ2·w2^T·τ'(z1)·x^T`\n",
    "\n",
    "**Softmax + Cross Entropy:**\n",
    "- `∂L/∂w3 = (o-y)·a2^T` ✓ (맞음, gradient는 단순화됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch/Full-batch Gradient\n",
    "\n",
    "**❌ 틀린 부분:**\n",
    "원래: `∂L/∂w3 = z2^T·(o-y)·τ'(z3)`\n",
    "\n",
    "**✓ 올바른 식:**\n",
    "- Batch size = N\n",
    "- A2: (N × hidden_dim) - 배치의 activation\n",
    "- O: (N × output_dim)\n",
    "- Y: (N × output_dim)\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_3} = A_2^T \\cdot (O - Y) \\cdot \\sigma'(Z_3)$$\n",
    "\n",
    "Shape: (hidden_dim × N) · (N × output_dim) = (hidden_dim × output_dim) ✓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM\n",
    "\n",
    "### Hard Margin\n",
    "\n",
    "**목적함수:**\n",
    "$$\\min_{w,b} \\frac{1}{2}\\|w\\|^2$$\n",
    "$$\\text{s.t. } y_i(w^Tx_i + b) \\geq 1$$\n",
    "\n",
    "### ❌ 개념 오류 수정\n",
    "\n",
    "**오해했던 부분:**\n",
    "> \"거리 d/||w||에서 d에 c를 곱해서 1로 만들면 w도 바뀌지만 1/||w||는 안바뀜\"\n",
    "\n",
    "**올바른 이해:**\n",
    "- Margin = 2/||w||\n",
    "- d = y(w^Tx + b) - 이것이 functional margin\n",
    "- Functional margin을 1로 normalize하는 것은 **scaling 자유도를 제거**하기 위함\n",
    "- w를 스케일링해도 결정 경계는 동일 (w, 2w, 3w 모두 같은 hyperplane)\n",
    "- **제약식을 ≥1로 고정**함으로써 unique solution 가능\n",
    "\n",
    "**Lagrangian:**\n",
    "$$L = \\frac{1}{2}\\|w\\|^2 - \\sum_i \\lambda_i[y_i(w^Tx_i+b) - 1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin SVM\n",
    "\n",
    "**Slack variable ξ:**\n",
    "\n",
    "**목적함수:**\n",
    "$$\\min_{w,b,\\xi} \\frac{1}{2}\\|w\\|^2 + C\\sum_i \\xi_i$$\n",
    "\n",
    "**제약:**\n",
    "$$y_i(w^Tx_i + b) \\geq 1 - \\xi_i$$\n",
    "$$\\xi_i \\geq 0$$\n",
    "\n",
    "### Slack Variable 해석 ✓ (맞음)\n",
    "- **ξ = 0:** 정확히 분류됨, margin 밖\n",
    "- **0 < ξ < 1:** margin 안에 있지만 올바른 쪽\n",
    "- **ξ > 1:** 잘못 분류됨 (hyperplane 넘어감)\n",
    "\n",
    "### C 파라미터 ✓ (맞음)\n",
    "- **C가 클수록:** 오분류 페널티 큼 → margin 좁음, overfitting 위험\n",
    "- **C가 작을수록:** 오분류 허용 → margin 넓음, underfitting 가능\n",
    "\n",
    "### ❌ 표현 수정\n",
    "원래: \"SVS가 좁게/넓게 잡힘\"\n",
    "올바름: \"Support Vectors (SVs)가 적어짐/많아짐, Margin이 좁아짐/넓어짐\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습 문제\n",
    "\n",
    "### 문제 1: Matrix Gradient\n",
    "다음을 증명하시오.\n",
    "\n",
    "$$f(x) = x^TAx \\text{ (A는 대칭 행렬)}$$\n",
    "$$\\frac{\\partial f}{\\partial x} = 2Ax$$\n",
    "\n",
    "**Hint:** Chain rule 사용, x^TAx를 전개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# 수치적 검증\n",
    "A = np.array([[2, 1], [1, 3]])\n",
    "x = np.array([[1], [2]])\n",
    "\n",
    "# f(x) = x^T A x\n",
    "f = x.T @ A @ x\n",
    "print(f\"f(x) = {f[0,0]}\")\n",
    "\n",
    "# Gradient = 2Ax\n",
    "grad = 2 * A @ x\n",
    "print(f\"∇f = \\n{grad}\")\n",
    "\n",
    "# 수치 미분으로 검증\n",
    "eps = 1e-5\n",
    "numerical_grad = np.zeros_like(x, dtype=float)\n",
    "for i in range(len(x)):\n",
    "    x_plus = x.copy().astype(float)\n",
    "    x_plus[i] += eps\n",
    "    x_minus = x.copy().astype(float)\n",
    "    x_minus[i] -= eps\n",
    "    numerical_grad[i] = ((x_plus.T @ A @ x_plus) - (x_minus.T @ A @ x_minus)) / (2*eps)\n",
    "\n",
    "print(f\"\\n수치 미분 결과:\\n{numerical_grad}\")\n",
    "print(f\"\\n오차: {np.max(np.abs(grad - numerical_grad))}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2: Backpropagation\n",
    "\n",
    "2층 신경망이 다음과 같이 주어졌을 때:\n",
    "- Input: x = [1, 2]\n",
    "- w1 = [[0.5, 0.3], [0.2, 0.4]] (2×2)\n",
    "- w2 = [[0.6], [0.7]] (2×1)\n",
    "- Activation: ReLU\n",
    "- Loss: MSE, y = 1\n",
    "\n",
    "∂L/∂w1과 ∂L/∂w2를 계산하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Forward pass\n",
    "x = np.array([[1], [2]])\n",
    "w1 = np.array([[0.5, 0.3], [0.2, 0.4]])\n",
    "w2 = np.array([[0.6], [0.7]])\n",
    "y = 1\n",
    "\n",
    "# Forward\n",
    "z1 = w1 @ x\n",
    "a1 = np.maximum(0, z1)  # ReLU\n",
    "z2 = w2.T @ a1\n",
    "output = z2[0, 0]\n",
    "\n",
    "print(f\"z1 = \\n{z1}\")\n",
    "print(f\"a1 = \\n{a1}\")\n",
    "print(f\"output = {output}\")\n",
    "\n",
    "# Loss\n",
    "loss = 0.5 * (output - y)**2\n",
    "print(f\"Loss = {loss}\")\n",
    "\n",
    "# Backward\n",
    "# ∂L/∂z2\n",
    "dL_dz2 = output - y\n",
    "print(f\"\\n∂L/∂z2 = {dL_dz2}\")\n",
    "\n",
    "# ∂L/∂w2 = a1 * ∂L/∂z2\n",
    "dL_dw2 = a1 * dL_dz2\n",
    "print(f\"∂L/∂w2 = \\n{dL_dw2}\")\n",
    "\n",
    "# ∂L/∂a1 = w2 * ∂L/∂z2\n",
    "dL_da1 = w2 * dL_dz2\n",
    "\n",
    "# ∂L/∂z1 = ∂L/∂a1 * ReLU'(z1)\n",
    "dL_dz1 = dL_da1 * (z1 > 0)\n",
    "print(f\"∂L/∂z1 = \\n{dL_dz1}\")\n",
    "\n",
    "# ∂L/∂w1 = ∂L/∂z1 @ x^T\n",
    "dL_dw1 = dL_dz1 @ x.T\n",
    "print(f\"∂L/∂w1 = \\n{dL_dw1}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3: SVM Margin 계산\n",
    "\n",
    "다음 데이터가 주어졌을 때:\n",
    "- Class +1: (2, 3), (3, 3)\n",
    "- Class -1: (1, 1), (2, 1)\n",
    "\n",
    "결정 경계가 w = [1, 1], b = -3일 때:\n",
    "1. Margin 폭을 계산하시오\n",
    "2. Support Vector를 찾으시오\n",
    "3. Hard margin 조건을 만족하는지 확인하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 데이터\n",
    "X_pos = np.array([[2, 3], [3, 3]])\n",
    "X_neg = np.array([[1, 1], [2, 1]])\n",
    "w = np.array([1, 1])\n",
    "b = -3\n",
    "\n",
    "# 1. Margin 계산\n",
    "margin = 2 / np.linalg.norm(w)\n",
    "print(f\"Margin = 2/||w|| = {margin:.4f}\")\n",
    "\n",
    "# 2. 각 점의 functional margin 계산\n",
    "print(\"\\n각 점의 y(w^Tx + b):\")\n",
    "for i, x in enumerate(X_pos):\n",
    "    val = w @ x + b\n",
    "    print(f\"  +1 class, point {i+1}: {val:.2f}\")\n",
    "    \n",
    "for i, x in enumerate(X_neg):\n",
    "    val = -(w @ x + b)  # y = -1\n",
    "    print(f\"  -1 class, point {i+1}: {val:.2f}\")\n",
    "\n",
    "# 3. Support Vector 찾기 (margin = 1인 점들)\n",
    "print(\"\\nSupport Vectors (y(w^Tx + b) = 1):\")\n",
    "for i, x in enumerate(X_pos):\n",
    "    if np.abs(w @ x + b - 1) < 0.01:\n",
    "        print(f\"  {x}\")\n",
    "for i, x in enumerate(X_neg):\n",
    "    if np.abs(-(w @ x + b) - 1) < 0.01:\n",
    "        print(f\"  {x}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 4: Soft Margin과 C 파라미터\n",
    "\n",
    "다음 상황을 분석하시오:\n",
    "- 어떤 점의 slack variable ξ = 0.5\n",
    "- C = 1일 때와 C = 10일 때 이 점이 목적함수에 미치는 영향 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "xi = 0.5\n",
    "\n",
    "# C = 1\n",
    "penalty_C1 = 1 * xi\n",
    "print(f\"C=1일 때 페널티: {penalty_C1}\")\n",
    "\n",
    "# C = 10  \n",
    "penalty_C10 = 10 * xi\n",
    "print(f\"C=10일 때 페널티: {penalty_C10}\")\n",
    "\n",
    "print(f\"\\n페널티 비율: {penalty_C10/penalty_C1}배\")\n",
    "print(\"\\n해석:\")\n",
    "print(\"- C가 클수록 오분류/margin 위반에 대한 페널티가 커짐\")\n",
    "print(\"- C=10: 더 엄격하게 분류, margin 좁음, overfitting 위험\")\n",
    "print(\"- C=1: 여유있게 분류, margin 넓음, generalization 좋음\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### 오늘 수정한 주요 오류:\n",
    "1. ✅ Backpropagation에서 활성화 함수 미분 누락 수정\n",
    "2. ✅ Batch gradient의 행렬 곱셈 순서 수정\n",
    "3. ✅ SVM에서 거리/margin 개념 명확화\n",
    "4. ✅ C 파라미터의 역할 정확히 이해\n",
    "\n",
    "### 핵심 원칙:\n",
    "- **행렬 미분**: 결과의 shape = 미분 변수의 shape\n",
    "- **Transpose**: 차원 맞추기 위해 사용\n",
    "- **Backprop**: Chain rule 철저히 적용\n",
    "- **SVM**: Margin 최대화 = ||w|| 최소화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
