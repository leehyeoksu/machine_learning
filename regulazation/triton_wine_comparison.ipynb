{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WSL Ubuntu에서 Triton vs PyTorch 비교 (Wine 데이터셋)\n",
                "\n",
                "## 목표\n",
                "- PyTorch 기본 학습 vs Triton 커널 직접 제어\n",
                "- CPU 환경에서 병렬 처리 구조 이해\n",
                "- Wine 데이터셋으로 실전 비교"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 환경 설정"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ Triton 버전: 3.5.1\n",
                        "PyTorch: 2.9.0+cpu\n",
                        "Device: CPU\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import time\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from dataset_wine import get_wine_loaders, set_seed\n",
                "\n",
                "# Triton import\n",
                "try:\n",
                "    import triton\n",
                "    import triton.language as tl\n",
                "    TRITON_AVAILABLE = True\n",
                "    print(f\"✅ Triton 버전: {triton.__version__}\")\n",
                "except ImportError:\n",
                "    TRITON_AVAILABLE = False\n",
                "    print(\"⚠️ Triton 미설치. 설치: pip install triton\")\n",
                "\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
                "\n",
                "set_seed(42)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "입력 차원: 13, 클래스 수: 3\n"
                    ]
                }
            ],
            "source": [
                "# 데이터 로드\n",
                "train_loader, val_loader, test_loader, input_dim, num_classes = get_wine_loaders(batch_size=64)\n",
                "print(f\"입력 차원: {input_dim}, 클래스 수: {num_classes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 모델 및 함수 정의"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MLP(nn.Module):\n",
                "    def __init__(self, input_dim, num_classes, h1=64, h2=32):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(input_dim, h1), nn.ReLU(),\n",
                "            nn.Linear(h1, h2), nn.ReLU(),\n",
                "            nn.Linear(h2, num_classes)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "def accuracy(model, loader):\n",
                "    model.eval()\n",
                "    correct = total = 0\n",
                "    with torch.no_grad():\n",
                "        for x, y in loader:\n",
                "            x, y = x.to(device), y.to(device)\n",
                "            pred = model(x).argmax(dim=1)\n",
                "            correct += (pred == y).sum().item()\n",
                "            total += y.size(0)\n",
                "    return correct / total"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. 사진 전: PyTorch 기본 학습 (L2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 10: val_acc=0.9722\n",
                        "Epoch 20: val_acc=0.9722\n",
                        "Epoch 30: val_acc=0.9722\n",
                        "Epoch 40: val_acc=0.9722\n",
                        "Epoch 50: val_acc=0.9722\n",
                        "\n",
                        "✅ PyTorch 결과: Val=0.9722, Test=0.9722, Time=0.30s\n"
                    ]
                }
            ],
            "source": [
                "def train_pytorch(model, epochs=50, lr=1e-3, weight_decay=1e-4):\n",
                "    \"\"\"PyTorch 기본 학습 (L2 정규화)\"\"\"\n",
                "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
                "    ce = nn.CrossEntropyLoss()\n",
                "    best_val = 0.0\n",
                "    \n",
                "    start = time.time()\n",
                "    for ep in range(1, epochs+1):\n",
                "        model.train()\n",
                "        for x, y in train_loader:\n",
                "            x, y = x.to(device), y.to(device)\n",
                "            opt.zero_grad()\n",
                "            loss = ce(model(x), y)\n",
                "            loss.backward()\n",
                "            opt.step()\n",
                "        \n",
                "        val_acc = accuracy(model, val_loader)\n",
                "        best_val = max(best_val, val_acc)\n",
                "        if ep % 10 == 0:\n",
                "            print(f\"Epoch {ep}: val_acc={val_acc:.4f}\")\n",
                "    \n",
                "    train_time = time.time() - start\n",
                "    test_acc = accuracy(model, test_loader)\n",
                "    return best_val, test_acc, train_time\n",
                "\n",
                "model_pt = MLP(input_dim, num_classes).to(device)\n",
                "val_pt, test_pt, time_pt = train_pytorch(model_pt)\n",
                "\n",
                "print(f\"\\n✅ PyTorch 결과: Val={val_pt:.4f}, Test={test_pt:.4f}, Time={time_pt:.2f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. 사진 후: Triton 커널 구현"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "ename": "RuntimeError",
                    "evalue": "0 active drivers ([]). There should only be one.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m test_b = torch.arange(\u001b[32m1000\u001b[39m, dtype=torch.float32)\n\u001b[32m     23\u001b[39m pt_result = test_a + test_b\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m tr_result = \u001b[43mtriton_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Triton 커널 테스트:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  정확도 일치: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.allclose(pt_result,\u001b[38;5;250m \u001b[39mtr_result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtriton_add\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m     14\u001b[39m n = a.numel()\n\u001b[32m     15\u001b[39m grid = \u001b[38;5;28;01mlambda\u001b[39;00m meta: (triton.cdiv(n, meta[\u001b[33m'\u001b[39m\u001b[33mBLOCK_SIZE\u001b[39m\u001b[33m'\u001b[39m]),)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43madd_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ml/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:419\u001b[39m, in \u001b[36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) -> T:\n\u001b[32m    414\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[33;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    memorizes the grid.\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ml/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:713\u001b[39m, in \u001b[36mJITFunction.run\u001b[39m\u001b[34m(self, grid, warmup, *args, **kwargs)\u001b[39m\n\u001b[32m    710\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mdebug\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdebug\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.debug) \u001b[38;5;129;01mor\u001b[39;00m knobs.runtime.debug\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# parse options\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m device = \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactive\u001b[49m.get_current_device()\n\u001b[32m    714\u001b[39m stream = driver.active.get_current_stream(device)\n\u001b[32m    716\u001b[39m \u001b[38;5;66;03m# Execute pre run hooks with args and kwargs\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ml/.venv/lib/python3.12/site-packages/triton/runtime/driver.py:28\u001b[39m, in \u001b[36mDriverConfig.active\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mactive\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DriverBase:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._active \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28mself\u001b[39m._active = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._active\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ml/.venv/lib/python3.12/site-packages/triton/runtime/driver.py:22\u001b[39m, in \u001b[36mDriverConfig.default\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DriverBase:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[38;5;28mself\u001b[39m._default = \u001b[43m_create_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._default\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ml/.venv/lib/python3.12/site-packages/triton/runtime/driver.py:9\u001b[39m, in \u001b[36m_create_driver\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m active_drivers = [x.driver \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m backends.values() \u001b[38;5;28;01mif\u001b[39;00m x.driver.is_active()]\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(active_drivers) != \u001b[32m1\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(active_drivers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m active drivers (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactive_drivers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). There should only be one.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m active_drivers[\u001b[32m0\u001b[39m]()\n",
                        "\u001b[31mRuntimeError\u001b[39m: 0 active drivers ([]). There should only be one."
                    ]
                }
            ],
            "source": [
                "if TRITON_AVAILABLE:\n",
                "    @triton.jit\n",
                "    def add_kernel(a_ptr, b_ptr, c_ptr, n, BLOCK_SIZE: tl.constexpr):\n",
                "        \"\"\"간단한 벡터 덧셈 커널 (예시)\"\"\"\n",
                "        pid = tl.program_id(0)\n",
                "        offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
                "        mask = offs < n\n",
                "        a = tl.load(a_ptr + offs, mask=mask)\n",
                "        b = tl.load(b_ptr + offs, mask=mask)\n",
                "        tl.store(c_ptr + offs, a + b, mask=mask)\n",
                "    \n",
                "    def triton_add(a, b):\n",
                "        c = torch.empty_like(a)\n",
                "        n = a.numel()\n",
                "        grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)\n",
                "        add_kernel[grid](a, b, c, n, BLOCK_SIZE=256)\n",
                "        return c\n",
                "    \n",
                "    # 테스트\n",
                "    test_a = torch.arange(1000, dtype=torch.float32)\n",
                "    test_b = torch.arange(1000, dtype=torch.float32)\n",
                "    \n",
                "    pt_result = test_a + test_b\n",
                "    tr_result = triton_add(test_a, test_b)\n",
                "    \n",
                "    print(f\"\\n✅ Triton 커널 테스트:\")\n",
                "    print(f\"  정확도 일치: {torch.allclose(pt_result, tr_result)}\")\n",
                "    print(f\"  최대 오차: {torch.max(torch.abs(pt_result - tr_result)).item():.2e}\")\n",
                "    \n",
                "    # 성능 비교\n",
                "    N = 1_000_000\n",
                "    a = torch.randn(N)\n",
                "    b = torch.randn(N)\n",
                "    \n",
                "    start = time.time()\n",
                "    _ = a + b\n",
                "    pt_time = time.time() - start\n",
                "    \n",
                "    _ = triton_add(a, b)  # warm-up\n",
                "    start = time.time()\n",
                "    _ = triton_add(a, b)\n",
                "    tr_time = time.time() - start\n",
                "    \n",
                "    print(f\"\\n⏱️ 성능 비교 (N={N:,}):\")\n",
                "    print(f\"  PyTorch: {pt_time:.6f}s\")\n",
                "    print(f\"  Triton:  {tr_time:.6f}s\")\n",
                "    print(f\"  비율: {tr_time/pt_time:.2f}x\")\n",
                "else:\n",
                "    print(\"⚠️ Triton이 설치되지 않아 건너뜁니다.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 시각화 및 비교"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if TRITON_AVAILABLE:\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "    \n",
                "    # 모델 정확도\n",
                "    ax1.bar(['PyTorch'], [test_pt], alpha=0.7, label='Test Acc')\n",
                "    ax1.set_ylabel('정확도')\n",
                "    ax1.set_title('모델 성능')\n",
                "    ax1.set_ylim(0, 1)\n",
                "    ax1.legend()\n",
                "    \n",
                "    # 커널 성능\n",
                "    ax2.bar(['PyTorch', 'Triton'], [pt_time, tr_time], alpha=0.7)\n",
                "    ax2.set_ylabel('시간 (초)')\n",
                "    ax2.set_title('벡터 덧셈 성능')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"핵심 학습 포인트\")\n",
                "    print(\"=\"*60)\n",
                "    print(\"\\n1. PyTorch (사진 전):\")\n",
                "    print(\"   - 매우 편리한 API\")\n",
                "    print(\"   - 내부 동작이 블랙박스\")\n",
                "    print(\"   - 병렬 처리 구조를 알 수 없음\")\n",
                "    print(\"\\n2. Triton (사진 후):\")\n",
                "    print(\"   - 커널을 직접 작성 (학습 필요)\")\n",
                "    print(\"   - program_id, BLOCK_SIZE로 병렬 구조 명시\")\n",
                "    print(\"   - tl.load/store로 메모리 접근 제어\")\n",
                "    print(\"   - GPU 프로그래밍 개념 학습\")\n",
                "    print(\"\\n3. CPU Fallback:\")\n",
                "    print(\"   - GPU 없이도 커널 프로그래밍 학습 가능\")\n",
                "    print(\"   - 성능은 PyTorch가 더 빠를 수 있음 (BLAS)\")\n",
                "    print(\"   - 목적은 성능 < 병렬 구조 이해\")\n",
                "    print(\"\\n4. WSL Ubuntu:\")\n",
                "    print(\"   - Triton 설치 안정적\")\n",
                "    print(\"   - Linux 딥러닝 생태계 호환\")\n",
                "    print(\"   - 향후 GPU 전환 용이\")\n",
                "    print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 결론\n",
                "\n",
                "### Before (PyTorch)\n",
                "```python\n",
                "y = a + b  # ❓ 어떻게 동작할까?\n",
                "```\n",
                "\n",
                "### After (Triton)\n",
                "```python\n",
                "@triton.jit\n",
                "def add_kernel(...):\n",
                "    pid = tl.program_id(0)  # ✅ 블록 ID\n",
                "    offs = pid * BLOCK_SIZE  # ✅ 시작 위치\n",
                "    # ✅ 명시적 메모리 제어\n",
                "```\n",
                "\n",
                "### 핵심 메시지\n",
                "\n",
                "> **\"GPU 없이도 WSL Ubuntu에서 Triton으로**  \n",
                "> **CPU 커널을 직접 제어하며 병렬 구조를 학습할 수 있다!\"**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
