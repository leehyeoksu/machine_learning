{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04. SVM Soft Margin 완전 정복\n",
                "\n",
                "## 목표\n",
                "- Slack variable의 **동기와 필요성** 완벽 이해\n",
                "- Soft Margin의 수학적 formulation 마스터\n",
                "- C parameter의 역할과 의미\n",
                "- Hard Margin에서 Soft Margin으로의 자연스러운 확장\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Hard Margin의 한계\n",
                "\n",
                "### 1.1 문제점\n",
                "\n",
                "**Hard Margin SVM**의 요구사항:\n",
                "$$y_i(w^T x_i + b) \\geq 1 \\quad \\forall i$$\n",
                "\n",
                "**문제**:\n",
                "1. 데이터가 **선형 분리 불가능**하면 해가 존재하지 않음\n",
                "2. **Outlier** 하나 때문에 전체 초평면이 크게 바뀜\n",
                "3. **과적합**: Training data에 너무 엄격하게 맞춤\n",
                "\n",
                "### 1.2 해결책\n",
                "\n",
                "**Soft Margin**: 일부 점들이 margin을 **위반**하는 것을 허용\n",
                "\n",
                "But:\n",
                "- 위반에 대한 **penalty**를 부과\n",
                "- Margin 최대화와 penalty 최소화 사이의 **trade-off**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.svm import SVC\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "np.random.seed(42)\n",
                "\n",
                "# 선형 분리 불가능한 데이터 생성\n",
                "# Class +1\n",
                "X_pos = np.random.randn(30, 2) + np.array([2, 2])\n",
                "y_pos = np.ones(30)\n",
                "\n",
                "# Class -1  \n",
                "X_neg = np.random.randn(30, 2) + np.array([-2, -2])\n",
                "y_neg = -np.ones(30)\n",
                "\n",
                "# Outliers 추가\n",
                "X_outlier_pos = np.array([[-2, -1]])  # Negative region에 positive outlier\n",
                "y_outlier_pos = np.array([1])\n",
                "X_outlier_neg = np.array([[2, 1]])     # Positive region에 negative outlier\n",
                "y_outlier_neg = np.array([-1])\n",
                "\n",
                "# 전체 데이터\n",
                "X = np.vstack([X_pos, X_neg, X_outlier_pos, X_outlier_neg])\n",
                "y = np.hstack([y_pos, y_neg, y_outlier_pos, y_outlier_neg])\n",
                "\n",
                "# 시각화\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.scatter(X_pos[:, 0], X_pos[:, 1], c='red', marker='o', s=100, label='Class +1', edgecolors='k')\n",
                "plt.scatter(X_neg[:, 0], X_neg[:, 1], c='blue', marker='s', s=100, label='Class -1', edgecolors='k')\n",
                "plt.scatter(X_outlier_pos[:, 0], X_outlier_pos[:, 1], c='red', marker='*', s=500, \n",
                "           label='Outlier (+1)', edgecolors='black', linewidths=2)\n",
                "plt.scatter(X_outlier_neg[:, 0], X_outlier_neg[:, 1], c='blue', marker='*', s=500,\n",
                "           label='Outlier (-1)', edgecolors='black', linewidths=2)\n",
                "\n",
                "plt.xlabel('$x_1$', fontsize=14)\n",
                "plt.ylabel('$x_2$', fontsize=14)\n",
                "plt.title('선형 분리 불가능 데이터 (Outliers 포함)', fontsize=16, fontweight='bold')\n",
                "plt.legend(fontsize=12)\n",
                "plt.grid(alpha=0.3)\n",
                "plt.axis('equal')\n",
                "plt.show()\n",
                "\n",
                "print(\"문제: Outliers 때문에 완벽한 선형 분리가 불가능!\")\n",
                "print(\"→ Hard Margin SVM은 해를 찾을 수 없습니다.\")\n",
                "print(\"→ Soft Margin SVM이 필요합니다!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Slack Variable (ξ) 도입\n",
                "\n",
                "### 2.1 동기 (Motivation)\n",
                "\n",
                "**아이디어**: 각 데이터 포인트가 제약을 **얼마나 위반**하는지 측정\n",
                "\n",
                "원래 제약:\n",
                "$$y_i(w^T x_i + b) \\geq 1$$\n",
                "\n",
                "Slack variable $\\xi_i \\geq 0$를 도입:\n",
                "$$y_i(w^T x_i + b) \\geq 1 - \\xi_i$$\n",
                "\n",
                "### 2.2 ξ의 의미\n",
                "\n",
                "- $\\xi_i = 0$: 제약을 **완벽하게** 만족 (margin 밖)\n",
                "- $0 < \\xi_i < 1$: Margin 안쪽이지만 **올바르게** 분류됨\n",
                "- $\\xi_i \\geq 1$: **잘못** 분류됨 (decision boundary 건너편)\n",
                "\n",
                "### 2.3 Geometric Interpretation\n",
                "\n",
                "점 $x_i$의 \"위반 정도\":\n",
                "$$\\xi_i = \\max(0, 1 - y_i(w^T x_i + b))$$\n",
                "\n",
                "이것은 **hinge loss**입니다!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Slack variable 시각화\n",
                "\n",
                "# Hinge loss\n",
                "def hinge_loss(margin):\n",
                "    \"\"\"Hinge loss: max(0, 1 - margin)\"\"\"\n",
                "    return np.maximum(0, 1 - margin)\n",
                "\n",
                "# y * (w^T x + b) 값\n",
                "margin_values = np.linspace(-2, 3, 100)\n",
                "loss_values = hinge_loss(margin_values)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Hinge loss plot\n",
                "ax = axes[0]\n",
                "ax.plot(margin_values, loss_values, 'b-', linewidth=3, label='ξ = max(0, 1 - yf(x))')\n",
                "ax.axhline(0, color='k', linestyle='-', linewidth=0.5)\n",
                "ax.axvline(1, color='r', linestyle='--', linewidth=2, label='Margin boundary')\n",
                "ax.fill_between(margin_values, 0, loss_values, alpha=0.3)\n",
                "ax.set_xlabel('$y_i(w^T x_i + b)$', fontsize=14)\n",
                "ax.set_ylabel('Slack $\\\\xi_i$', fontsize=14)\n",
                "ax.set_title('Hinge Loss (Slack Variable)', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=12)\n",
                "ax.grid(alpha=0.3)\n",
                "ax.set_ylim(0, 3)\n",
                "\n",
                "# 예시 점들\n",
                "ax = axes[1]\n",
                "example_points = [\n",
                "    {'margin': 2.0, 'xi': 0, 'label': 'Correct (margin 밖)', 'color': 'green'},\n",
                "    {'margin': 0.5, 'xi': 0.5, 'label': 'Margin 안 (올바름)', 'color': 'orange'},\n",
                "    {'margin': -0.5, 'xi': 1.5, 'label': '잘못 분류', 'color': 'red'},\n",
                "]\n",
                "\n",
                "x_pos = np.arange(len(example_points))\n",
                "xi_values = [p['xi'] for p in example_points]\n",
                "colors = [p['color'] for p in example_points]\n",
                "\n",
                "bars = ax.bar(x_pos, xi_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
                "ax.axhline(1, color='r', linestyle='--', linewidth=2, label='ξ = 1 (decision boundary)')\n",
                "ax.set_xlabel('Point type', fontsize=14)\n",
                "ax.set_ylabel('Slack $\\\\xi$', fontsize=14)\n",
                "ax.set_title('Slack Variable Values', fontsize=14, fontweight='bold')\n",
                "ax.set_xticks(x_pos)\n",
                "ax.set_xticklabels([p['label'] for p in example_points], rotation=15, ha='right')\n",
                "ax.legend(fontsize=12)\n",
                "ax.grid(alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"=== Slack Variable 해석 ===\")\n",
                "for i, p in enumerate(example_points):\n",
                "    print(f\"{i+1}. {p['label']}:\")\n",
                "    print(f\"   y(w^T x + b) = {p['margin']:.1f}, ξ = {p['xi']:.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Soft Margin Primal Problem\n",
                "\n",
                "### 3.1 최적화 문제\n",
                "\n",
                "**목표**: Margin 최대화 + Slack penalty 최소화\n",
                "\n",
                "$$\\begin{align}\n",
                "\\min_{w, b, \\xi} \\quad & \\frac{1}{2}||w||^2 + C\\sum_{i=1}^{n} \\xi_i \\\\\n",
                "\\text{subject to} \\quad & y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad i = 1, \\ldots, n \\\\\n",
                "& \\xi_i \\geq 0, \\quad i = 1, \\ldots, n\n",
                "\\end{align}$$\n",
                "\n",
                "### 3.2 C Parameter의 역할\n",
                "\n",
                "**C**: Regularization parameter\n",
                "\n",
                "- **C 큼** (예: C = 1000):\n",
                "  - Slack penalty가 크다\n",
                "  - 위반을 최소화 → Hard Margin에 가까움\n",
                "  - 과적합 위험\n",
                "  \n",
                "- **C 작음** (예: C = 0.01):\n",
                "  - Slack penalty가 작다\n",
                "  - Margin 최대화 우선 → 많은 위반 허용\n",
                "  - 과소적합 위험\n",
                "\n",
                "### 3.3 Trade-off\n",
                "\n",
                "$$\\underbrace{\\frac{1}{2}||w||^2}_{\\text{Margin 최대화}} + \\underbrace{C\\sum_i \\xi_i}_{\\text{Error 최소화}}$$\n",
                "\n",
                "**C가 조절**하는 것:\n",
                "- Large margin을 원하는가?\n",
                "- Training error를 줄이기 원하는가?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# C parameter 효과 시각화\n",
                "\n",
                "C_values = [0.01, 0.1, 1, 100]\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, C in enumerate(C_values):\n",
                "    # SVM 학습\n",
                "    svm = SVC(kernel='linear', C=C)\n",
                "    svm.fit(X, y)\n",
                "    \n",
                "    ax = axes[idx]\n",
                "    \n",
                "    # Decision boundary\n",
                "    xx, yy = np.meshgrid(np.linspace(-6, 6, 200), np.linspace(-6, 6, 200))\n",
                "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
                "    Z = Z.reshape(xx.shape)\n",
                "    \n",
                "    # Contour plot\n",
                "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], linewidths=[2, 3, 2],\n",
                "              linestyles=['--', '-', '--'], colors=['b', 'g', 'r'])\n",
                "    \n",
                "    # 데이터 플롯\n",
                "    ax.scatter(X_pos[:, 0], X_pos[:, 1], c='red', marker='o', s=50, edgecolors='k', alpha=0.6)\n",
                "    ax.scatter(X_neg[:, 0], X_neg[:, 1], c='blue', marker='s', s=50, edgecolors='k', alpha=0.6)\n",
                "    ax.scatter(X_outlier_pos[:, 0], X_outlier_pos[:, 1], c='red', marker='*', s=300,\n",
                "              edgecolors='black', linewidths=2)\n",
                "    ax.scatter(X_outlier_neg[:, 0], X_outlier_neg[:, 1], c='blue', marker='*', s=300,\n",
                "              edgecolors='black', linewidths=2)\n",
                "    \n",
                "    # Support vectors\n",
                "    ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
                "              s=200, facecolors='none', edgecolors='yellow', linewidths=3, label='Support Vectors')\n",
                "    \n",
                "    # Margin width 계산\n",
                "    w = svm.coef_[0]\n",
                "    margin = 2 / np.linalg.norm(w)\n",
                "    \n",
                "    ax.set_xlim(-6, 6)\n",
                "    ax.set_ylim(-6, 6)\n",
                "    ax.set_xlabel('$x_1$', fontsize=12)\n",
                "    ax.set_ylabel('$x_2$', fontsize=12)\n",
                "    ax.set_title(f'C = {C}, Margin = {margin:.2f}, #SV = {len(svm.support_vectors_)}',\n",
                "                fontsize=12, fontweight='bold')\n",
                "    ax.legend()\n",
                "    ax.grid(alpha=0.3)\n",
                "    ax.set_aspect('equal')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"=== C Parameter 효과 ===\")\n",
                "print(\"\\nC 작음 (0.01):\")\n",
                "print(\"  - Margin이 넓음\")\n",
                "print(\"  - 많은 support vectors\")\n",
                "print(\"  - Outliers를 무시\")\n",
                "print(\"\\nC 큼 (100):\")\n",
                "print(\"  - Margin이 좁음\")\n",
                "print(\"  - 적은 support vectors\")\n",
                "print(\"  - Outliers에 민감\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Lagrangian과 Dual Problem\n",
                "\n",
                "### 4.1 Lagrangian\n",
                "\n",
                "Lagrange multipliers: $\\alpha_i \\geq 0$ (for margin constraints), $\\mu_i \\geq 0$ (for $\\xi_i \\geq 0$)\n",
                "\n",
                "$$\\mathcal{L}(w, b, \\xi, \\alpha, \\mu) = \\frac{1}{2}||w||^2 + C\\sum_{i=1}^{n} \\xi_i - \\sum_{i=1}^{n} \\alpha_i [y_i(w^T x_i + b) - 1 + \\xi_i] - \\sum_{i=1}^{n} \\mu_i \\xi_i$$\n",
                "\n",
                "### 4.2 KKT Stationarity Conditions\n",
                "\n",
                "$$\\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_{i=1}^{n} \\alpha_i y_i x_i = 0 \\quad \\Rightarrow \\quad w = \\sum_{i=1}^{n} \\alpha_i y_i x_i$$\n",
                "\n",
                "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{n} \\alpha_i y_i = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0$$\n",
                "\n",
                "$$\\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0 \\quad \\Rightarrow \\quad \\alpha_i + \\mu_i = C$$\n",
                "\n",
                "### 4.3 핵심 통찰\n",
                "\n",
                "$\\mu_i \\geq 0$이고 $\\alpha_i + \\mu_i = C$이므로:\n",
                "\n",
                "$$0 \\leq \\alpha_i \\leq C$$\n",
                "\n",
                "**Box constraint**!\n",
                "\n",
                "### 4.4 Dual Problem\n",
                "\n",
                "위를 대입하면 Hard Margin과 **똑같은 형태**:\n",
                "\n",
                "$$\\begin{align}\n",
                "\\max_{\\alpha} \\quad & \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j \\\\\n",
                "\\text{subject to} \\quad & 0 \\leq \\alpha_i \\leq C, \\quad i = 1, \\ldots, n \\\\\n",
                "& \\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
                "\\end{align}$$\n",
                "\n",
                "**차이점**: $\\alpha_i \\geq 0$ → $0 \\leq \\alpha_i \\leq C$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. KKT Complementary Slackness\n",
                "\n",
                "### 5.1 조건들\n",
                "\n",
                "1. $\\alpha_i [y_i(w^T x_i + b) - 1 + \\xi_i] = 0$\n",
                "2. $\\mu_i \\xi_i = 0$\n",
                "3. $\\alpha_i + \\mu_i = C$\n",
                "\n",
                "### 5.2 케이스 분석\n",
                "\n",
                "**Case 1**: $\\alpha_i = 0$\n",
                "- $\\mu_i = C > 0$ → $\\xi_i = 0$\n",
                "- $y_i(w^T x_i + b) > 1$ (margin 밖, 올바르게 분류)\n",
                "- **Non-support vector**\n",
                "\n",
                "**Case 2**: $0 < \\alpha_i < C$\n",
                "- $\\mu_i > 0$ → $\\xi_i = 0$\n",
                "- $y_i(w^T x_i + b) = 1$ (margin 경계)\n",
                "- **Support vector on the margin**\n",
                "\n",
                "**Case 3**: $\\alpha_i = C$\n",
                "- $\\mu_i = 0$ → $\\xi_i \\geq 0$ (자유)\n",
                "- **Sub-cases**: \n",
                "  - $\\xi_i < 1$: Margin 안이지만 올바르게 분류\n",
                "  - $\\xi_i = 1$: Decision boundary 위\n",
                "  - $\\xi_i > 1$: 잘못 분류\n",
                "- **Support vector inside margin or misclassified**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Soft Margin SVM 직접 풀기\n",
                "from scipy.optimize import minimize\n",
                "\n",
                "# 데이터 (outlier 포함)\n",
                "X_train = np.vstack([X_pos[:5], X_neg[:5], X_outlier_pos, X_outlier_neg])\n",
                "y_train = np.hstack([y_pos[:5], y_neg[:5], y_outlier_pos, y_outlier_neg])\n",
                "\n",
                "n_samples = len(y_train)\n",
                "C_param = 1.0\n",
                "\n",
                "# Kernel matrix\n",
                "K = X_train @ X_train.T\n",
                "Q = np.outer(y_train, y_train) * K\n",
                "\n",
                "def dual_objective(alpha):\n",
                "    return 0.5 * alpha @ Q @ alpha - np.sum(alpha)\n",
                "\n",
                "def dual_gradient(alpha):\n",
                "    return Q @ alpha - np.ones(n_samples)\n",
                "\n",
                "# 제약\n",
                "constraints = {'type': 'eq', 'fun': lambda alpha: np.sum(alpha * y_train)}\n",
                "bounds = [(0, C_param) for _ in range(n_samples)]  # Box constraint!\n",
                "\n",
                "# 최적화\n",
                "alpha_init = np.ones(n_samples) * 0.1\n",
                "result = minimize(\n",
                "    dual_objective,\n",
                "    alpha_init,\n",
                "    method='SLSQP',\n",
                "    jac=dual_gradient,\n",
                "    bounds=bounds,\n",
                "    constraints=constraints\n",
                ")\n",
                "\n",
                "alpha_opt = result.x\n",
                "\n",
                "print(\"=== Soft Margin SVM 풀이 ===\")\n",
                "print(f\"C = {C_param}\")\n",
                "print(f\"\\n최적 α:\")\n",
                "for i, a in enumerate(alpha_opt):\n",
                "    print(f\"  α_{i} = {a:.4f}\")\n",
                "\n",
                "# w 복원\n",
                "w_opt = np.sum(alpha_opt[:, np.newaxis] * y_train[:, np.newaxis] * X_train, axis=0)\n",
                "print(f\"\\nw = {w_opt}\")\n",
                "\n",
                "# Support vectors 분류\n",
                "print(\"\\n=== Support Vectors 분석 ===\")\n",
                "eps = 1e-5\n",
                "\n",
                "for i in range(n_samples):\n",
                "    if alpha_opt[i] < eps:\n",
                "        sv_type = \"Non-SV (α=0)\"\n",
                "    elif alpha_opt[i] < C_param - eps:\n",
                "        sv_type = \"SV on margin (0 < α < C)\"\n",
                "    else:\n",
                "        sv_type = \"SV inside/misclassified (α=C)\"\n",
                "    \n",
                "    print(f\"Point {i}: α={alpha_opt[i]:.4f}, {sv_type}\")\n",
                "\n",
                "# b 계산 (0 < α < C인 점들 사용)\n",
                "margin_sv = np.where((alpha_opt > eps) & (alpha_opt < C_param - eps))[0]\n",
                "if len(margin_sv) > 0:\n",
                "    b_values = []\n",
                "    for idx in margin_sv:\n",
                "        b_val = y_train[idx] - w_opt @ X_train[idx]\n",
                "        b_values.append(b_val)\n",
                "    b_opt = np.mean(b_values)\n",
                "else:\n",
                "    # Fallback: use all support vectors\n",
                "    sv_all = np.where(alpha_opt > eps)[0]\n",
                "    b_values = [y_train[i] - w_opt @ X_train[i] for i in sv_all]\n",
                "    b_opt = np.mean(b_values)\n",
                "\n",
                "print(f\"\\nb = {b_opt:.4f}\")\n",
                "\n",
                "# Slack variables 계산\n",
                "print(\"\\n=== Slack Variables ===\")\n",
                "for i in range(n_samples):\n",
                "    margin_val = y_train[i] * (w_opt @ X_train[i] + b_opt)\n",
                "    xi = max(0, 1 - margin_val)\n",
                "    print(f\"Point {i}: y(w^T x + b) = {margin_val:.4f}, ξ = {xi:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Soft Margin vs Hard Margin 비교\n",
                "\n",
                "| 항목 | Hard Margin | Soft Margin |\n",
                "|------|-------------|-------------|\n",
                "| **제약** | $y_i(w^T x_i + b) \\geq 1$ | $y_i(w^T x_i + b) \\geq 1 - \\xi_i$ |\n",
                "| **목적 함수** | $\\frac{1}{2}\\\\|w\\\\|^2$ | $\\frac{1}{2}\\\\|w\\\\|^2 + C\\sum_i \\xi_i$ |\n",
                "| **Dual 제약** | $\\alpha_i \\geq 0$ | $0 \\leq \\alpha_i \\leq C$ |\n",
                "| **요구사항** | 선형 분리 가능 | 항상 해가 존재 |\n",
                "| **Outlier** | 매우 민감 | 강건 (robust) |\n",
                "| **파라미터** | 없음 | C (regularization) |\n",
                "\n",
                "### 6.1 C → ∞의 극한\n",
                "\n",
                "- $C \\to \\infty$: $\\sum_i \\xi_i \\to 0$ (모든 slack이 0)\n",
                "- **Hard Margin과 같아짐**!\n",
                "\n",
                "따라서 Soft Margin은 Hard Margin의 **일반화**입니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# C 변화에 따른 수렴 시각화\n",
                "\n",
                "# 선형 분리 가능한 데이터 (outlier 제외)\n",
                "X_clean = np.vstack([X_pos[:10], X_neg[:10]])\n",
                "y_clean = np.hstack([y_pos[:10], y_neg[:10]])\n",
                "\n",
                "C_range = np.logspace(-2, 3, 20)\n",
                "margins = []\n",
                "n_support_vectors = []\n",
                "\n",
                "for C in C_range:\n",
                "    svm = SVC(kernel='linear', C=C)\n",
                "    svm.fit(X_clean, y_clean)\n",
                "    \n",
                "    w = svm.coef_[0]\n",
                "    margin = 2 / np.linalg.norm(w)\n",
                "    margins.append(margin)\n",
                "    n_support_vectors.append(len(svm.support_vectors_))\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Margin vs C\n",
                "ax = axes[0]\n",
                "ax.semilogx(C_range, margins, 'o-', linewidth=2, markersize=8)\n",
                "ax.set_xlabel('C (log scale)', fontsize=14)\n",
                "ax.set_ylabel('Margin', fontsize=14)\n",
                "ax.set_title('C → ∞ 일 때 Hard Margin으로 수렴', fontsize=14, fontweight='bold')\n",
                "ax.grid(alpha=0.3)\n",
                "\n",
                "# Number of SVs vs C\n",
                "ax = axes[1]\n",
                "ax.semilogx(C_range, n_support_vectors, 'o-', linewidth=2, markersize=8, color='orange')\n",
                "ax.set_xlabel('C (log scale)', fontsize=14)\n",
                "ax.set_ylabel('# Support Vectors', fontsize=14)\n",
                "ax.set_title('C 증가 → Support Vectors 감소', fontsize=14, fontweight='bold')\n",
                "ax.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"=== C의 효과 ===\")\n",
                "print(\"\\nC가 작을 때:\")\n",
                "print(f\"  Margin = {margins[0]:.4f}\")\n",
                "print(f\"  # SVs = {n_support_vectors[0]}\")\n",
                "print(\"\\nC가 클 때:\")\n",
                "print(f\"  Margin = {margins[-1]:.4f}\")\n",
                "print(f\"  # SVs = {n_support_vectors[-1]}\")\n",
                "print(\"\\n→ C ↑ : Margin ↓, # SVs ↓\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Hinge Loss와 SVM\n",
                "\n",
                "### 7.1 Unconstrained Formulation\n",
                "\n",
                "Soft Margin SVM은 **unconstrained** 형태로 쓸 수 있습니다:\n",
                "\n",
                "$$\\min_{w, b} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^{n} \\max(0, 1 - y_i(w^T x_i + b))$$\n",
                "\n",
                "**Hinge loss**: $\\ell(y, f(x)) = \\max(0, 1 - yf(x))$\n",
                "\n",
                "### 7.2 Regularized ERM\n",
                "\n",
                "$$\\underbrace{\\frac{\\lambda}{2}||w||^2}_{\\text{Regularization}} + \\underbrace{\\sum_{i=1}^{n} \\ell(y_i, f(x_i))}_{\\text{Empirical Risk}}$$\n",
                "\n",
                "where $\\lambda = \\frac{1}{C}$\n",
                "\n",
                "**SVM의 본질**: L2 regularization + Hinge loss\n",
                "\n",
                "### 7.3 다른 Loss들과 비교\n",
                "\n",
                "- **0-1 Loss**: $\\mathbb{1}[yf(x) < 0]$ (not convex, hard to optimize)\n",
                "- **Hinge Loss**: $\\max(0, 1 - yf(x))$ (convex, sparse)\n",
                "- **Logistic Loss**: $\\log(1 + e^{-yf(x)})$ (convex, smooth)\n",
                "- **Squared Hinge**: $(\\max(0, 1 - yf(x)))^2$ (differentiable)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 다양한 loss functions 비교\n",
                "\n",
                "def zero_one_loss(yf):\n",
                "    return (yf < 0).astype(float)\n",
                "\n",
                "def hinge_loss(yf):\n",
                "    return np.maximum(0, 1 - yf)\n",
                "\n",
                "def logistic_loss(yf):\n",
                "    return np.log(1 + np.exp(-yf))\n",
                "\n",
                "def squared_hinge(yf):\n",
                "    return np.maximum(0, 1 - yf) ** 2\n",
                "\n",
                "yf_range = np.linspace(-2, 3, 200)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(yf_range, zero_one_loss(yf_range), 'k-', linewidth=2, label='0-1 Loss')\n",
                "plt.plot(yf_range, hinge_loss(yf_range), 'b-', linewidth=2, label='Hinge Loss')\n",
                "plt.plot(yf_range, logistic_loss(yf_range), 'r-', linewidth=2, label='Logistic Loss')\n",
                "plt.plot(yf_range, squared_hinge(yf_range), 'g-', linewidth=2, label='Squared Hinge')\n",
                "\n",
                "plt.axvline(0, color='gray', linestyle='--', alpha=0.5, label='Decision boundary')\n",
                "plt.axvline(1, color='orange', linestyle='--', alpha=0.5, label='Margin')\n",
                "\n",
                "plt.xlabel('$y \\\\cdot f(x)$', fontsize=14)\n",
                "plt.ylabel('Loss', fontsize=14)\n",
                "plt.title('Classification Loss Functions Comparison', fontsize=16, fontweight='bold')\n",
                "plt.legend(fontsize=12)\n",
                "plt.grid(alpha=0.3)\n",
                "plt.ylim(0, 4)\n",
                "plt.show()\n",
                "\n",
                "print(\"=== Loss Functions 특징 ===\")\n",
                "print(\"\\n0-1 Loss:\")\n",
                "print(\"  - 실제 classification error\")\n",
                "print(\"  - Non-convex → 최적화 어려움\")\n",
                "print(\"\\nHinge Loss (SVM):\")\n",
                "print(\"  - Convex upper bound of 0-1\")\n",
                "print(\"  - Sparse solution (많은 α=0)\")\n",
                "print(\"  - Margin 개념 포함\")\n",
                "print(\"\\nLogistic Loss:\")\n",
                "print(\"  - Smooth, differentiable everywhere\")\n",
                "print(\"  - Probabilistic interpretation\")\n",
                "print(\"  - Dense solution (모든 점이 영향)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. 요약\n",
                "\n",
                "### 8.1 Soft Margin SVM 핵심\n",
                "\n",
                "1. **Slack variable** $\\xi_i$:\n",
                "   - Margin 위반 정도 측정\n",
                "   - $\\xi_i = \\max(0, 1 - y_i(w^T x_i + b))$ (Hinge loss)\n",
                "\n",
                "2. **C parameter**:\n",
                "   - Margin vs Error trade-off\n",
                "   - $C \\to \\infty$: Hard Margin\n",
                "   - $C \\to 0$: Maximum Margin (many violations)\n",
                "\n",
                "3. **Dual formulation**:\n",
                "   - Hard Margin과 거의 같음\n",
                "   - 차이: $0 \\leq \\alpha_i \\leq C$ (box constraint)\n",
                "\n",
                "4. **Support Vectors**:\n",
                "   - $\\alpha_i = 0$: Non-SV\n",
                "   - $0 < \\alpha_i < C$: SV on margin\n",
                "   - $\\alpha_i = C$: SV inside margin or misclassified\n",
                "\n",
                "### 8.2 실전 사용\n",
                "\n",
                "- **항상 Soft Margin 사용**! (Hard Margin은 너무 제한적)\n",
                "- **C는 cross-validation으로** 선택\n",
                "- Dual을 풀면 **kernel trick** 사용 가능\n",
                "\n",
                "### 8.3 다음 단계\n",
                "\n",
                "- Kernel SVM (비선형 분류)\n",
                "- SMO algorithm (효율적 QP solver)\n",
                "- Multi-class SVM"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. 연습문제\n",
                "\n",
                "### 문제 1: Slack Variable 계산\n",
                "\n",
                "초평면 $w = [1, 1]^T$, $b = 0$이 주어졌을 때, 다음 점들의 slack variable을 계산하시오:\n",
                "\n",
                "1. $x_1 = [2, 2]^T$, $y_1 = +1$\n",
                "2. $x_2 = [1, 0.5]^T$, $y_2 = +1$\n",
                "3. $x_3 = [-0.5, -0.5]^T$, $y_3 = -1$\n",
                "4. $x_4 = [0.5, 0.5]^T$, $y_4 = -1$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 문제 1 풀이\n",
                "\n",
                "w = np.array([1, 1])\n",
                "b = 0\n",
                "\n",
                "points = [\n",
                "    {'x': np.array([2, 2]), 'y': 1},\n",
                "    {'x': np.array([1, 0.5]), 'y': 1},\n",
                "    {'x': np.array([-0.5, -0.5]), 'y': -1},\n",
                "    {'x': np.array([0.5, 0.5]), 'y': -1},\n",
                "]\n",
                "\n",
                "print(\"=== 문제 1: Slack Variable 계산 ===\")\n",
                "print(f\"w = {w}, b = {b}\\n\")\n",
                "\n",
                "for i, p in enumerate(points, 1):\n",
                "    x, y = p['x'], p['y']\n",
                "    \n",
                "    # Margin\n",
                "    margin = y * (w @ x + b)\n",
                "    \n",
                "    # Slack\n",
                "    xi = max(0, 1 - margin)\n",
                "    \n",
                "    # 분류\n",
                "    if xi == 0:\n",
                "        status = \"정상 (margin 밖)\"\n",
                "    elif xi < 1:\n",
                "        status = \"margin 안 (올바른 분류)\"\n",
                "    else:\n",
                "        status = \"잘못 분류\"\n",
                "    \n",
                "    print(f\"점 {i}: x = {x}, y = {y:+d}\")\n",
                "    print(f\"  w^T x + b = {w @ x + b:.2f}\")\n",
                "    print(f\"  y(w^T x + b) = {margin:.2f}\")\n",
                "    print(f\"  ξ = max(0, 1 - {margin:.2f}) = {xi:.2f}\")\n",
                "    print(f\"  상태: {status}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 문제 2: C 파라미터 선택\n",
                "\n",
                "주어진 데이터에 대해 다양한 C 값으로 SVM을 학습하고:\n",
                "1. Train accuracy\n",
                "2. Number of support vectors\n",
                "3. Margin width\n",
                "\n",
                "를 계산하여 비교하시오."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 문제 2 풀이\n",
                "\n",
                "# 데이터 생성\n",
                "from sklearn.datasets import make_classification\n",
                "\n",
                "X_data, y_data = make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
                "                                      n_clusters_per_class=1, random_state=42)\n",
                "y_data = 2 * y_data - 1  # {0, 1} → {-1, +1}\n",
                "\n",
                "C_values = [0.01, 0.1, 1, 10, 100]\n",
                "results = []\n",
                "\n",
                "for C in C_values:\n",
                "    svm = SVC(kernel='linear', C=C)\n",
                "    svm.fit(X_data, y_data)\n",
                "    \n",
                "    # Metrics\n",
                "    train_acc = svm.score(X_data, y_data)\n",
                "    n_sv = len(svm.support_vectors_)\n",
                "    w = svm.coef_[0]\n",
                "    margin = 2 / np.linalg.norm(w)\n",
                "    \n",
                "    results.append({\n",
                "        'C': C,\n",
                "        'train_acc': train_acc,\n",
                "        'n_sv': n_sv,\n",
                "        'margin': margin\n",
                "    })\n",
                "\n",
                "print(\"=== 문제 2: C 파라미터 비교 ===\")\n",
                "print(f\"{'C':>8} {'Train Acc':>12} {'# SVs':>8} {'Margin':>10}\")\n",
                "print(\"-\" * 42)\n",
                "for r in results:\n",
                "    print(f\"{r['C']:>8.2f} {r['train_acc']:>12.4f} {r['n_sv']:>8d} {r['margin']:>10.4f}\")\n",
                "\n",
                "# 시각화\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "C_vals = [r['C'] for r in results]\n",
                "\n",
                "ax = axes[0]\n",
                "ax.semilogx(C_vals, [r['train_acc'] for r in results], 'o-', linewidth=2, markersize=10)\n",
                "ax.set_xlabel('C', fontsize=14)\n",
                "ax.set_ylabel('Train Accuracy', fontsize=14)\n",
                "ax.set_title('C vs Accuracy', fontsize=14, fontweight='bold')\n",
                "ax.grid(alpha=0.3)\n",
                "\n",
                "ax = axes[1]\n",
                "ax.semilogx(C_vals, [r['n_sv'] for r in results], 'o-', linewidth=2, markersize=10, color='orange')\n",
                "ax.set_xlabel('C', fontsize=14)\n",
                "ax.set_ylabel('# Support Vectors', fontsize=14)\n",
                "ax.set_title('C vs # SVs', fontsize=14, fontweight='bold')\n",
                "ax.grid(alpha=0.3)\n",
                "\n",
                "ax = axes[2]\n",
                "ax.semilogx(C_vals, [r['margin'] for r in results], 'o-', linewidth=2, markersize=10, color='green')\n",
                "ax.set_xlabel('C', fontsize=14)\n",
                "ax.set_ylabel('Margin', fontsize=14)\n",
                "ax.set_title('C vs Margin', fontsize=14, fontweight='bold')\n",
                "ax.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n관찰:\")\n",
                "print(\"- C ↑ : Train acc ↑, # SVs ↓, Margin ↓\")\n",
                "print(\"- 너무 큰 C는 과적합 위험!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. 추가 자료\n",
                "\n",
                "### 강의 자료\n",
                "- ML_L15b_SVM_(linear-soft).pdf\n",
                "- ML_L15c_SVM_(kernel).pdf\n",
                "\n",
                "### Online Resources\n",
                "1. **YouTube**:\n",
                "   - StatQuest: Soft Margin SVM\n",
                "   - Andrew Ng: SVM with Outliers\n",
                "\n",
                "2. **Blogs**:\n",
                "   - [SVM explained with cats](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
                "   - [Kernel Methods Tutorial](https://people.csail.mit.edu/)\n",
                "\n",
                "3. **Papers**:\n",
                "   - Cortes & Vapnik (1995): Support-Vector Networks\n",
                "   - Vapnik (1998): Statistical Learning Theory\n",
                "\n",
                "### 다음 학습 주제\n",
                "- Kernel SVM\n",
                "- SMO algorithm\n",
                "- ν-SVM (alternative parameterization)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}