# í–‰ë ¬ ë¯¸ë¶„ ì¹˜íŠ¸ì‹œíŠ¸ (Cheat Sheet)
## Matrix Calculus Quick Reference

---

## ğŸ¯ í•µì‹¬ ì›ì¹™

### Rule #1: Shape Matching
```
âˆ‚f/âˆ‚Aì˜ shape = Aì˜ shape (í•­ìƒ!)
```

### Rule #2: Layout Convention
```
MLì—ì„œëŠ” Denominator Layout ì‚¬ìš©
â†’ âˆ‚(Ax)/âˆ‚x = A^T (ì „ì¹˜!)
```

### Rule #3: Element-wise
```
(âˆ‚f/âˆ‚A)áµ¢â±¼ = âˆ‚f/âˆ‚Aáµ¢â±¼
```

---

## ğŸ“ ë²¡í„° ë¯¸ë¶„ ê³µì‹

| Function | Gradient | Notes |
|----------|----------|-------|
| `a^T x` | `a` | ì„ í˜• |
| `x^T a` | `a` | ë™ì¼ |
| `x^T x` | `2x` | Quadratic |
| `Ax` | `A^T` | **ì „ì¹˜ ì£¼ì˜!** |
| `x^T A` | `A` | ì „ì¹˜ ì—†ìŒ |
| `x^T Ax` | `(A + A^T)x` | A ëŒ€ì¹­ì´ë©´ `2Ax` |
| `â€–xâ€–â‚‚` | `x/â€–xâ€–â‚‚` | Normalization |
| `â€–xâ€–â‚‚Â²` | `2x` | MSE |
| `â€–Ax - bâ€–Â²` | `2A^T(Ax - b)` | Least Squares |

---

## ğŸ”² í–‰ë ¬ ë¯¸ë¶„ ê³µì‹

### Trace ê¸°ë³¸ ê³µì‹

| Function | Gradient | Condition |
|----------|----------|-----------|
| `tr(A)` | `I` | |
| `tr(AB)` | `B^T` | B ê³ ì • |
| `tr(A^T B)` | `B` | B ê³ ì • |
| `tr(ABA^T)` | `AB^T + AB` | B ëŒ€ì¹­ì´ë©´ `2AB` |
| `tr(A^T BA)` | `(B + B^T)A` | B ëŒ€ì¹­ì´ë©´ `2BA` |
| `tr(A^k)` | `k(A^(k-1))^T` | |

### Norm ê³µì‹

| Function | Gradient |
|----------|----------|
| `â€–Aâ€–_FÂ²` | `2A` |
| `â€–AXâ€–_FÂ²` | `2AXX^T` |
| `â€–AX - Bâ€–_FÂ²` | `2(AX - B)X^T` |
| `â€–XAâ€–_FÂ²` | `2X^TXA` |

### í–‰ë ¬ì‹ (Determinant)

| Function | Gradient |
|----------|----------|
| `\|A\|` | `\|A\|(A^(-1))^T` |
| `log\|A\|` | `(A^(-1))^T` |

---

## ğŸ§® ìì£¼ ì“°ëŠ” íŒ¨í„´

### Linear Regression
```
L(w) = â€–Xw - yâ€–Â²

âˆ‚L/âˆ‚w = 2X^T(Xw - y)

ìµœì í•´: w* = (X^T X)^(-1) X^T y
```

### Ridge Regression
```
L(w) = â€–Xw - yâ€–Â² + Î»â€–wâ€–Â²

âˆ‚L/âˆ‚w = 2X^T(Xw - y) + 2Î»w

ìµœì í•´: w* = (X^T X + Î»I)^(-1) X^T y
```

### Weight Matrix Gradient (Neural Networks)
```
L = â€–WX - Yâ€–Â²

âˆ‚L/âˆ‚W = 2(WX - Y)X^T
```

### Logistic Regression
```
L(w) = -Î£[y log Ïƒ(w^T x) + (1-y)log(1-Ïƒ(w^T x))]

âˆ‚L/âˆ‚w = X^T(Ïƒ(Xw) - y)
```

### Softmax + Cross-Entropy
```
L = -Î£ y_i log Ïƒ(z)_i

âˆ‚L/âˆ‚z = Ïƒ(z) - y  â† ë§¤ìš° ê°„ë‹¨!
```

---

## ğŸ”— Chain Rule

### ë²¡í„° Chain Rule
```
z = f(y), y = g(x)

âˆ‚z/âˆ‚x = (âˆ‚y/âˆ‚x)^T Â· (âˆ‚z/âˆ‚y)
```

**ì˜ˆì œ:**
```
L = â€–Ax - bâ€–Â²
u = Ax - b
L = u^T u

âˆ‚L/âˆ‚x = (âˆ‚u/âˆ‚x)^T Â· (âˆ‚L/âˆ‚u)
      = A^T Â· (2u)
      = 2A^T(Ax - b)
```

### í–‰ë ¬ Chain Rule
```
L = tr(f(A))

âˆ‚L/âˆ‚A = (âˆ‚f/âˆ‚A)^T Â· (âˆ‚L/âˆ‚f)  (in trace form)
```

---

## ğŸ¨ Trace Tricks

### Cyclic Property
```
tr(ABC) = tr(BCA) = tr(CAB)
```

### Transpose Invariance
```
tr(A^T) = tr(A)
```

### Scalar to Trace
```
x^T Ax = tr(x^T Ax) = tr(Axx^T)
```

### í™œìš© ì˜ˆì‹œ
```
L = â€–WX - Yâ€–_FÂ²
  = tr((WX - Y)^T(WX - Y))
  = tr(X^T W^T WX - 2X^T W^T Y + Y^T Y)

âˆ‚L/âˆ‚W = 2(WX - Y)X^T
```

---

## ğŸ“ Jacobian Matrix

### ì •ì˜
```
y = f(x), y âˆˆ R^m, x âˆˆ R^n

J = âˆ‚y/âˆ‚x âˆˆ R^(mÃ—n)

Jáµ¢â±¼ = âˆ‚yáµ¢/âˆ‚xâ±¼
```

### ì˜ˆì œ
```
y = Ax, A âˆˆ R^(mÃ—n)

J = A âˆˆ R^(mÃ—n)

í•˜ì§€ë§Œ scalar lossì˜ ê²½ìš°:
âˆ‚L/âˆ‚x = A^T Â· (âˆ‚L/âˆ‚y)  â† ì „ì¹˜!
```

---

## ğŸ” ê²€ì¦ ë°©ë²•

### Numerical Gradient
```python
def numerical_grad(f, x, eps=1e-5):
    grad = np.zeros_like(x)
    for i in range(len(x)):
        x_plus = x.copy()
        x_plus[i] += eps
        x_minus = x.copy()
        x_minus[i] -= eps
        grad[i] = (f(x_plus) - f(x_minus)) / (2*eps)
    return grad
```

### Shape Check Workflow
```
1. Input shape í™•ì¸
2. Output shape ì˜ˆìƒ
3. Gradient shape = Input shape í™•ì¸
4. Numerical gradientë¡œ ê²€ì¦
```

---

## âš ï¸ ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜

### âŒ ì‹¤ìˆ˜ 1: ì „ì¹˜ ë¹ ëœ¨ë¦¼
```
âŒ âˆ‚(Ax)/âˆ‚x = A
âœ… âˆ‚(Ax)/âˆ‚x = A^T
```

### âŒ ì‹¤ìˆ˜ 2: Chain Ruleì—ì„œ ì°¨ì› ì•ˆ ë§ì¶¤
```
âŒ âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚x
âœ… âˆ‚L/âˆ‚x = (âˆ‚y/âˆ‚x)^T Â· (âˆ‚L/âˆ‚y)
```

### âŒ ì‹¤ìˆ˜ 3: Scalar vs Vector í—·ê°ˆë¦¼
```
L = x^T Ax (scalar)

âŒ âˆ‚L/âˆ‚x = Ax
âœ… âˆ‚L/âˆ‚x = (A + A^T)x
```

### âŒ ì‹¤ìˆ˜ 4: Layout Convention í˜¼ë™
```
Numerator layoutê³¼ Denominator layoutì´ ë‹¤ë¦„!
MLì—ì„œëŠ” Denominator layout ì‚¬ìš©
```

---

## ğŸ’¡ ë¹ ë¥¸ ìœ ë„ ì „ëµ

### Strategy 1: ê°„ë‹¨í•œ ì˜ˆì œë¡œ ì‹œì‘
```
2Ã—2 í–‰ë ¬ë¡œ elementë³„ë¡œ ê³„ì‚°
â†’ íŒ¨í„´ ë°œê²¬
â†’ ì¼ë°˜í™”
```

### Strategy 2: Trace í™œìš©
```
Scalarë¥¼ traceë¡œ ë³€í™˜
â†’ Trace ë¯¸ë¶„ ê³µì‹ ì‚¬ìš©
â†’ Cyclic propertyë¡œ ì •ë¦¬
```

### Strategy 3: Element-wise ì ‘ê·¼
```
(âˆ‚f/âˆ‚A)áµ¢â±¼ = âˆ‚f/âˆ‚Aáµ¢â±¼

íŠ¹ì • elementì— ëŒ€í•´ í¸ë¯¸ë¶„
â†’ ì „ì²´ í–‰ë ¬ë¡œ ì¡°ë¦½
```

---

## ğŸ“š ì°¨ì›ë³„ ë¶„ë¥˜

### Scalar â†’ Vector
```
f: R â†’ R^n
âˆ‚f/âˆ‚x âˆˆ R^n
```

### Vector â†’ Scalar
```
f: R^n â†’ R
âˆ‚f/âˆ‚x âˆˆ R^n (gradient)
```

### Vector â†’ Vector
```
f: R^n â†’ R^m
âˆ‚f/âˆ‚x âˆˆ R^(mÃ—n) (Jacobian)
```

### Matrix â†’ Scalar (ê°€ì¥ ì¤‘ìš”!)
```
f: R^(mÃ—n) â†’ R
âˆ‚f/âˆ‚A âˆˆ R^(mÃ—n) (gradient)
```

### Matrix â†’ Matrix (ê±°ì˜ ì•ˆ ì”€)
```
f: R^(mÃ—n) â†’ R^(pÃ—q)
âˆ‚f/âˆ‚A âˆˆ R^(pqÃ—mn) (Jacobian, 4D tensor)
```

---

## ğŸš€ ì‹¤ì „ íŒ

### Tip 1: Shape First!
ëª¨ë“  ê³„ì‚° ì „ì— shape ë¨¼ì € í™•ì¸

### Tip 2: ê³µì‹ ì™¸ìš°ì§€ ë§ê³  ìœ ë„
ê¸°ë³¸ ê³µì‹ ëª‡ ê°œë§Œ ê¸°ì–µí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìœ ë„

### Tip 3: Numericalë¡œ í•­ìƒ ê²€ì¦
Analytical gradient êµ¬í•œ í›„ ë°˜ë“œì‹œ numericalë¡œ ì²´í¬

### Tip 4: Vectorization
For loop ëŒ€ì‹  í–‰ë ¬ ì—°ì‚° ì‚¬ìš©

### Tip 5: Dimension Matching
Chain rule ì“¸ ë•Œ ì°¨ì› í•­ìƒ í™•ì¸

---

## ğŸ¯ ì•”ê¸°í•´ì•¼ í•  ìµœì†Œ ê³µì‹

**ì´ê²ƒë§Œ ì™¸ìš°ë©´ ëœë‹¤:**

```
1. âˆ‚(Ax)/âˆ‚x = A^T
2. âˆ‚(x^T Ax)/âˆ‚x = (A + A^T)x
3. âˆ‚tr(AB)/âˆ‚A = B^T
4. âˆ‚â€–Ax - bâ€–Â²/âˆ‚x = 2A^T(Ax - b)
```

ë‚˜ë¨¸ì§€ëŠ” ìœ„ 4ê°œë¡œ ìœ ë„ ê°€ëŠ¥!

---

## ğŸ“– ì°¸ê³  ìë£Œ

- Matrix Cookbook (Petersen & Pedersen)
- CS231n Backpropagation Notes
- Deep Learning Book (Goodfellow et al.) - Chapter 2
- ML_L04_vector.calculus_review.pdf

---

**ì´ ì¹˜íŠ¸ì‹œíŠ¸ë¥¼ ì €ì¥í•´ë‘ê³  í•„ìš”í•  ë•Œë§ˆë‹¤ ì°¸ê³ í•˜ì„¸ìš”!** ğŸ“Œ
