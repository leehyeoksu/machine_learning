"""
í–‰ë ¬-í–‰ë ¬ ë¯¸ë¶„ ì‹¤ì „ ì˜ˆì œ
Matrix-to-Matrix Derivatives with Concrete Examples

ì´ íŒŒì¼ì€ í–‰ë ¬ ë¯¸ë¶„ì˜ í—·ê°ˆë¦¬ëŠ” ë¶€ë¶„ì„ ëª…í™•í•˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import numpy as np

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: Numerical Gradient
# ============================================================================

def numerical_gradient_matrix(f, A, eps=1e-5):
    """
    í–‰ë ¬ Aì— ëŒ€í•œ ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ fì˜ numerical gradient ê³„ì‚°
    
    Parameters:
    - f: function that takes matrix A and returns scalar
    - A: numpy array of any shape
    - eps: small perturbation for finite difference
    
    Returns:
    - grad: same shape as A, containing âˆ‚f/âˆ‚A
    """
    grad = np.zeros_like(A)
    
    for i in range(A.shape[0]):
        for j in range(A.shape[1]):
            # Aì˜ (i,j) elementë¥¼ epsë§Œí¼ ì¦ê°€
            A_plus = A.copy()
            A_plus[i, j] += eps
            f_plus = f(A_plus)
            
            # Aì˜ (i,j) elementë¥¼ epsë§Œí¼ ê°ì†Œ
            A_minus = A.copy()
            A_minus[i, j] -= eps
            f_minus = f(A_minus)
            
            # Finite difference
            grad[i, j] = (f_plus - f_minus) / (2 * eps)
    
    return grad


def compare_gradients(analytical, numerical, name="Gradient"):
    """ë‘ gradientë¥¼ ë¹„êµí•˜ê³  ê²°ê³¼ ì¶œë ¥"""
    print(f"\n{'='*60}")
    print(f"{name} ë¹„êµ")
    print(f"{'='*60}")
    print(f"Analytical gradient:\n{analytical}\n")
    print(f"Numerical gradient:\n{numerical}\n")
    error = np.max(np.abs(analytical - numerical))
    print(f"Max error: {error:.2e}")
    
    if error < 1e-5:
        print("âœ… PASS: Gradientê°€ ì •í™•í•©ë‹ˆë‹¤!")
    else:
        print("âŒ FAIL: Gradientì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
    print(f"{'='*60}\n")


# ============================================================================
# ì˜ˆì œ 1: tr(AB) - ê°€ì¥ ê¸°ë³¸ì ì¸ ê²½ìš°
# ============================================================================

print("\n" + "="*70)
print("ì˜ˆì œ 1: âˆ‚tr(AB)/âˆ‚A = B^T")
print("="*70)

# Setup
A = np.random.randn(3, 2)  # 3x2 í–‰ë ¬
B = np.random.randn(2, 3)  # 2x3 í–‰ë ¬ (ê³ ì •)

print(f"A shape: {A.shape}")
print(f"B shape: {B.shape}")
print(f"AB shape: {(A @ B).shape}")

# í•¨ìˆ˜ ì •ì˜: f(A) = tr(AB)
def f1(A):
    return np.trace(A @ B)

# Analytical gradient: âˆ‚tr(AB)/âˆ‚A = B^T
grad_analytical = B.T

# Numerical gradient
grad_numerical = numerical_gradient_matrix(f1, A)

compare_gradients(grad_analytical, grad_numerical, "âˆ‚tr(AB)/âˆ‚A")


# ============================================================================
# ì˜ˆì œ 2: tr(A^T B) - ì „ì¹˜ ìœ„ì¹˜ ì¤‘ìš”!
# ============================================================================

print("\n" + "="*70)
print("ì˜ˆì œ 2: âˆ‚tr(A^T B)/âˆ‚A = B")
print("="*70)

# Setup: Aì™€ Bê°€ ê°™ì€ í¬ê¸°
A = np.random.randn(3, 2)
B = np.random.randn(3, 2)  # ê°™ì€ í¬ê¸°!

print(f"A shape: {A.shape}")
print(f"B shape: {B.shape}")

# í•¨ìˆ˜ ì •ì˜: f(A) = tr(A^T B)
def f2(A):
    return np.trace(A.T @ B)

# Analytical gradient: âˆ‚tr(A^T B)/âˆ‚A = B
grad_analytical = B

# Numerical gradient
grad_numerical = numerical_gradient_matrix(f2, A)

compare_gradients(grad_analytical, grad_numerical, "âˆ‚tr(A^T B)/âˆ‚A")


# ============================================================================
# ì˜ˆì œ 3: Frobenius Norm - ||A||_F^2
# ============================================================================

print("\n" + "="*70)
print("ì˜ˆì œ 3: âˆ‚||A||_F^2 / âˆ‚A = 2A")
print("="*70)

A = np.random.randn(2, 3)

print(f"A shape: {A.shape}")
print(f"||A||_F = {np.linalg.norm(A, 'fro'):.4f}")

# í•¨ìˆ˜ ì •ì˜: f(A) = ||A||_F^2 = tr(A^T A)
def f3(A):
    return np.sum(A ** 2)  # ë˜ëŠ” np.trace(A.T @ A)

# Analytical gradient
grad_analytical = 2 * A

# Numerical gradient
grad_numerical = numerical_gradient_matrix(f3, A)

compare_gradients(grad_analytical, grad_numerical, "âˆ‚||A||_F^2 / âˆ‚A")


# ============================================================================
# ì˜ˆì œ 4: ||AX - B||_F^2 - Linear Regression with matrices
# ============================================================================

print("\n" + "="*70)
print("ì˜ˆì œ 4: âˆ‚||AX - B||_F^2 / âˆ‚A = 2(AX - B)X^T")
print("="*70)

# Setup
A = np.random.randn(3, 4)  # ìš°ë¦¬ê°€ ìµœì í™”í•  í–‰ë ¬ (weight matrix)
X = np.random.randn(4, 5)  # ì…ë ¥ ë°ì´í„° (ê³ ì •)
B = np.random.randn(3, 5)  # ëª©í‘œ ì¶œë ¥ (ê³ ì •)

print(f"A shape: {A.shape}")
print(f"X shape: {X.shape}")
print(f"B shape: {B.shape}")
print(f"AX shape: {(A @ X).shape}")

# í•¨ìˆ˜ ì •ì˜: f(A) = ||AX - B||_F^2
def f4(A):
    residual = A @ X - B
    return np.sum(residual ** 2)

# Analytical gradient: âˆ‚||AX - B||_F^2 / âˆ‚A = 2(AX - B)X^T
residual = A @ X - B
grad_analytical = 2 * residual @ X.T

# Numerical gradient
grad_numerical = numerical_gradient_matrix(f4, A)

compare_gradients(grad_analytical, grad_numerical, "âˆ‚||AX - B||_F^2 / âˆ‚A")


# ============================================================================
# ì˜ˆì œ 5: tr(AXA^T) - ëŒ€ì¹­ í˜•íƒœ
# ============================================================================

print("\n" + "="*70)
print("ì˜ˆì œ 5: âˆ‚tr(AXA^T)/âˆ‚A = AX^T + AX (Xê°€ ëŒ€ì¹­ì´ë©´ 2AX)")
print("="*70)

# Setup
A = np.random.randn(3, 3)
X = np.random.randn(3, 3)
X = (X + X.T) / 2  # Xë¥¼ ëŒ€ì¹­ í–‰ë ¬ë¡œ ë§Œë“¤ê¸°

print(f"A shape: {A.shape}")
print(f"X shape: {X.shape}")
print(f"X is symmetric: {np.allclose(X, X.T)}")

# í•¨ìˆ˜ ì •ì˜: f(A) = tr(AXA^T)
def f5(A):
    return np.trace(A @ X @ A.T)

# Analytical gradient
# Xê°€ ëŒ€ì¹­ì´ë©´: âˆ‚tr(AXA^T)/âˆ‚A = 2AX
grad_analytical = 2 * A @ X

# Numerical gradient
grad_numerical = numerical_gradient_matrix(f5, A)

compare_gradients(grad_analytical, grad_numerical, "âˆ‚tr(AXA^T)/âˆ‚A (X symmetric)")


# ============================================================================
# ì˜ˆì œ 6: Elementë³„ ê³„ì‚° vs ê³µì‹ (ì‘ì€ ì˜ˆì œë¡œ ì§ì ‘ í™•ì¸)
# ============================================================================

print("\n" + "="*70)
print("ì˜ˆì œ 6: Elementë³„ ê³„ì‚°ìœ¼ë¡œ ê³µì‹ ìœ ë„ í™•ì¸")
print("="*70)

# ê°„ë‹¨í•œ 2x2 ì˜ˆì œ
A = np.array([[1.0, 2.0],
              [3.0, 4.0]])

X = np.array([[5.0, 6.0],
              [7.0, 8.0]])

print("A =")
print(A)
print("\nX =")
print(X)

# f(A) = tr(AX) ê³„ì‚°
AX = A @ X
f_val = np.trace(AX)
print(f"\nAX =")
print(AX)
print(f"\ntr(AX) = {f_val}")

# Elementë³„ë¡œ ì§ì ‘ ê³„ì‚°
print("\n--- Elementë³„ ë¯¸ë¶„ ê³„ì‚° ---")
print("f = tr(AX) = (AX)[0,0] + (AX)[1,1]")
print("  = (A[0,0]*X[0,0] + A[0,1]*X[1,0]) + (A[1,0]*X[0,1] + A[1,1]*X[1,1])")
print(f"  = ({A[0,0]}*{X[0,0]} + {A[0,1]}*{X[1,0]}) + ({A[1,0]}*{X[0,1]} + {A[1,1]}*{X[1,1]})")
print(f"  = {A[0,0]*X[0,0] + A[0,1]*X[1,0]} + {A[1,0]*X[0,1] + A[1,1]*X[1,1]}")
print(f"  = {f_val}")

print("\nElementë³„ í¸ë¯¸ë¶„:")
print(f"âˆ‚f/âˆ‚A[0,0] = X[0,0] = {X[0,0]}")
print(f"âˆ‚f/âˆ‚A[0,1] = X[1,0] = {X[1,0]}")
print(f"âˆ‚f/âˆ‚A[1,0] = X[0,1] = {X[0,1]}")
print(f"âˆ‚f/âˆ‚A[1,1] = X[1,1] = {X[1,1]}")

grad_manual = np.array([[X[0,0], X[1,0]],
                        [X[0,1], X[1,1]]])

print("\nìˆ˜ë™ìœ¼ë¡œ ë§Œë“  gradient:")
print(grad_manual)

print("\nX^T (ê³µì‹ ì‚¬ìš©):")
print(X.T)

print("\në¹„êµ:")
print(f"ìˆ˜ë™ ê³„ì‚° == X^T: {np.allclose(grad_manual, X.T)}")


# ============================================================================
# ì˜ˆì œ 7: ë‹¤ë¥¸ í¬ê¸° í–‰ë ¬ (2x4 @ 4x2)
# ============================================================================

print("\n" + "="*70)
print("ì˜ˆì œ 7: ë‹¤ë¥¸ í¬ê¸° í–‰ë ¬ - A(2x4) @ X(4x2)")
print("="*70)

A = np.random.randn(2, 4)  # 2x4
X = np.random.randn(4, 2)  # 4x2

print(f"A shape: {A.shape}")
print(f"X shape: {X.shape}")
print(f"AX shape: {(A @ X).shape}")

# f(A) = tr(AX)
def f7(A):
    return np.trace(A @ X)

# Analytical: âˆ‚tr(AX)/âˆ‚A = X^T
# X^TëŠ” 2x4ê°€ ë˜ì–´ì•¼ Aì™€ ê°™ì€ í¬ê¸°!
print(f"X^T shape: {X.T.shape}")  # 2x4 âœ“

grad_analytical = X.T

# Numerical
grad_numerical = numerical_gradient_matrix(f7, A)

compare_gradients(grad_analytical, grad_numerical, "âˆ‚tr(AX)/âˆ‚A (different sizes)")


# ============================================================================
# ì˜ˆì œ 8: Chain Rule - Neural Network Weight Gradient
# ============================================================================

print("\n" + "="*70)
print("ì˜ˆì œ 8: Neural Network - âˆ‚L/âˆ‚W where L = ||WX - Y||^2")
print("="*70)

# Setup: WëŠ” weight matrix, XëŠ” input, YëŠ” target
W = np.random.randn(3, 4)  # Weight matrix (output_dim x input_dim)
X = np.random.randn(4, 10)  # Input batch (input_dim x batch_size)
Y = np.random.randn(3, 10)  # Target batch (output_dim x batch_size)

print(f"W shape: {W.shape}")
print(f"X shape: {X.shape}")
print(f"Y shape: {Y.shape}")

# Loss function
def loss(W):
    predictions = W @ X
    return 0.5 * np.sum((predictions - Y) ** 2)

# Analytical gradient: âˆ‚L/âˆ‚W = (WX - Y) @ X^T
predictions = W @ X
residual = predictions - Y
grad_analytical = residual @ X.T

# Numerical gradient
grad_numerical = numerical_gradient_matrix(loss, W)

compare_gradients(grad_analytical, grad_numerical, "âˆ‚L/âˆ‚W (Neural Network)")


# ============================================================================
# ìš”ì•½
# ============================================================================

print("\n" + "="*70)
print("ìš”ì•½: í•µì‹¬ ê³µì‹ë“¤")
print("="*70)

summary = """
âœ… í–‰ë ¬ ë¯¸ë¶„ì˜ í•µì‹¬ ì›ì¹™:
   - Gradientì˜ shape = ë¯¸ë¶„ ëŒ€ìƒì˜ shape
   - âˆ‚f/âˆ‚A âˆˆ R^(mÃ—n) if A âˆˆ R^(mÃ—n)

âœ… ìì£¼ ì“°ì´ëŠ” ê³µì‹:
   1. âˆ‚tr(AB)/âˆ‚A = B^T
   2. âˆ‚tr(A^T B)/âˆ‚A = B
   3. âˆ‚||A||_F^2/âˆ‚A = 2A
   4. âˆ‚||AX - B||_F^2/âˆ‚A = 2(AX - B)X^T
   5. âˆ‚tr(AXA^T)/âˆ‚A = A(X + X^T)  [X ëŒ€ì¹­ì´ë©´ 2AX]

âœ… ê²€ì¦ ë°©ë²•:
   - í•­ìƒ numerical gradientë¡œ í™•ì¸!
   - Shapeê°€ ë§ëŠ”ì§€ ë¨¼ì € ì²´í¬!
   - ê°„ë‹¨í•œ 2x2 ì˜ˆì œë¡œ elementë³„ ê³„ì‚°í•´ë³´ê¸°!

âœ… MLì—ì„œì˜ í™œìš©:
   - Loss functionì€ ëŒ€ë¶€ë¶„ scalar
   - Weight matrix Wì— ëŒ€í•œ gradient ê³„ì‚°
   - Backpropagation = chain rule with matrix gradients
"""

print(summary)

print("\nëª¨ë“  ì˜ˆì œ ì™„ë£Œ! ğŸ‰")
print("ì´ì œ í–‰ë ¬ ë¯¸ë¶„ì´ ëª…í™•í•´ì¡Œë‚˜ìš”?")
print("="*70)
