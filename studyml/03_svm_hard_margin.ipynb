{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03. SVM Hard Margin 완전 정복\n",
                "\n",
                "## 목표\n",
                "- 왜 margin을 1로 정규화하는지 기하학적으로 이해\n",
                "- Lagrangian multiplier 방법 완전 마스터\n",
                "- Primal problem에서 Dual problem 유도\n",
                "- KKT 조건의 의미와 역할 이해\n",
                "\n",
                "---\n",
                "\n",
                "## 1. SVM의 직관\n",
                "\n",
                "### 1.1 문제 설정\n",
                "\n",
                "**Linear classification**: 데이터를 선형으로 분리하는 초평면(hyperplane)을 찾기\n",
                "\n",
                "- 데이터: $(x_1, y_1), \\ldots, (x_n, y_n)$ where $x_i \\in \\mathbb{R}^d$, $y_i \\in \\{-1, +1\\}$\n",
                "- 초평면: $w^T x + b = 0$\n",
                "- 분류 규칙: $\\text{sign}(w^T x + b)$\n",
                "\n",
                "### 1.2 여러 개의 분류 초평면\n",
                "\n",
                "선형 분리 가능한 데이터라면 **무수히 많은** 초평면이 존재합니다.\n",
                "\n",
                "**질문**: 어떤 초평면이 가장 좋을까?\n",
                "\n",
                "**SVM의 답**: **Margin이 최대인** 초평면!\n",
                "\n",
                "### 1.3 Margin의 정의\n",
                "\n",
                "**Geometric Margin**: 가장 가까운 데이터 포인트와 초평면 사이의 거리\n",
                "\n",
                "점 $x_0$에서 초평면 $w^T x + b = 0$까지의 거리:\n",
                "\n",
                "$$\\text{distance} = \\frac{|w^T x_0 + b|}{||w||}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.patches import FancyArrowPatch\n",
                "import seaborn as sns\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "np.random.seed(42)\n",
                "\n",
                "# 2D 데이터 생성\n",
                "# Class +1\n",
                "X_pos = np.random.randn(20, 2) + np.array([2, 2])\n",
                "y_pos = np.ones(20)\n",
                "\n",
                "# Class -1\n",
                "X_neg = np.random.randn(20, 2) + np.array([-2, -2])\n",
                "y_neg = -np.ones(20)\n",
                "\n",
                "X = np.vstack([X_pos, X_neg])\n",
                "y = np.hstack([y_pos, y_neg])\n",
                "\n",
                "# 여러 개의 분리 초평면\n",
                "hyperplanes = [\n",
                "    {'w': np.array([1, 1]), 'b': 0},  # Good\n",
                "    {'w': np.array([1, 0.5]), 'b': 0.5},  # Bad (too close to positive)\n",
                "    {'w': np.array([0.5, 1]), 'b': -0.5},  # Bad (too close to negative)\n",
                "]\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "for idx, (ax, hp) in enumerate(zip(axes, hyperplanes)):\n",
                "    # 데이터 플롯\n",
                "    ax.scatter(X_pos[:, 0], X_pos[:, 1], c='red', marker='o', s=100, label='Class +1', edgecolors='k')\n",
                "    ax.scatter(X_neg[:, 0], X_neg[:, 1], c='blue', marker='s', s=100, label='Class -1', edgecolors='k')\n",
                "    \n",
                "    # 초평면 그리기\n",
                "    w = hp['w']\n",
                "    b = hp['b']\n",
                "    \n",
                "    xx = np.linspace(-5, 5, 100)\n",
                "    yy = -(w[0] * xx + b) / w[1]\n",
                "    ax.plot(xx, yy, 'g-', linewidth=2, label='Decision boundary')\n",
                "    \n",
                "    # Margin 계산\n",
                "    margins = []\n",
                "    for xi, yi in zip(X, y):\n",
                "        distance = np.abs(w @ xi + b) / np.linalg.norm(w)\n",
                "        margins.append(distance)\n",
                "    \n",
                "    min_margin = np.min(margins)\n",
                "    \n",
                "    ax.set_xlim(-5, 5)\n",
                "    ax.set_ylim(-5, 5)\n",
                "    ax.set_xlabel('$x_1$')\n",
                "    ax.set_ylabel('$x_2$')\n",
                "    ax.set_title(f'Hyperplane {idx+1}\\nMargin = {min_margin:.2f}')\n",
                "    ax.legend()\n",
                "    ax.grid(alpha=0.3)\n",
                "    ax.set_aspect('equal')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"관찰: 첫 번째 초평면이 가장 큰 margin을 가집니다!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 왜 Margin = 1로 정규화하는가?\n",
                "\n",
                "### 2.1 스케일 불변성 (Scale Invariance)\n",
                "\n",
                "초평면 $w^T x + b = 0$을 $c(w^T x + b) = 0$ (단, $c > 0$)으로 스케일해도 **같은 초평면**입니다!\n",
                "\n",
                "- $(w, b)$와 $(cw, cb)$는 같은 초평면을 정의\n",
                "- 따라서 $(w, b)$에 **자유도**가 있음\n",
                "\n",
                "### 2.2 정규화 (Normalization)\n",
                "\n",
                "이 자유도를 이용하여 **가장 가까운 점**에서:\n",
                "\n",
                "$$|w^T x_{\\text{closest}} + b| = 1$$\n",
                "\n",
                "로 **정규화**할 수 있습니다.\n",
                "\n",
                "### 2.3 결과: Canonical Form\n",
                "\n",
                "정규화 후:\n",
                "- Positive class: $w^T x_i + b \\geq +1$ for $y_i = +1$\n",
                "- Negative class: $w^T x_i + b \\leq -1$ for $y_i = -1$\n",
                "\n",
                "합쳐서:\n",
                "$$y_i(w^T x_i + b) \\geq 1 \\quad \\forall i$$\n",
                "\n",
                "**Margin** (초평면에서 서포트 벡터까지 거리):\n",
                "$$\\gamma = \\frac{1}{||w||}$$\n",
                "\n",
                "### 2.4 왜 1인가?\n",
                "\n",
                "**답**: 임의의 선택! 0.5나 2로 해도 되지만:\n",
                "- 1이 가장 **간단**\n",
                "- 수식이 **깔끔**해짐\n",
                "- 전체 margin = $\\frac{2}{||w||}$ (양쪽 합)\n",
                "\n",
                "**핵심**: 스케일 불변성 덕분에 어떤 값으로 정규화해도 **본질적으로 같은 문제**입니다!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 스케일 불변성 시각화\n",
                "\n",
                "# 원래 초평면\n",
                "w = np.array([1, 1])\n",
                "b = 0\n",
                "\n",
                "# 스케일된 버전들\n",
                "scales = [0.5, 1, 2, 3]\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# 데이터\n",
                "X_pos_simple = np.array([[2, 1], [3, 2]])\n",
                "X_neg_simple = np.array([[-2, -1], [-3, -2]])\n",
                "\n",
                "# 같은 초평면\n",
                "ax = axes[0]\n",
                "ax.scatter(X_pos_simple[:, 0], X_pos_simple[:, 1], c='red', marker='o', s=150, edgecolors='k', label='Class +1')\n",
                "ax.scatter(X_neg_simple[:, 0], X_neg_simple[:, 1], c='blue', marker='s', s=150, edgecolors='k', label='Class -1')\n",
                "\n",
                "xx = np.linspace(-4, 4, 100)\n",
                "colors = plt.cm.rainbow(np.linspace(0, 1, len(scales)))\n",
                "\n",
                "for scale, color in zip(scales, colors):\n",
                "    w_scaled = scale * w\n",
                "    b_scaled = scale * b\n",
                "    yy = -(w_scaled[0] * xx + b_scaled) / w_scaled[1]\n",
                "    ax.plot(xx, yy, color=color, linewidth=2, label=f'c={scale}')\n",
                "\n",
                "ax.set_xlim(-4, 4)\n",
                "ax.set_ylim(-4, 4)\n",
                "ax.set_xlabel('$x_1$')\n",
                "ax.set_ylabel('$x_2$')\n",
                "ax.set_title('스케일이 달라도 같은 초평면!')\n",
                "ax.legend()\n",
                "ax.grid(alpha=0.3)\n",
                "ax.set_aspect('equal')\n",
                "\n",
                "# Margin 변화\n",
                "ax = axes[1]\n",
                "margins = [1 / (scale * np.linalg.norm(w)) for scale in scales]\n",
                "ax.bar(range(len(scales)), margins, color=colors, edgecolor='black', linewidth=2)\n",
                "ax.set_xlabel('스케일 c')\n",
                "ax.set_ylabel('Margin (1/||cw||)')\n",
                "ax.set_title('스케일에 따른 Margin 변화')\n",
                "ax.set_xticks(range(len(scales)))\n",
                "ax.set_xticklabels([f'c={s}' for s in scales])\n",
                "ax.grid(alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"=== 스케일 불변성 ===\")\n",
                "for scale in scales:\n",
                "    w_scaled = scale * w\n",
                "    b_scaled = scale * b\n",
                "    margin = 1 / np.linalg.norm(w_scaled)\n",
                "    print(f\"c = {scale}: w = {w_scaled}, margin = {margin:.4f}\")\n",
                "print(\"\\n→ 스케일이 커질수록 ||w||도 커져서 margin이 작아짐\")\n",
                "print(\"→ Canonical form (margin=1 at support vectors)으로 정규화하면 ||w||이 고정됨!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Primal Problem\n",
                "\n",
                "### 3.1 최적화 문제 정식화\n",
                "\n",
                "**목표**: Margin $\\gamma = \\frac{1}{||w||}$을 최대화\n",
                "\n",
                "이것은 $||w||$를 최소화하는 것과 같습니다.\n",
                "\n",
                "편의상 $\\frac{1}{2}||w||^2$을 최소화합니다 (미분이 깔끔):\n",
                "\n",
                "$$\\begin{align}\n",
                "\\min_{w, b} \\quad & \\frac{1}{2}||w||^2 \\\\\n",
                "\\text{subject to} \\quad & y_i(w^T x_i + b) \\geq 1, \\quad i = 1, \\ldots, n\n",
                "\\end{align}$$\n",
                "\n",
                "이것이 **Primal Problem**입니다!\n",
                "\n",
                "### 3.2 문제의 성질\n",
                "\n",
                "- **Convex optimization**: 목적 함수가 convex, 제약 조건이 linear\n",
                "- **Quadratic Programming (QP)**: 목적 함수가 이차식\n",
                "- **해가 유일**: Strictly convex하므로 global minimum이 유일"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Lagrangian과 KKT 조건\n",
                "\n",
                "### 4.1 Lagrangian 함수\n",
                "\n",
                "제약 조건을 Lagrange multiplier $\\alpha_i \\geq 0$로 통합:\n",
                "\n",
                "$$\\mathcal{L}(w, b, \\alpha) = \\frac{1}{2}||w||^2 - \\sum_{i=1}^{n} \\alpha_i [y_i(w^T x_i + b) - 1]$$\n",
                "\n",
                "**왜 마이너스?** \n",
                "- 제약: $y_i(w^T x_i + b) - 1 \\geq 0$\n",
                "- 위반하면 penalty: $-\\alpha_i \\times (\\text{negative value}) = \\text{positive penalty}$\n",
                "\n",
                "### 4.2 Primal Formulation\n",
                "\n",
                "$$\\min_{w, b} \\max_{\\alpha \\geq 0} \\mathcal{L}(w, b, \\alpha)$$\n",
                "\n",
                "**해석**:\n",
                "- 제약을 만족하면: $\\max_{\\alpha} \\mathcal{L} = \\frac{1}{2}||w||^2$ ($\\alpha_i = 0$에서 최대)\n",
                "- 제약을 위반하면: $\\max_{\\alpha} \\mathcal{L} = +\\infty$ ($\\alpha_i \\to \\infty$)\n",
                "\n",
                "### 4.3 Dual Formulation\n",
                "\n",
                "Convex optimization에서 **Dual problem**:\n",
                "\n",
                "$$\\max_{\\alpha \\geq 0} \\min_{w, b} \\mathcal{L}(w, b, \\alpha)$$\n",
                "\n",
                "**Strong duality** (Slater's condition 만족):\n",
                "$$\\min_{w, b} \\max_{\\alpha} \\mathcal{L} = \\max_{\\alpha} \\min_{w, b} \\mathcal{L}$$\n",
                "\n",
                "따라서 두 문제의 **최적값이 같습니다**!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Dual Problem 유도\n",
                "\n",
                "### 5.1 Step 1: $\\mathcal{L}$을 $w, b$에 대해 최소화\n",
                "\n",
                "$$\\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_{i=1}^{n} \\alpha_i y_i x_i = 0$$\n",
                "\n",
                "$$\\Rightarrow w = \\sum_{i=1}^{n} \\alpha_i y_i x_i$$\n",
                "\n",
                "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{n} \\alpha_i y_i = 0$$\n",
                "\n",
                "$$\\Rightarrow \\sum_{i=1}^{n} \\alpha_i y_i = 0$$\n",
                "\n",
                "### 5.2 Step 2: 대입해서 $\\alpha$만의 함수로\n",
                "\n",
                "$w = \\sum_i \\alpha_i y_i x_i$를 $\\mathcal{L}$에 대입:\n",
                "\n",
                "$$\\begin{align}\n",
                "\\mathcal{L} &= \\frac{1}{2}w^T w - \\sum_i \\alpha_i y_i w^T x_i - b\\sum_i \\alpha_i y_i + \\sum_i \\alpha_i \\\\\n",
                "&= \\frac{1}{2}w^T w - w^T w - 0 + \\sum_i \\alpha_i \\quad (\\text{since } \\sum_i \\alpha_i y_i = 0, w^T x_i = \\sum_j \\alpha_j y_j x_j^T x_i) \\\\\n",
                "&= -\\frac{1}{2}w^T w + \\sum_i \\alpha_i \\\\\n",
                "&= -\\frac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_i \\alpha_i\n",
                "\\end{align}$$\n",
                "\n",
                "### 5.3 Dual Problem\n",
                "\n",
                "$$\\begin{align}\n",
                "\\max_{\\alpha} \\quad & \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j \\\\\n",
                "\\text{subject to} \\quad & \\alpha_i \\geq 0, \\quad i = 1, \\ldots, n \\\\\n",
                "& \\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
                "\\end{align}$$\n",
                "\n",
                "또는 최소화 형태로:\n",
                "\n",
                "$$\\begin{align}\n",
                "\\min_{\\alpha} \\quad & \\frac{1}{2}\\sum_{i,j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^{n} \\alpha_i \\\\\n",
                "\\text{subject to} \\quad & \\alpha_i \\geq 0, \\quad i = 1, \\ldots, n \\\\\n",
                "& \\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
                "\\end{align}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dual problem을 직접 풀어보기 (2D 예제)\n",
                "from scipy.optimize import minimize\n",
                "\n",
                "# 간단한 데이터\n",
                "X_train = np.array([\n",
                "    [1, 2],\n",
                "    [2, 3],\n",
                "    [-1, -2],\n",
                "    [-2, -3]\n",
                "])\n",
                "y_train = np.array([1, 1, -1, -1])\n",
                "\n",
                "n_samples = len(y_train)\n",
                "\n",
                "# Gram matrix: K[i,j] = x_i^T x_j\n",
                "K = X_train @ X_train.T\n",
                "\n",
                "# Dual objective: 1/2 * α^T Q α - 1^T α\n",
                "# where Q[i,j] = y_i * y_j * K[i,j]\n",
                "Q = np.outer(y_train, y_train) * K\n",
                "\n",
                "def dual_objective(alpha):\n",
                "    \"\"\"Dual objective (to minimize)\"\"\"\n",
                "    return 0.5 * alpha @ Q @ alpha - np.sum(alpha)\n",
                "\n",
                "def dual_gradient(alpha):\n",
                "    \"\"\"Gradient of dual objective\"\"\"\n",
                "    return Q @ alpha - np.ones(n_samples)\n",
                "\n",
                "# 제약 조건: sum(α_i * y_i) = 0\n",
                "constraints = {'type': 'eq', 'fun': lambda alpha: np.sum(alpha * y_train)}\n",
                "\n",
                "# 경계: α_i >= 0\n",
                "bounds = [(0, None) for _ in range(n_samples)]\n",
                "\n",
                "# 초기값\n",
                "alpha_init = np.ones(n_samples)\n",
                "\n",
                "# 최적화\n",
                "result = minimize(\n",
                "    dual_objective,\n",
                "    alpha_init,\n",
                "    method='SLSQP',\n",
                "    jac=dual_gradient,\n",
                "    bounds=bounds,\n",
                "    constraints=constraints\n",
                ")\n",
                "\n",
                "alpha_opt = result.x\n",
                "\n",
                "print(\"=== Dual Problem 풀이 ===\")\n",
                "print(f\"최적 α: {alpha_opt}\")\n",
                "print(f\"Objective value: {result.fun:.4f}\")\n",
                "\n",
                "# w 복원: w = Σ α_i y_i x_i\n",
                "w_opt = np.sum(alpha_opt[:, np.newaxis] * y_train[:, np.newaxis] * X_train, axis=0)\n",
                "print(f\"\\n복원된 w: {w_opt}\")\n",
                "\n",
                "# Support vectors (α > 0인 점들)\n",
                "sv_indices = np.where(alpha_opt > 1e-5)[0]\n",
                "print(f\"\\nSupport vector indices: {sv_indices}\")\n",
                "print(f\"Support vectors:\")\n",
                "for idx in sv_indices:\n",
                "    print(f\"  x_{idx} = {X_train[idx]}, y_{idx} = {y_train[idx]}, α_{idx} = {alpha_opt[idx]:.4f}\")\n",
                "\n",
                "# b 계산: y_i(w^T x_i + b) = 1 for support vectors\n",
                "b_values = []\n",
                "for idx in sv_indices:\n",
                "    b_val = y_train[idx] - w_opt @ X_train[idx]\n",
                "    b_values.append(b_val)\n",
                "b_opt = np.mean(b_values)\n",
                "print(f\"\\n복원된 b: {b_opt:.4f}\")\n",
                "\n",
                "# 검증\n",
                "print(\"\\n=== 검증 ===\")\n",
                "for i in range(n_samples):\n",
                "    decision = w_opt @ X_train[i] + b_opt\n",
                "    margin = y_train[i] * decision\n",
                "    print(f\"x_{i}: y * (w^T x + b) = {margin:.4f} (should be >= 1)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. KKT 조건 (Karush-Kuhn-Tucker Conditions)\n",
                "\n",
                "### 6.1 KKT 조건이란?\n",
                "\n",
                "제약이 있는 최적화 문제의 **필요충분조건**:\n",
                "\n",
                "1. **Stationarity** (정류조건):\n",
                "   $$\\nabla_w \\mathcal{L} = 0, \\quad \\nabla_b \\mathcal{L} = 0$$\n",
                "\n",
                "2. **Primal feasibility** (주문제 실행가능성):\n",
                "   $$y_i(w^T x_i + b) \\geq 1, \\quad \\forall i$$\n",
                "\n",
                "3. **Dual feasibility** (쌍대 실행가능성):\n",
                "   $$\\alpha_i \\geq 0, \\quad \\forall i$$\n",
                "\n",
                "4. **Complementary slackness** (상보성):\n",
                "   $$\\alpha_i [y_i(w^T x_i + b) - 1] = 0, \\quad \\forall i$$\n",
                "\n",
                "### 6.2 Complementary Slackness의 의미\n",
                "\n",
                "각 데이터 포인트 $i$에 대해 **둘 중 하나**가 성립:\n",
                "\n",
                "1. $\\alpha_i = 0$: 제약이 inactive (margin 밖, 분류에 영향 없음)\n",
                "2. $y_i(w^T x_i + b) = 1$: 제약이 active (margin 위, **support vector**)\n",
                "\n",
                "**핵심**:\n",
                "- $\\alpha_i > 0$ ⇔ $x_i$는 support vector\n",
                "- 대부분의 $\\alpha_i = 0$ ⇒ **Sparse solution**!\n",
                "\n",
                "### 6.3 Geometric Interpretation\n",
                "\n",
                "- **Support vectors**: Margin 경계에 정확히 위치한 점들\n",
                "- **Non-support vectors**: Margin 밖에 있는 점들 ($\\alpha_i = 0$)\n",
                "- 초평면은 **오직 support vectors만**으로 결정됩니다!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# KKT 조건 시각화\n",
                "\n",
                "# Support vectors 하이라이트\n",
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "# 모든 데이터\n",
                "for i in range(n_samples):\n",
                "    if i in sv_indices:\n",
                "        # Support vector\n",
                "        if y_train[i] == 1:\n",
                "            plt.scatter(X_train[i, 0], X_train[i, 1], c='red', marker='o', s=400, \n",
                "                       edgecolors='black', linewidths=3, label='SV (+1)' if i == sv_indices[0] else '')\n",
                "        else:\n",
                "            plt.scatter(X_train[i, 0], X_train[i, 1], c='blue', marker='s', s=400,\n",
                "                       edgecolors='black', linewidths=3, label='SV (-1)' if y_train[i] == -1 and i == sv_indices[1] else '')\n",
                "    else:\n",
                "        # Non-support vector\n",
                "        if y_train[i] == 1:\n",
                "            plt.scatter(X_train[i, 0], X_train[i, 1], c='pink', marker='o', s=200,\n",
                "                       edgecolors='gray', linewidths=2, alpha=0.5)\n",
                "        else:\n",
                "            plt.scatter(X_train[i, 0], X_train[i, 1], c='lightblue', marker='s', s=200,\n",
                "                       edgecolors='gray', linewidths=2, alpha=0.5)\n",
                "\n",
                "# Decision boundary\n",
                "xx = np.linspace(-4, 4, 100)\n",
                "yy = -(w_opt[0] * xx + b_opt) / w_opt[1]\n",
                "plt.plot(xx, yy, 'g-', linewidth=3, label='Decision boundary')\n",
                "\n",
                "# Margins\n",
                "yy_margin_pos = -(w_opt[0] * xx + b_opt - 1) / w_opt[1]\n",
                "yy_margin_neg = -(w_opt[0] * xx + b_opt + 1) / w_opt[1]\n",
                "plt.plot(xx, yy_margin_pos, 'r--', linewidth=2, label='Margin (+1)')\n",
                "plt.plot(xx, yy_margin_neg, 'b--', linewidth=2, label='Margin (-1)')\n",
                "\n",
                "# Normal vector\n",
                "origin = np.array([0, 0])\n",
                "plt.arrow(origin[0], origin[1], w_opt[0], w_opt[1], \n",
                "         head_width=0.3, head_length=0.3, fc='green', ec='green', linewidth=2)\n",
                "plt.text(w_opt[0] + 0.5, w_opt[1] + 0.5, 'w', fontsize=16, fontweight='bold')\n",
                "\n",
                "plt.xlabel('$x_1$', fontsize=14)\n",
                "plt.ylabel('$x_2$', fontsize=14)\n",
                "plt.title('SVM Hard Margin: Support Vectors', fontsize=16, fontweight='bold')\n",
                "plt.legend(fontsize=12)\n",
                "plt.grid(alpha=0.3)\n",
                "plt.axis('equal')\n",
                "plt.xlim(-4, 4)\n",
                "plt.ylim(-5, 5)\n",
                "plt.show()\n",
                "\n",
                "# KKT 조건 검증\n",
                "print(\"\\n=== KKT 조건 검증 ===\")\n",
                "print(\"\\n1. Stationarity:\")\n",
                "print(f\"   ∇_w L = {w_opt - np.sum(alpha_opt[:, np.newaxis] * y_train[:, np.newaxis] * X_train, axis=0)}\")\n",
                "print(f\"   ∇_b L = {-np.sum(alpha_opt * y_train):.10f}\")\n",
                "\n",
                "print(\"\\n2. Primal feasibility:\")\n",
                "for i in range(n_samples):\n",
                "    margin = y_train[i] * (w_opt @ X_train[i] + b_opt)\n",
                "    print(f\"   y_{i}(w^T x_{i} + b) = {margin:.4f} >= 1: {margin >= 1 - 1e-6}\")\n",
                "\n",
                "print(\"\\n3. Dual feasibility:\")\n",
                "for i in range(n_samples):\n",
                "    print(f\"   α_{i} = {alpha_opt[i]:.4f} >= 0: {alpha_opt[i] >= -1e-10}\")\n",
                "\n",
                "print(\"\\n4. Complementary slackness:\")\n",
                "for i in range(n_samples):\n",
                "    slack = y_train[i] * (w_opt @ X_train[i] + b_opt) - 1\n",
                "    product = alpha_opt[i] * slack\n",
                "    print(f\"   α_{i} * [y_{i}(w^T x_{i} + b) - 1] = {product:.10f} ≈ 0: {abs(product) < 1e-6}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. 요약 및 핵심 공식\n",
                "\n",
                "### 7.1 SVM Hard Margin 핵심 정리\n",
                "\n",
                "| 항목 | 내용 |\n",
                "|------|------|\n",
                "| **목표** | Margin 최대화 |\n",
                "| **Margin** | $\\frac{1}{\\\\|w\\\\|}$ (한쪽), $\\frac{2}{\\\\|w\\\\|}$ (전체) |\n",
                "| **정규화** | $y_i(w^T x_i + b) \\geq 1$ (Canonical form) |\n",
                "| **Primal** | $\\min \\frac{1}{2}\\\\|w\\\\|^2$ s.t. $y_i(w^T x_i + b) \\geq 1$ |\n",
                "| **Dual** | $\\max \\sum_i \\alpha_i - \\frac{1}{2}\\sum_{ij} \\alpha_i \\alpha_j y_i y_j x_i^T x_j$ |\n",
                "| **Solution** | $w = \\sum_i \\alpha_i y_i x_i$ |\n",
                "| **Support Vectors** | $\\alpha_i > 0$ ⇔ $y_i(w^T x_i + b) = 1$ |\n",
                "\n",
                "### 7.2 왜 Dual을 푸는가?\n",
                "\n",
                "1. **Kernel trick**: Dual은 $x_i^T x_j$만 필요 → 커널로 대체 가능!\n",
                "2. **Sparsity**: 대부분의 $\\alpha_i = 0$ → 효율적\n",
                "3. **Convexity**: QP solver로 글로벌 최적해 보장\n",
                "\n",
                "### 7.3 한계\n",
                "\n",
                "**Hard Margin의 문제점**:\n",
                "- 선형 분리 가능해야만 작동\n",
                "- Outlier에 매우 민감\n",
                "- 실제 데이터는 대부분 **선형 분리 불가능**\n",
                "\n",
                "→ 다음 노트북에서 **Soft Margin SVM**으로 해결!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. 연습문제\n",
                "\n",
                "### 문제 1: Margin 계산\n",
                "\n",
                "다음 초평면과 점이 주어졌을 때:\n",
                "- $w = [3, 4]^T$, $b = 0$\n",
                "- $x_0 = [1, 2]^T$\n",
                "\n",
                "1. 점 $x_0$에서 초평면까지의 거리를 계산하시오.\n",
                "2. $w$를 정규화하여 margin이 1이 되도록 스케일하시오."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 문제 1 풀이\n",
                "\n",
                "w = np.array([3, 4])\n",
                "b = 0\n",
                "x0 = np.array([1, 2])\n",
                "\n",
                "# 1. 거리 계산\n",
                "distance = np.abs(w @ x0 + b) / np.linalg.norm(w)\n",
                "print(\"=== 문제 1 ===\")\n",
                "print(f\"1. 거리 = |w^T x0 + b| / ||w|| = {distance:.4f}\")\n",
                "\n",
                "# 2. 정규화\n",
                "# margin이 1이 되려면: |w^T x0 + b| = 1\n",
                "# 현재: |w^T x0 + b| = {w @ x0 + b}\n",
                "# 스케일: c = 1 / |w^T x0 + b|\n",
                "scale = 1 / np.abs(w @ x0 + b)\n",
                "w_normalized = scale * w\n",
                "b_normalized = scale * b\n",
                "\n",
                "print(f\"\\n2. 정규화:\")\n",
                "print(f\"   원본: w = {w}, b = {b}\")\n",
                "print(f\"   스케일 c = {scale:.4f}\")\n",
                "print(f\"   정규화: w = {w_normalized}, b = {b_normalized}\")\n",
                "print(f\"   검증: |w^T x0 + b| = {np.abs(w_normalized @ x0 + b_normalized):.4f}\")\n",
                "print(f\"   Margin = 1 / ||w|| = {1 / np.linalg.norm(w_normalized):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 문제 2: Dual 유도 연습\n",
                "\n",
                "Lagrangian에서 Dual을 유도하는 과정을 직접 계산하시오:\n",
                "\n",
                "1. $\\mathcal{L}(w, b, \\alpha) = \\frac{1}{2}||w||^2 - \\sum_i \\alpha_i [y_i(w^T x_i + b) - 1]$\n",
                "2. $\\frac{\\partial \\mathcal{L}}{\\partial w} = 0$에서 $w$를 구하시오.\n",
                "3. $\\frac{\\partial \\mathcal{L}}{\\partial b} = 0$에서 제약 조건을 구하시오.\n",
                "4. 위의 결과를 $\\mathcal{L}$에 대입하여 Dual objective를 유도하시오."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 문제 2 풀이 (수식 유도는 위 마크다운 참조)\n",
                "\n",
                "print(\"=== 문제 2: Dual 유도 ===\")\n",
                "print(\"\\n1. Lagrangian:\")\n",
                "print(\"   L(w, b, α) = (1/2)||w||² - Σ αᵢ[yᵢ(w^T xᵢ + b) - 1]\")\n",
                "\n",
                "print(\"\\n2. ∂L/∂w = 0:\")\n",
                "print(\"   w - Σ αᵢ yᵢ xᵢ = 0\")\n",
                "print(\"   → w = Σ αᵢ yᵢ xᵢ\")\n",
                "\n",
                "print(\"\\n3. ∂L/∂b = 0:\")\n",
                "print(\"   -Σ αᵢ yᵢ = 0\")\n",
                "print(\"   → Σ αᵢ yᵢ = 0\")\n",
                "\n",
                "print(\"\\n4. Dual objective (대입 후):\")\n",
                "print(\"   L(α) = Σ αᵢ - (1/2)Σᵢⱼ αᵢ αⱼ yᵢ yⱼ xᵢ^T xⱼ\")\n",
                "print(\"   subject to: αᵢ >= 0, Σ αᵢ yᵢ = 0\")\n",
                "\n",
                "# 수치 예제로 검증\n",
                "print(\"\\n=== 수치 검증 ===\")\n",
                "# 앞서 구한 최적해 사용\n",
                "print(f\"최적 α: {alpha_opt}\")\n",
                "print(f\"w = Σ αᵢ yᵢ xᵢ = {w_opt}\")\n",
                "print(f\"Σ αᵢ yᵢ = {np.sum(alpha_opt * y_train):.10f} (should be 0)\")\n",
                "\n",
                "# Dual objective value\n",
                "dual_val = np.sum(alpha_opt) - 0.5 * alpha_opt @ Q @ alpha_opt\n",
                "primal_val = 0.5 * np.linalg.norm(w_opt)**2\n",
                "print(f\"\\nDual objective = {-dual_val:.4f}\")\n",
                "print(f\"Primal objective = {primal_val:.4f}\")\n",
                "print(f\"Duality gap = {abs(primal_val - (-dual_val)):.10f} (should be 0)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. 추가 자료\n",
                "\n",
                "### 참고 논문 및 교재\n",
                "\n",
                "1. **강의 자료**:\n",
                "   - ML_L14a_Constrained.Optimization_part1.pdf\n",
                "   - ML_L14b_Constrained.Optimization_part2.pdf\n",
                "   - ML_L15a_SVM_(linear-hard).pdf\n",
                "\n",
                "2. **Online Resources**:\n",
                "   - [MIT OpenCourseWare: Optimization](https://ocw.mit.edu/)\n",
                "   - [Stanford CS229: SVM Notes](http://cs229.stanford.edu/notes/)\n",
                "\n",
                "3. **YouTube**:\n",
                "   - StatQuest: Support Vector Machines\n",
                "   - 3Blue1Brown: Lagrange Multipliers\n",
                "\n",
                "### 다음 단계\n",
                "\n",
                "- 다음 노트북: **SVM Soft Margin** (Slack variables, C parameter)\n",
                "- 추가 학습: Kernel SVM (ML_L15c_SVM_(kernel).pdf)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "WSL ML (venv)",
            "language": "python",
            "name": "wsl-ml"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
