{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. 행렬 Gradient 연산 심화\n",
    "\n",
    "## 목표\n",
    "- 행렬 미분에서 **전치(transpose)**가 어디에 붙는지 완벽하게 이해\n",
    "- 벡터/행렬 미분의 핵심 패턴 학습\n",
    "- ML에서 자주 나오는 gradient 계산 마스터\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 기초: 왜 전치가 나타나는가?\n",
    "\n",
    "### 1.1 기본 규칙\n",
    "\n",
    "**핵심 원칙**: Gradient는 항상 **원래 변수와 같은 shape**을 가져야 한다!\n",
    "\n",
    "- $x \\in \\mathbb{R}^{n}$ (column vector)이면 $\\frac{\\partial f}{\\partial x} \\in \\mathbb{R}^{n}$ (column vector)\n",
    "- $A \\in \\mathbb{R}^{m \\times n}$이면 $\\frac{\\partial f}{\\partial A} \\in \\mathbb{R}^{m \\times n}$\n",
    "\n",
    "### 1.2 Numerator Layout vs Denominator Layout\n",
    "\n",
    "우리는 **Denominator Layout**을 사용합니다 (ML에서 표준):\n",
    "- $\\frac{\\partial (Ax)}{\\partial x} = A^T$ ← 전치가 붙는다!\n",
    "- $\\frac{\\partial (x^T A)}{\\partial x} = A$ ← 전치 없음!\n",
    "\n",
    "> **왜?** Chain rule과 차원 맞추기 때문입니다. 아래에서 자세히 설명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 핵심 공식들\n",
    "\n",
    "### 2.1 선형 변환\n",
    "\n",
    "#### 공식 1: $\\frac{\\partial (Ax)}{\\partial x} = A^T$\n",
    "\n",
    "**유도:**\n",
    "- $y = Ax$라고 하자. $y \\in \\mathbb{R}^m, x \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{m \\times n}$\n",
    "- $y_i = \\sum_{j=1}^{n} A_{ij} x_j$\n",
    "- $\\frac{\\partial y_i}{\\partial x_k} = A_{ik}$\n",
    "- 따라서 Jacobian은: $J = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \\end{bmatrix} = A$\n",
    "\n",
    "하지만 우리가 원하는 것은 scalar function $f(y)$의 $x$에 대한 gradient:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial x} = \\frac{\\partial f}{\\partial y} J^T = \\frac{\\partial f}{\\partial y} A^T$$\n",
    "\n",
    "만약 $f(y) = y$ (identity)라면:\n",
    "$$\\frac{\\partial (Ax)}{\\partial x} = A^T$$\n",
    "\n",
    "#### 공식 2: $\\frac{\\partial (x^T A)}{\\partial x} = A$\n",
    "\n",
    "**유도:**\n",
    "- $z = x^T A$, $z \\in \\mathbb{R}^{1 \\times m}$ (row vector)\n",
    "- $z_j = \\sum_{i=1}^{n} x_i A_{ij}$\n",
    "- $\\frac{\\partial z_j}{\\partial x_k} = A_{kj}$\n",
    "- 따라서: $\\frac{\\partial (x^T A)}{\\partial x} = A$ (전치 없음!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 공식 검증: d(Ax)/dx = A^T\n",
    "np.random.seed(42)\n",
    "\n",
    "# A: 3x2 행렬, x: 2x1 벡터\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])\n",
    "x = np.array([[1],\n",
    "              [2]])\n",
    "\n",
    "# Ax 계산\n",
    "y = A @ x\n",
    "print(\"A shape:\", A.shape)\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"Ax shape:\", y.shape)\n",
    "print(\"\\nAx =\", y.T)\n",
    "\n",
    "# Gradient는 A^T\n",
    "print(\"\\nGradient d(Ax)/dx = A^T:\")\n",
    "print(A.T)\n",
    "print(\"Shape:\", A.T.shape, \"← x와 같은 shape (2x1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 이차 형식 (Quadratic Forms)\n",
    "\n",
    "#### 공식 3: $\\frac{\\partial (x^T A x)}{\\partial x} = (A + A^T)x$\n",
    "\n",
    "**유도:**\n",
    "- $f(x) = x^T A x = \\sum_{i,j} x_i A_{ij} x_j$\n",
    "- $\\frac{\\partial f}{\\partial x_k} = \\sum_j A_{kj} x_j + \\sum_i x_i A_{ik}$\n",
    "- $= (Ax)_k + (A^T x)_k$\n",
    "- $= [(A + A^T)x]_k$\n",
    "\n",
    "**특별한 경우: A가 대칭행렬**\n",
    "- $A = A^T$이면: $\\frac{\\partial (x^T A x)}{\\partial x} = 2Ax$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공식 검증: d(x^T A x)/dx\n",
    "\n",
    "# 대칭 행렬\n",
    "A_sym = np.array([[2, 1],\n",
    "                   [1, 3]])\n",
    "x = np.array([[1],\n",
    "              [2]])\n",
    "\n",
    "# x^T A x 계산\n",
    "quad_form = x.T @ A_sym @ x\n",
    "print(\"x^T A x =\", quad_form[0, 0])\n",
    "\n",
    "# Gradient (대칭이므로 2Ax)\n",
    "grad = 2 * A_sym @ x\n",
    "print(\"\\nGradient d(x^T A x)/dx = 2Ax:\")\n",
    "print(grad.T)\n",
    "\n",
    "# 비대칭 행렬인 경우\n",
    "A_nonsym = np.array([[1, 2],\n",
    "                      [3, 4]])\n",
    "quad_form2 = x.T @ A_nonsym @ x\n",
    "print(\"\\n\\n비대칭 행렬 경우:\")\n",
    "print(\"x^T A x =\", quad_form2[0, 0])\n",
    "\n",
    "grad2 = (A_nonsym + A_nonsym.T) @ x\n",
    "print(\"Gradient d(x^T A x)/dx = (A + A^T)x:\")\n",
    "print(grad2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 선형 항 + 이차 항\n",
    "\n",
    "#### 공식 4: $\\frac{\\partial (b^T x + x^T A x)}{\\partial x} = b + (A + A^T)x$\n",
    "\n",
    "이것은 **Linear Regression의 gradient**입니다!\n",
    "\n",
    "**예제: MSE Loss**\n",
    "\n",
    "$$L(w) = \\frac{1}{2}||Xw - y||^2 = \\frac{1}{2}(Xw - y)^T(Xw - y)$$\n",
    "\n",
    "전개하면:\n",
    "$$L(w) = \\frac{1}{2}w^T X^T X w - w^T X^T y + \\frac{1}{2}y^T y$$\n",
    "\n",
    "Gradient:\n",
    "$$\\frac{\\partial L}{\\partial w} = X^T X w - X^T y$$\n",
    "\n",
    "최적해 (gradient = 0):\n",
    "$$w^* = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression gradient 유도 검증\n",
    "\n",
    "# 데이터 생성\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 3\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "w_true = np.array([[2], [3], [-1]])\n",
    "y = X @ w_true + np.random.randn(n_samples, 1) * 0.1\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Gradient 계산\n",
    "def compute_gradient(X, y, w):\n",
    "    \"\"\"MSE loss의 gradient\"\"\"\n",
    "    return X.T @ (X @ w - y)\n",
    "\n",
    "# 초기 w\n",
    "w_init = np.zeros((n_features, 1))\n",
    "grad_init = compute_gradient(X, y, w_init)\n",
    "\n",
    "print(\"\\nInitial gradient:\")\n",
    "print(grad_init.T)\n",
    "\n",
    "# Closed-form solution\n",
    "w_optimal = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "print(\"\\nOptimal w:\")\n",
    "print(w_optimal.T)\n",
    "print(\"\\nTrue w:\")\n",
    "print(w_true.T)\n",
    "\n",
    "# 최적해에서 gradient는 0이어야 함\n",
    "grad_optimal = compute_gradient(X, y, w_optimal)\n",
    "print(\"\\nGradient at optimal (should be ~0):\")\n",
    "print(grad_optimal.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 자주 헷갈리는 케이스들\n",
    "\n",
    "### 3.1 Chain Rule과 전치\n",
    "\n",
    "**문제**: $f(x) = ||Ax - b||^2$의 gradient를 구하라.\n",
    "\n",
    "**풀이**:\n",
    "\n",
    "$$f(x) = (Ax - b)^T(Ax - b)$$\n",
    "\n",
    "$u = Ax - b$로 놓으면:\n",
    "$$f = u^T u$$\n",
    "\n",
    "Chain rule:\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial u} \\frac{\\partial u}{\\partial x}$$\n",
    "\n",
    "- $\\frac{\\partial f}{\\partial u} = \\frac{\\partial (u^T u)}{\\partial u} = 2u^T$ (row vector!)\n",
    "- $\\frac{\\partial u}{\\partial x} = \\frac{\\partial (Ax)}{\\partial x} = A^T$ ← **전치!**\n",
    "\n",
    "하지만 차원을 맞춰야 합니다:\n",
    "- $(2u^T) \\cdot A^T$는 차원이 안 맞습니다!\n",
    "- 올바른 형태: $\\frac{\\partial f}{\\partial x} = A^T (2u) = 2A^T(Ax - b)$\n",
    "\n",
    "**결론**: $\\frac{\\partial ||Ax - b||^2}{\\partial x} = 2A^T(Ax - b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ||Ax - b||^2의 gradient 검증\n",
    "\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])\n",
    "x = np.array([[1],\n",
    "              [1]])\n",
    "b = np.array([[1],\n",
    "              [2],\n",
    "              [3]])\n",
    "\n",
    "# Loss 계산\n",
    "residual = A @ x - b\n",
    "loss = np.sum(residual ** 2)\n",
    "print(\"Loss ||Ax - b||^2 =\", loss)\n",
    "\n",
    "# Analytical gradient\n",
    "grad_analytical = 2 * A.T @ residual\n",
    "print(\"\\nAnalytical gradient:\")\n",
    "print(grad_analytical.T)\n",
    "\n",
    "# Numerical gradient (유한 차분)\n",
    "eps = 1e-5\n",
    "grad_numerical = np.zeros_like(x)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    x_plus = x.copy()\n",
    "    x_plus[i] += eps\n",
    "    loss_plus = np.sum((A @ x_plus - b) ** 2)\n",
    "    \n",
    "    x_minus = x.copy()\n",
    "    x_minus[i] -= eps\n",
    "    loss_minus = np.sum((A @ x_minus - b) ** 2)\n",
    "    \n",
    "    grad_numerical[i] = (loss_plus - loss_minus) / (2 * eps)\n",
    "\n",
    "print(\"\\nNumerical gradient:\")\n",
    "print(grad_numerical.T)\n",
    "\n",
    "print(\"\\nDifference (should be very small):\")\n",
    "print(np.max(np.abs(grad_analytical - grad_numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Softmax와 Cross-Entropy의 Gradient\n",
    "\n",
    "**Softmax**: $\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$\n",
    "\n",
    "**Cross-Entropy Loss**: $L = -\\sum_i y_i \\log \\sigma(z)_i$\n",
    "\n",
    "**Gradient**: $\\frac{\\partial L}{\\partial z} = \\sigma(z) - y$\n",
    "\n",
    "이 유도는 다음 노트북(확률론)에서 자세히 다룹니다!\n",
    "\n",
    "**핵심**: Softmax + Cross-Entropy를 합치면 gradient가 **매우 간단해집니다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax + Cross-Entropy gradient\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Numerically stable softmax\"\"\"\n",
    "    exp_z = np.exp(z - np.max(z))  # overflow 방지\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    \"\"\"Cross-entropy loss\"\"\"\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-10))  # log(0) 방지\n",
    "\n",
    "# 예제\n",
    "z = np.array([1.0, 2.0, 3.0])  # logits\n",
    "y_true = np.array([0, 0, 1])   # one-hot encoded label\n",
    "\n",
    "# Forward pass\n",
    "y_pred = softmax(z)\n",
    "loss = cross_entropy(y_true, y_pred)\n",
    "\n",
    "print(\"Logits z:\", z)\n",
    "print(\"Softmax σ(z):\", y_pred)\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Gradient: σ(z) - y\n",
    "grad = y_pred - y_true\n",
    "print(\"\\nGradient dL/dz:\")\n",
    "print(grad)\n",
    "print(\"\\n해석: 정답 클래스(index 2)에서 gradient가 음수 → z[2]를 증가시켜야 함\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 행렬에 대한 미분\n",
    "\n",
    "### 4.1 Trace Trick\n",
    "\n",
    "**Trace의 성질**:\n",
    "- $\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)$\n",
    "- $\\text{tr}(AB) = \\text{tr}(BA)$ (cyclic property)\n",
    "- $\\text{tr}(A^T) = \\text{tr}(A)$\n",
    "- $x^T A x = \\text{tr}(x^T A x) = \\text{tr}(A x x^T)$\n",
    "\n",
    "### 4.2 행렬 미분 공식\n",
    "\n",
    "$$\\frac{\\partial \\text{tr}(AB)}{\\partial A} = B^T$$\n",
    "\n",
    "$$\\frac{\\partial \\text{tr}(A^T B)}{\\partial A} = B$$\n",
    "\n",
    "$$\\frac{\\partial \\log |A|}{\\partial A} = (A^{-1})^T = (A^T)^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace 성질 검증\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "x = np.array([[1], [2]])\n",
    "\n",
    "# tr(AB) = tr(BA)\n",
    "print(\"tr(AB) =\", np.trace(A @ B))\n",
    "print(\"tr(BA) =\", np.trace(B @ A))\n",
    "\n",
    "# x^T A x = tr(A x x^T)\n",
    "quad_form = x.T @ A @ x\n",
    "trace_form = np.trace(A @ x @ x.T)\n",
    "\n",
    "print(\"\\nx^T A x =\", quad_form[0, 0])\n",
    "print(\"tr(A x x^T) =\", trace_form)\n",
    "\n",
    "# outer product x x^T\n",
    "print(\"\\nx x^T (outer product):\")\n",
    "print(x @ x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 연습문제\n",
    "\n",
    "### 문제 1: 기본 공식\n",
    "\n",
    "다음 gradient를 구하시오 ($x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^n$):\n",
    "\n",
    "1. $\\frac{\\partial (b^T x)}{\\partial x}$\n",
    "2. $\\frac{\\partial (x^T x)}{\\partial x}$\n",
    "3. $\\frac{\\partial (Ax + b)^T (Ax + b)}{\\partial x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 1 풀이\n",
    "\n",
    "# 답안:\n",
    "# 1. d(b^T x)/dx = b\n",
    "# 2. d(x^T x)/dx = 2x\n",
    "# 3. d((Ax + b)^T (Ax + b))/dx = 2A^T(Ax + b)\n",
    "\n",
    "# 검증\n",
    "n = 3\n",
    "A = np.random.randn(n, n)\n",
    "b = np.random.randn(n, 1)\n",
    "x = np.random.randn(n, 1)\n",
    "\n",
    "print(\"=== 문제 1-1: d(b^T x)/dx ===\")\n",
    "# Analytical\n",
    "grad1_analytical = b\n",
    "print(\"Analytical:\", grad1_analytical.T)\n",
    "\n",
    "# Numerical\n",
    "eps = 1e-5\n",
    "grad1_numerical = np.zeros_like(x)\n",
    "for i in range(n):\n",
    "    x_plus = x.copy()\n",
    "    x_plus[i] += eps\n",
    "    f_plus = b.T @ x_plus\n",
    "    \n",
    "    x_minus = x.copy()\n",
    "    x_minus[i] -= eps\n",
    "    f_minus = b.T @ x_minus\n",
    "    \n",
    "    grad1_numerical[i] = (f_plus - f_minus) / (2 * eps)\n",
    "\n",
    "print(\"Numerical:\", grad1_numerical.T)\n",
    "print(\"Error:\", np.max(np.abs(grad1_analytical - grad1_numerical)))\n",
    "\n",
    "print(\"\\n=== 문제 1-2: d(x^T x)/dx ===\")\n",
    "# Analytical\n",
    "grad2_analytical = 2 * x\n",
    "print(\"Analytical:\", grad2_analytical.T)\n",
    "\n",
    "# Numerical\n",
    "grad2_numerical = np.zeros_like(x)\n",
    "for i in range(n):\n",
    "    x_plus = x.copy()\n",
    "    x_plus[i] += eps\n",
    "    f_plus = x_plus.T @ x_plus\n",
    "    \n",
    "    x_minus = x.copy()\n",
    "    x_minus[i] -= eps\n",
    "    f_minus = x_minus.T @ x_minus\n",
    "    \n",
    "    grad2_numerical[i] = (f_plus - f_minus) / (2 * eps)\n",
    "\n",
    "print(\"Numerical:\", grad2_numerical.T)\n",
    "print(\"Error:\", np.max(np.abs(grad2_analytical - grad2_numerical)))\n",
    "\n",
    "print(\"\\n=== 문제 1-3: d((Ax+b)^T(Ax+b))/dx ===\")\n",
    "# Analytical\n",
    "grad3_analytical = 2 * A.T @ (A @ x + b)\n",
    "print(\"Analytical:\", grad3_analytical.T)\n",
    "\n",
    "# Numerical\n",
    "grad3_numerical = np.zeros_like(x)\n",
    "for i in range(n):\n",
    "    x_plus = x.copy()\n",
    "    x_plus[i] += eps\n",
    "    residual_plus = A @ x_plus + b\n",
    "    f_plus = residual_plus.T @ residual_plus\n",
    "    \n",
    "    x_minus = x.copy()\n",
    "    x_minus[i] -= eps\n",
    "    residual_minus = A @ x_minus + b\n",
    "    f_minus = residual_minus.T @ residual_minus\n",
    "    \n",
    "    grad3_numerical[i] = (f_plus - f_minus) / (2 * eps)\n",
    "\n",
    "print(\"Numerical:\", grad3_numerical.T)\n",
    "print(\"Error:\", np.max(np.abs(grad3_analytical - grad3_numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2: Logistic Regression\n",
    "\n",
    "Logistic regression의 loss function:\n",
    "\n",
    "$$L(w) = -\\frac{1}{m}\\sum_{i=1}^{m} [y_i \\log(\\sigma(w^T x_i)) + (1-y_i)\\log(1 - \\sigma(w^T x_i))]$$\n",
    "\n",
    "where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "**과제**: $\\frac{\\partial L}{\\partial w}$를 유도하고 코드로 검증하시오.\n",
    "\n",
    "**힌트**: $\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1 - \\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 2 풀이\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(X, y, w):\n",
    "    \"\"\"Logistic regression loss (binary cross-entropy)\"\"\"\n",
    "    m = len(y)\n",
    "    z = X @ w\n",
    "    h = sigmoid(z)\n",
    "    \n",
    "    # Numerical stability\n",
    "    loss = -np.mean(y * np.log(h + 1e-10) + (1 - y) * np.log(1 - h + 1e-10))\n",
    "    return loss\n",
    "\n",
    "def logistic_gradient(X, y, w):\n",
    "    \"\"\"Gradient of logistic regression loss\n",
    "    \n",
    "    유도:\n",
    "    dL/dw = -1/m * Σ [y_i * (1/σ(z_i)) * σ(z_i)(1-σ(z_i)) * x_i - (1-y_i) * (1/(1-σ(z_i))) * σ(z_i)(1-σ(z_i)) * x_i]\n",
    "          = -1/m * Σ [y_i * (1-σ(z_i)) - (1-y_i) * σ(z_i)] * x_i\n",
    "          = -1/m * Σ [y_i - σ(z_i)] * x_i\n",
    "          = 1/m * X^T (σ(Xw) - y)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    z = X @ w\n",
    "    h = sigmoid(z)\n",
    "    grad = (1 / m) * X.T @ (h - y)\n",
    "    return grad\n",
    "\n",
    "# 데이터 생성\n",
    "np.random.seed(42)\n",
    "m, n = 100, 3\n",
    "X = np.random.randn(m, n)\n",
    "w_true = np.array([[1], [-2], [0.5]])\n",
    "y = (sigmoid(X @ w_true) > 0.5).astype(float)\n",
    "\n",
    "w_test = np.random.randn(n, 1) * 0.1\n",
    "\n",
    "# Analytical gradient\n",
    "grad_analytical = logistic_gradient(X, y, w_test)\n",
    "print(\"Analytical gradient:\")\n",
    "print(grad_analytical.T)\n",
    "\n",
    "# Numerical gradient\n",
    "eps = 1e-5\n",
    "grad_numerical = np.zeros_like(w_test)\n",
    "\n",
    "for i in range(n):\n",
    "    w_plus = w_test.copy()\n",
    "    w_plus[i] += eps\n",
    "    loss_plus = logistic_loss(X, y, w_plus)\n",
    "    \n",
    "    w_minus = w_test.copy()\n",
    "    w_minus[i] -= eps\n",
    "    loss_minus = logistic_loss(X, y, w_minus)\n",
    "    \n",
    "    grad_numerical[i] = (loss_plus - loss_minus) / (2 * eps)\n",
    "\n",
    "print(\"\\nNumerical gradient:\")\n",
    "print(grad_numerical.T)\n",
    "\n",
    "print(\"\\nError:\")\n",
    "print(np.max(np.abs(grad_analytical - grad_numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3: Neural Network Backpropagation\n",
    "\n",
    "2-layer neural network:\n",
    "\n",
    "$$h = \\sigma(W_1 x + b_1)$$\n",
    "$$\\hat{y} = W_2 h + b_2$$\n",
    "$$L = \\frac{1}{2}||\\hat{y} - y||^2$$\n",
    "\n",
    "**과제**: $\\frac{\\partial L}{\\partial W_1}$, $\\frac{\\partial L}{\\partial W_2}$를 구하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 3 풀이\n",
    "\n",
    "# 네트워크 구조\n",
    "input_dim = 3\n",
    "hidden_dim = 4\n",
    "output_dim = 2\n",
    "\n",
    "# 파라미터 초기화\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_dim, input_dim) * 0.1\n",
    "b1 = np.zeros((hidden_dim, 1))\n",
    "W2 = np.random.randn(output_dim, hidden_dim) * 0.1\n",
    "b2 = np.zeros((output_dim, 1))\n",
    "\n",
    "# 샘플 데이터\n",
    "x = np.random.randn(input_dim, 1)\n",
    "y = np.random.randn(output_dim, 1)\n",
    "\n",
    "def forward(x, W1, b1, W2, b2):\n",
    "    \"\"\"Forward pass\"\"\"\n",
    "    z1 = W1 @ x + b1\n",
    "    h = sigmoid(z1)\n",
    "    z2 = W2 @ h + b2\n",
    "    y_hat = z2\n",
    "    return z1, h, z2, y_hat\n",
    "\n",
    "def mse_loss(y_hat, y):\n",
    "    \"\"\"MSE loss\"\"\"\n",
    "    return 0.5 * np.sum((y_hat - y) ** 2)\n",
    "\n",
    "# Forward pass\n",
    "z1, h, z2, y_hat = forward(x, W1, b1, W2, b2)\n",
    "loss = mse_loss(y_hat, y)\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(\"z1 shape:\", z1.shape)\n",
    "print(\"h shape:\", h.shape)\n",
    "print(\"y_hat shape:\", y_hat.shape)\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Backpropagation (analytical)\n",
    "# dL/dy_hat = y_hat - y\n",
    "dL_dy_hat = y_hat - y\n",
    "\n",
    "# dL/dW2 = dL/dy_hat * dy_hat/dW2 = dL/dy_hat * h^T\n",
    "dL_dW2 = dL_dy_hat @ h.T\n",
    "\n",
    "# dL/dh = W2^T @ dL/dy_hat\n",
    "dL_dh = W2.T @ dL_dy_hat\n",
    "\n",
    "# dL/dz1 = dL/dh * dh/dz1 = dL/dh * σ'(z1)\n",
    "dh_dz1 = h * (1 - h)  # sigmoid derivative\n",
    "dL_dz1 = dL_dh * dh_dz1\n",
    "\n",
    "# dL/dW1 = dL/dz1 @ x^T\n",
    "dL_dW1 = dL_dz1 @ x.T\n",
    "\n",
    "print(\"\\nAnalytical gradients:\")\n",
    "print(\"dL/dW1 shape:\", dL_dW1.shape)\n",
    "print(\"dL/dW2 shape:\", dL_dW2.shape)\n",
    "\n",
    "# Numerical gradient for W2\n",
    "print(\"\\n=== Verifying dL/dW2 ===\")\n",
    "eps = 1e-5\n",
    "dL_dW2_numerical = np.zeros_like(W2)\n",
    "\n",
    "for i in range(W2.shape[0]):\n",
    "    for j in range(W2.shape[1]):\n",
    "        W2_plus = W2.copy()\n",
    "        W2_plus[i, j] += eps\n",
    "        _, _, _, y_hat_plus = forward(x, W1, b1, W2_plus, b2)\n",
    "        loss_plus = mse_loss(y_hat_plus, y)\n",
    "        \n",
    "        W2_minus = W2.copy()\n",
    "        W2_minus[i, j] -= eps\n",
    "        _, _, _, y_hat_minus = forward(x, W1, b1, W2_minus, b2)\n",
    "        loss_minus = mse_loss(y_hat_minus, y)\n",
    "        \n",
    "        dL_dW2_numerical[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "\n",
    "print(\"Analytical dL/dW2:\")\n",
    "print(dL_dW2)\n",
    "print(\"\\nNumerical dL/dW2:\")\n",
    "print(dL_dW2_numerical)\n",
    "print(\"\\nError:\", np.max(np.abs(dL_dW2 - dL_dW2_numerical)))\n",
    "\n",
    "# Sample check for W1 (only first element for speed)\n",
    "print(\"\\n=== Verifying dL/dW1[0,0] ===\")\n",
    "W1_plus = W1.copy()\n",
    "W1_plus[0, 0] += eps\n",
    "_, _, _, y_hat_plus = forward(x, W1_plus, b1, W2, b2)\n",
    "loss_plus = mse_loss(y_hat_plus, y)\n",
    "\n",
    "W1_minus = W1.copy()\n",
    "W1_minus[0, 0] -= eps\n",
    "_, _, _, y_hat_minus = forward(x, W1_minus, b1, W2, b2)\n",
    "loss_minus = mse_loss(y_hat_minus, y)\n",
    "\n",
    "dL_dW1_00_numerical = (loss_plus - loss_minus) / (2 * eps)\n",
    "\n",
    "print(\"Analytical dL/dW1[0,0]:\", dL_dW1[0, 0])\n",
    "print(\"Numerical dL/dW1[0,0]:\", dL_dW1_00_numerical)\n",
    "print(\"Error:\", abs(dL_dW1[0, 0] - dL_dW1_00_numerical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 요약 및 체크리스트\n",
    "\n",
    "### 핵심 공식 정리\n",
    "\n",
    "| 함수 | Gradient | 비고 |\n",
    "|------|----------|------|\n",
    "| $Ax$ | $A^T$ | **전치 필요!** |\n",
    "| $x^T A$ | $A$ | 전치 없음 |\n",
    "| $x^T A x$ (A 대칭) | $2Ax$ | 이차 형식 |\n",
    "| $x^T A x$ (A 일반) | $(A + A^T)x$ | 일반 이차 형식 |\n",
    "| $\\|Ax - b\\|^2$ | $2A^T(Ax - b)$ | MSE의 기본 |\n",
    "\n",
    "### 전치가 나타나는 이유\n",
    "\n",
    "1. **Chain rule**: $\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial x}$에서 Jacobian의 전치\n",
    "2. **차원 일치**: Gradient는 항상 원래 변수와 같은 shape\n",
    "3. **Denominator layout**: ML에서 표준 규약\n",
    "\n",
    "### 자주 하는 실수\n",
    "\n",
    "- ❌ $\\frac{\\partial (Ax)}{\\partial x} = A$ (틀림!)\n",
    "- ✅ $\\frac{\\partial (Ax)}{\\partial x} = A^T$ (맞음!)\n",
    "\n",
    "- ❌ Chain rule에서 차원 안 맞춤\n",
    "- ✅ 항상 차원 체크하기\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "- 다음 노트북: **확률론 심화** (조건부 확률 → Bayes → Softmax)\n",
    "- 추가 학습: ML_L04_vector.calculus_review.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 추가 연습문제\n",
    "\n",
    "직접 풀어보고 numerical gradient로 검증하세요!\n",
    "\n",
    "1. $\\frac{\\partial ||x||_2}{\\partial x}$ (L2 norm)\n",
    "2. $\\frac{\\partial (x^T A^T A x)}{\\partial x}$\n",
    "3. $\\frac{\\partial \\log(1 + e^{w^T x})}{\\partial w}$ (logistic loss의 일부)\n",
    "4. Ridge regression loss: $L(w) = ||Xw - y||^2 + \\lambda ||w||^2$의 gradient\n",
    "5. Weighted MSE: $L(w) = (Xw - y)^T W (Xw - y)$의 gradient (W는 대칭 행렬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 연습문제 템플릿\n",
    "# 여기에 직접 풀어보세요!\n",
    "\n",
    "# 문제 1: d(||x||_2)/dx\n",
    "# 답: x / ||x||_2\n",
    "\n",
    "# 문제 2: d(x^T A^T A x)/dx  \n",
    "# 답: 2 A^T A x (A^T A는 대칭)\n",
    "\n",
    "# 문제 3: d(log(1 + e^(w^T x)))/dw\n",
    "# 답: σ(w^T x) * x where σ is sigmoid\n",
    "\n",
    "# 문제 4: Ridge regression gradient\n",
    "# 답: 2 X^T(Xw - y) + 2λw\n",
    "\n",
    "# 문제 5: Weighted MSE gradient\n",
    "# 답: 2 X^T W (Xw - y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
